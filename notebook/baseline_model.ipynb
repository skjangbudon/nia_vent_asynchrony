{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('/VOLUME/nia_vent_asynchrony')\n",
    "from module.datasets import setup_data, set_dataloader, CustomDataset\n",
    "from module.utils import load_and_stack_data\n",
    "from model.VAE import VAE\n",
    "from model.AsynchModel import AsynchModel\n",
    "from module.metrics import calculate_any_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 480]              96\n",
      "       BatchNorm1d-2              [-1, 16, 480]              32\n",
      "              ReLU-3              [-1, 16, 480]               0\n",
      "         MaxPool1d-4              [-1, 16, 120]               0\n",
      "            Conv1d-5              [-1, 32, 120]           2,592\n",
      "       BatchNorm1d-6              [-1, 32, 120]              64\n",
      "              ReLU-7              [-1, 32, 120]               0\n",
      "         MaxPool1d-8               [-1, 32, 30]               0\n",
      "            Conv1d-9               [-1, 64, 30]          10,304\n",
      "      BatchNorm1d-10               [-1, 64, 30]             128\n",
      "             ReLU-11               [-1, 64, 30]               0\n",
      "           Linear-12                  [-1, 128]           3,968\n",
      "           Linear-13                  [-1, 128]           3,968\n",
      "           Linear-14                   [-1, 30]           3,870\n",
      "  ConvTranspose1d-15               [-1, 32, 30]          10,272\n",
      "      BatchNorm1d-16               [-1, 32, 30]              64\n",
      "             ReLU-17               [-1, 32, 30]               0\n",
      "         Upsample-18              [-1, 32, 120]               0\n",
      "  ConvTranspose1d-19              [-1, 16, 120]           2,576\n",
      "      BatchNorm1d-20              [-1, 16, 120]              32\n",
      "             ReLU-21              [-1, 16, 120]               0\n",
      "         Upsample-22              [-1, 16, 480]               0\n",
      "  ConvTranspose1d-23               [-1, 1, 480]              81\n",
      "      BatchNorm1d-24               [-1, 1, 480]               2\n",
      "             ReLU-25               [-1, 1, 480]               0\n",
      "================================================================\n",
      "Total params: 38,049\n",
      "Trainable params: 38,049\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.50\n",
      "Params size (MB): 0.15\n",
      "Estimated Total Size (MB): 0.64\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vae = VAE().cpu()\n",
    "summary(vae, input_size=(1, 480), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/VOLUME/nia_vent_asynchrony/data/processed_data/8c91bd20c48a51487ef5_feature.csv (25896, 481)\n",
      "/VOLUME/nia_vent_asynchrony/data/processed_data/MICU01_feature.csv (38100, 481)\n",
      "(63996, 481)\n"
     ]
    }
   ],
   "source": [
    "dat = load_and_stack_data(glob.glob('/VOLUME/nia_vent_asynchrony/data/processed_data/*_feature.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dat.loc[:,dat.columns!='label'].values\n",
    "label = dat['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63996, 480)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape (51196, 480) (12800, 480)\n",
      "Y class distribution 0.07029846081725134 0.073984375\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = setup_data(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train VAE\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bce_loss = torch.nn.BCELoss(reduction='mean')\n",
    "\n",
    "def calculate_vae_loss(out, xi, batch_size):\n",
    "    loss_r = bce_loss(out, xi)\n",
    "    loss_kl = torch.mean(.5 * torch.sum(mu.pow(2) + torch.exp(logVar) - 1 - logVar, 1))\n",
    "    loss = torch.mean(loss_r) + loss_kl\n",
    "    return loss, loss_r, loss_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bce_loss(out, target):\n",
    "    loss = bce_loss(out, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:3609: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 19.12981605529785 loss_r 0.7170383930206299 loss_kl 18.412776947021484\n",
      "step 1 loss 15.704118728637695 loss_r 0.6856671571731567 loss_kl 15.018451690673828\n",
      "step 2 loss 13.391410827636719 loss_r 0.6638031601905823 loss_kl 12.727607727050781\n",
      "step 3 loss 11.377877235412598 loss_r 0.6697244644165039 loss_kl 10.708152770996094\n",
      "step 4 loss 10.184852600097656 loss_r 0.6520732045173645 loss_kl 9.532779693603516\n",
      "step 5 loss 9.526041984558105 loss_r 0.6140482425689697 loss_kl 8.911993980407715\n",
      "step 6 loss 7.846816062927246 loss_r 0.6441250443458557 loss_kl 7.202691078186035\n",
      "step 7 loss 7.128811836242676 loss_r 0.6469420790672302 loss_kl 6.481869697570801\n",
      "step 8 loss 6.042996406555176 loss_r 0.6532232165336609 loss_kl 5.389773368835449\n",
      "step 9 loss 5.7660932540893555 loss_r 0.635067880153656 loss_kl 5.131025314331055\n",
      "step 10 loss 4.868652820587158 loss_r 0.6725306510925293 loss_kl 4.196122169494629\n",
      "step 11 loss 4.310087203979492 loss_r 0.6501337885856628 loss_kl 3.6599535942077637\n",
      "step 12 loss 4.016739845275879 loss_r 0.6454164385795593 loss_kl 3.371323347091675\n",
      "step 13 loss 3.8111650943756104 loss_r 0.6444609761238098 loss_kl 3.1667041778564453\n",
      "step 14 loss 3.564654588699341 loss_r 0.6169854998588562 loss_kl 2.94766902923584\n",
      "step 15 loss 3.207131862640381 loss_r 0.6022874712944031 loss_kl 2.604844331741333\n",
      "step 16 loss 3.1187045574188232 loss_r 0.5866100192070007 loss_kl 2.5320944786071777\n",
      "step 17 loss 2.6627402305603027 loss_r 0.5610044002532959 loss_kl 2.101735830307007\n",
      "step 18 loss 2.5383501052856445 loss_r 0.5954955220222473 loss_kl 1.9428545236587524\n",
      "step 19 loss 2.5300607681274414 loss_r 0.5666987299919128 loss_kl 1.9633619785308838\n",
      "step 20 loss 2.2102694511413574 loss_r 0.5142542123794556 loss_kl 1.6960151195526123\n",
      "step 21 loss 2.065276861190796 loss_r 0.5383827090263367 loss_kl 1.5268940925598145\n",
      "step 22 loss 2.0556068420410156 loss_r 0.5202153325080872 loss_kl 1.5353914499282837\n",
      "step 23 loss 1.9630876779556274 loss_r 0.5229460000991821 loss_kl 1.4401416778564453\n",
      "step 24 loss 1.732530951499939 loss_r 0.5130201578140259 loss_kl 1.219510793685913\n",
      "step 25 loss 1.791186809539795 loss_r 0.5291838049888611 loss_kl 1.2620030641555786\n",
      "step 26 loss 1.7775380611419678 loss_r 0.5046687126159668 loss_kl 1.272869348526001\n",
      "step 27 loss 1.6384189128875732 loss_r 0.527930736541748 loss_kl 1.1104881763458252\n",
      "step 28 loss 1.5356544256210327 loss_r 0.49065402150154114 loss_kl 1.045000433921814\n",
      "step 29 loss 1.4651880264282227 loss_r 0.5078168511390686 loss_kl 0.9573711156845093\n",
      "step 30 loss 1.563323736190796 loss_r 0.4854244589805603 loss_kl 1.0778992176055908\n",
      "step 31 loss 1.3606035709381104 loss_r 0.4815604090690613 loss_kl 0.8790431618690491\n",
      "step 32 loss 1.4685012102127075 loss_r 0.49098482728004456 loss_kl 0.9775164127349854\n",
      "step 33 loss 1.314950704574585 loss_r 0.46911486983299255 loss_kl 0.84583580493927\n",
      "step 34 loss 1.2139993906021118 loss_r 0.4922162890434265 loss_kl 0.7217831015586853\n",
      "step 35 loss 1.2595117092132568 loss_r 0.49085479974746704 loss_kl 0.7686569690704346\n",
      "step 36 loss 1.1716721057891846 loss_r 0.4742054343223572 loss_kl 0.6974666118621826\n",
      "step 37 loss 1.1804513931274414 loss_r 0.4261750876903534 loss_kl 0.7542762756347656\n",
      "step 38 loss 1.176329255104065 loss_r 0.4672682583332062 loss_kl 0.7090610265731812\n",
      "step 39 loss 1.1801203489303589 loss_r 0.47240856289863586 loss_kl 0.7077118158340454\n",
      "step 40 loss 1.1175936460494995 loss_r 0.45484328269958496 loss_kl 0.6627503633499146\n",
      "step 41 loss 1.0241681337356567 loss_r 0.45874401926994324 loss_kl 0.5654240846633911\n",
      "step 42 loss 1.0098437070846558 loss_r 0.4370981454849243 loss_kl 0.5727455615997314\n",
      "step 43 loss 1.0097154378890991 loss_r 0.4510495662689209 loss_kl 0.5586658716201782\n",
      "step 44 loss 0.9816173911094666 loss_r 0.4614911675453186 loss_kl 0.520126223564148\n",
      "step 45 loss 0.9411759376525879 loss_r 0.42618128657341003 loss_kl 0.5149946808815002\n",
      "step 46 loss 0.9355478286743164 loss_r 0.4525304436683655 loss_kl 0.4830174148082733\n",
      "step 47 loss 0.9480856657028198 loss_r 0.462108314037323 loss_kl 0.4859773814678192\n",
      "step 48 loss 0.922430157661438 loss_r 0.43560200929641724 loss_kl 0.48682817816734314\n",
      "step 49 loss 0.915778636932373 loss_r 0.4407743513584137 loss_kl 0.47500431537628174\n",
      "step 50 loss 0.9494296312332153 loss_r 0.43654492497444153 loss_kl 0.5128847360610962\n",
      "step 51 loss 0.9195098876953125 loss_r 0.45679670572280884 loss_kl 0.4627131521701813\n",
      "step 52 loss 0.8924367427825928 loss_r 0.4206601083278656 loss_kl 0.4717766046524048\n",
      "step 53 loss 0.874733030796051 loss_r 0.4251966178417206 loss_kl 0.44953641295433044\n",
      "step 54 loss 0.878267765045166 loss_r 0.45037105679512024 loss_kl 0.4278966784477234\n",
      "step 55 loss 0.8236039876937866 loss_r 0.42568066716194153 loss_kl 0.3979233205318451\n",
      "step 56 loss 0.858870267868042 loss_r 0.4400208294391632 loss_kl 0.4188494384288788\n",
      "step 57 loss 0.8114528656005859 loss_r 0.42723727226257324 loss_kl 0.3842155933380127\n",
      "step 58 loss 0.7962731719017029 loss_r 0.4016423225402832 loss_kl 0.3946308493614197\n",
      "step 59 loss 0.7829409837722778 loss_r 0.4169237017631531 loss_kl 0.36601725220680237\n",
      "step 60 loss 0.7905937433242798 loss_r 0.42822498083114624 loss_kl 0.36236873269081116\n",
      "step 61 loss 0.7813725471496582 loss_r 0.4213911294937134 loss_kl 0.3599814176559448\n",
      "step 62 loss 0.7773267030715942 loss_r 0.4324629008769989 loss_kl 0.3448638319969177\n",
      "step 63 loss 0.7667677402496338 loss_r 0.4044725000858307 loss_kl 0.3622952103614807\n",
      "step 64 loss 0.7917476892471313 loss_r 0.41434311866760254 loss_kl 0.3774045705795288\n",
      "step 65 loss 0.7584215402603149 loss_r 0.4240332245826721 loss_kl 0.33438828587532043\n",
      "step 66 loss 0.7627483606338501 loss_r 0.43804603815078735 loss_kl 0.32470232248306274\n",
      "step 67 loss 0.7464079856872559 loss_r 0.4056949019432068 loss_kl 0.3407130837440491\n",
      "step 68 loss 0.7771950960159302 loss_r 0.43009665608406067 loss_kl 0.3470984399318695\n",
      "step 69 loss 0.734817385673523 loss_r 0.4271838665008545 loss_kl 0.30763354897499084\n",
      "step 70 loss 0.7163841724395752 loss_r 0.388794869184494 loss_kl 0.3275892734527588\n",
      "step 71 loss 0.7260366082191467 loss_r 0.4247576594352722 loss_kl 0.3012789487838745\n",
      "step 72 loss 0.7314359545707703 loss_r 0.3772599697113037 loss_kl 0.35417598485946655\n",
      "step 73 loss 0.7245829701423645 loss_r 0.38646799325942993 loss_kl 0.33811497688293457\n",
      "step 74 loss 0.7482677102088928 loss_r 0.46108484268188477 loss_kl 0.28718286752700806\n",
      "step 75 loss 0.7121920585632324 loss_r 0.43205320835113525 loss_kl 0.28013888001441956\n",
      "step 76 loss 0.7226769924163818 loss_r 0.3945046067237854 loss_kl 0.32817235589027405\n",
      "step 77 loss 0.7132189273834229 loss_r 0.39142903685569763 loss_kl 0.3217898905277252\n",
      "step 78 loss 0.7042597532272339 loss_r 0.38889166712760925 loss_kl 0.31536808609962463\n",
      "step 79 loss 0.7038519382476807 loss_r 0.4306860864162445 loss_kl 0.27316582202911377\n",
      "step 80 loss 0.6949095129966736 loss_r 0.4129177927970886 loss_kl 0.28199172019958496\n",
      "step 81 loss 0.7067486047744751 loss_r 0.4291711151599884 loss_kl 0.2775775194168091\n",
      "step 82 loss 0.6542549133300781 loss_r 0.3903062045574188 loss_kl 0.2639487385749817\n",
      "step 83 loss 0.6526065468788147 loss_r 0.3942032754421234 loss_kl 0.2584032714366913\n",
      "step 84 loss 0.6720210313796997 loss_r 0.4041869640350342 loss_kl 0.2678340673446655\n",
      "step 85 loss 0.6709123849868774 loss_r 0.3907150328159332 loss_kl 0.2801973819732666\n",
      "step 86 loss 0.6711171865463257 loss_r 0.41238686442375183 loss_kl 0.25873029232025146\n",
      "step 87 loss 0.659243106842041 loss_r 0.3948092460632324 loss_kl 0.264433890581131\n",
      "step 88 loss 0.6723353862762451 loss_r 0.40409693121910095 loss_kl 0.26823848485946655\n",
      "step 89 loss 0.6343224048614502 loss_r 0.38401710987091064 loss_kl 0.25030526518821716\n",
      "step 90 loss 0.6405240297317505 loss_r 0.38084736466407776 loss_kl 0.25967663526535034\n",
      "step 91 loss 0.6456178426742554 loss_r 0.38390403985977173 loss_kl 0.26171383261680603\n",
      "step 92 loss 0.6407666802406311 loss_r 0.40308529138565063 loss_kl 0.23768140375614166\n",
      "step 93 loss 0.6325385570526123 loss_r 0.38350439071655273 loss_kl 0.24903413653373718\n",
      "step 94 loss 0.6407082676887512 loss_r 0.4037943482398987 loss_kl 0.23691391944885254\n",
      "step 95 loss 0.5974568128585815 loss_r 0.3693927824497223 loss_kl 0.22806406021118164\n",
      "step 96 loss 0.6148340106010437 loss_r 0.37864795327186584 loss_kl 0.23618604242801666\n",
      "step 97 loss 0.5925907492637634 loss_r 0.36839422583580017 loss_kl 0.22419653832912445\n",
      "step 98 loss 0.6166269779205322 loss_r 0.4023676812648773 loss_kl 0.21425926685333252\n",
      "step 99 loss 0.6194911003112793 loss_r 0.38738977909088135 loss_kl 0.23210129141807556\n",
      "step 100 loss 0.6071689128875732 loss_r 0.36989903450012207 loss_kl 0.23726990818977356\n",
      "step 101 loss 0.5913735628128052 loss_r 0.3683084547519684 loss_kl 0.2230650931596756\n",
      "step 102 loss 0.6129254698753357 loss_r 0.41129350662231445 loss_kl 0.20163197815418243\n",
      "step 103 loss 0.5948824882507324 loss_r 0.38624727725982666 loss_kl 0.20863524079322815\n",
      "step 104 loss 0.6021533012390137 loss_r 0.4049578607082367 loss_kl 0.19719544053077698\n",
      "step 105 loss 0.5907866358757019 loss_r 0.3904201090335846 loss_kl 0.2003665268421173\n",
      "step 106 loss 0.580845296382904 loss_r 0.37461739778518677 loss_kl 0.2062278836965561\n",
      "step 107 loss 0.5767251253128052 loss_r 0.3835899829864502 loss_kl 0.19313517212867737\n",
      "step 108 loss 0.5550916790962219 loss_r 0.35837990045547485 loss_kl 0.19671177864074707\n",
      "step 109 loss 0.5718157291412354 loss_r 0.36597394943237305 loss_kl 0.2058417648077011\n",
      "step 110 loss 0.5793652534484863 loss_r 0.36139366030693054 loss_kl 0.2179715633392334\n",
      "step 111 loss 0.5823342800140381 loss_r 0.38635146617889404 loss_kl 0.19598281383514404\n",
      "step 112 loss 0.5509767532348633 loss_r 0.3518161177635193 loss_kl 0.1991606056690216\n",
      "step 113 loss 0.5704789161682129 loss_r 0.3849547207355499 loss_kl 0.18552421033382416\n",
      "step 114 loss 0.5957503318786621 loss_r 0.40756985545158386 loss_kl 0.18818047642707825\n",
      "step 115 loss 0.5885070562362671 loss_r 0.40476569533348083 loss_kl 0.18374134600162506\n",
      "step 116 loss 0.568915843963623 loss_r 0.3877766728401184 loss_kl 0.18113917112350464\n",
      "step 117 loss 0.5939244031906128 loss_r 0.42821139097213745 loss_kl 0.16571298241615295\n",
      "step 118 loss 0.5436298847198486 loss_r 0.3533203899860382 loss_kl 0.19030949473381042\n",
      "step 119 loss 0.5705289244651794 loss_r 0.3896609842777252 loss_kl 0.18086792528629303\n",
      "step 120 loss 0.5483928918838501 loss_r 0.3676149845123291 loss_kl 0.180777907371521\n",
      "step 121 loss 0.542827844619751 loss_r 0.3531597852706909 loss_kl 0.18966808915138245\n",
      "step 122 loss 0.5466005802154541 loss_r 0.356050968170166 loss_kl 0.19054961204528809\n",
      "step 123 loss 0.571204662322998 loss_r 0.41596344113349915 loss_kl 0.1552412509918213\n",
      "step 124 loss 0.5562995076179504 loss_r 0.38852253556251526 loss_kl 0.16777698695659637\n",
      "step 125 loss 0.5617653131484985 loss_r 0.37485969066619873 loss_kl 0.1869056522846222\n",
      "step 126 loss 0.526315450668335 loss_r 0.36708739399909973 loss_kl 0.15922802686691284\n",
      "step 127 loss 0.5111170411109924 loss_r 0.3498317301273346 loss_kl 0.16128529608249664\n",
      "step 128 loss 0.5385667085647583 loss_r 0.35436323285102844 loss_kl 0.18420344591140747\n",
      "step 129 loss 0.5653976202011108 loss_r 0.4162924587726593 loss_kl 0.14910516142845154\n",
      "step 130 loss 0.5563977956771851 loss_r 0.4029543697834015 loss_kl 0.15344339609146118\n",
      "step 131 loss 0.5403403043746948 loss_r 0.3844929039478302 loss_kl 0.15584740042686462\n",
      "step 132 loss 0.545710027217865 loss_r 0.35024142265319824 loss_kl 0.19546860456466675\n",
      "step 133 loss 0.5321592688560486 loss_r 0.38302668929100037 loss_kl 0.14913256466388702\n",
      "step 134 loss 0.5130646228790283 loss_r 0.35920026898384094 loss_kl 0.15386438369750977\n",
      "step 135 loss 0.5421351194381714 loss_r 0.39669662714004517 loss_kl 0.14543846249580383\n",
      "step 136 loss 0.5261340737342834 loss_r 0.37613001465797424 loss_kl 0.1500040739774704\n",
      "step 137 loss 0.5192481279373169 loss_r 0.3749047815799713 loss_kl 0.1443433165550232\n",
      "step 138 loss 0.5274084210395813 loss_r 0.3928828835487366 loss_kl 0.13452553749084473\n",
      "step 139 loss 0.5233654379844666 loss_r 0.36124613881111145 loss_kl 0.1621192991733551\n",
      "step 140 loss 0.517349898815155 loss_r 0.34170374274253845 loss_kl 0.17564615607261658\n",
      "step 141 loss 0.512929379940033 loss_r 0.36557111144065857 loss_kl 0.1473582535982132\n",
      "step 142 loss 0.5012607574462891 loss_r 0.34221890568733215 loss_kl 0.1590418517589569\n",
      "step 143 loss 0.49330446124076843 loss_r 0.33297374844551086 loss_kl 0.16033071279525757\n",
      "step 144 loss 0.4982185959815979 loss_r 0.36454418301582336 loss_kl 0.13367441296577454\n",
      "step 145 loss 0.5090203881263733 loss_r 0.3402990400791168 loss_kl 0.16872134804725647\n",
      "step 146 loss 0.4922408163547516 loss_r 0.3430274724960327 loss_kl 0.14921334385871887\n",
      "step 147 loss 0.5019770860671997 loss_r 0.3720305860042572 loss_kl 0.1299464851617813\n",
      "step 148 loss 0.5156967639923096 loss_r 0.37271857261657715 loss_kl 0.1429782211780548\n",
      "step 149 loss 0.4872279167175293 loss_r 0.34912922978401184 loss_kl 0.13809868693351746\n",
      "step 150 loss 0.5329912900924683 loss_r 0.41118693351745605 loss_kl 0.12180434167385101\n",
      "step 151 loss 0.528809130191803 loss_r 0.4087454676628113 loss_kl 0.1200636774301529\n",
      "step 152 loss 0.5096523761749268 loss_r 0.34386971592903137 loss_kl 0.16578266024589539\n",
      "step 153 loss 0.4988839626312256 loss_r 0.3764135539531708 loss_kl 0.12247040122747421\n",
      "step 154 loss 0.4768345355987549 loss_r 0.337674617767334 loss_kl 0.1391599327325821\n",
      "step 155 loss 0.47008413076400757 loss_r 0.34002000093460083 loss_kl 0.13006412982940674\n",
      "step 156 loss 0.5151972770690918 loss_r 0.39187484979629517 loss_kl 0.12332241237163544\n",
      "step 157 loss 0.5106112957000732 loss_r 0.38455674052238464 loss_kl 0.126054584980011\n",
      "step 158 loss 0.4743160605430603 loss_r 0.3504857122898102 loss_kl 0.12383034825325012\n",
      "step 159 loss 0.4907324016094208 loss_r 0.30460575222969055 loss_kl 0.18612664937973022\n",
      "step 160 loss 0.4757291078567505 loss_r 0.34424784779548645 loss_kl 0.13148124516010284\n",
      "step 161 loss 0.4992126226425171 loss_r 0.387801855802536 loss_kl 0.11141077429056168\n",
      "step 162 loss 0.4656743109226227 loss_r 0.3259674906730652 loss_kl 0.1397068202495575\n",
      "step 163 loss 0.47866204380989075 loss_r 0.3261459469795227 loss_kl 0.15251609683036804\n",
      "step 164 loss 0.469300776720047 loss_r 0.3501233458518982 loss_kl 0.1191774383187294\n",
      "step 165 loss 0.467143714427948 loss_r 0.34344467520713806 loss_kl 0.12369902431964874\n",
      "step 166 loss 0.5109966397285461 loss_r 0.40531858801841736 loss_kl 0.10567804425954819\n",
      "step 167 loss 0.4724660813808441 loss_r 0.35293132066726685 loss_kl 0.11953476071357727\n",
      "step 168 loss 0.4675424098968506 loss_r 0.35285893082618713 loss_kl 0.11468347907066345\n",
      "step 169 loss 0.46929875016212463 loss_r 0.35606855154037476 loss_kl 0.11323019117116928\n",
      "step 170 loss 0.47487837076187134 loss_r 0.33202823996543884 loss_kl 0.1428501307964325\n",
      "step 171 loss 0.47566235065460205 loss_r 0.36483776569366455 loss_kl 0.1108245998620987\n",
      "step 172 loss 0.47053223848342896 loss_r 0.360526978969574 loss_kl 0.11000524461269379\n",
      "step 173 loss 0.46418851613998413 loss_r 0.34007692337036133 loss_kl 0.124111607670784\n",
      "step 174 loss 0.4823416471481323 loss_r 0.3743065595626831 loss_kl 0.10803508013486862\n",
      "step 175 loss 0.497527152299881 loss_r 0.3998839557170868 loss_kl 0.09764319658279419\n",
      "step 176 loss 0.45506221055984497 loss_r 0.34100160002708435 loss_kl 0.11406061798334122\n",
      "step 177 loss 0.46717768907546997 loss_r 0.36081257462501526 loss_kl 0.10636509954929352\n",
      "step 178 loss 0.48965543508529663 loss_r 0.38939791917800903 loss_kl 0.100257508456707\n",
      "step 179 loss 0.47790414094924927 loss_r 0.38197678327560425 loss_kl 0.09592735767364502\n",
      "step 180 loss 0.4535158574581146 loss_r 0.3488425016403198 loss_kl 0.1046733558177948\n",
      "step 181 loss 0.46675485372543335 loss_r 0.3601244390010834 loss_kl 0.10663040727376938\n",
      "step 182 loss 0.4465453028678894 loss_r 0.3186151683330536 loss_kl 0.12793013453483582\n",
      "step 183 loss 0.4553319215774536 loss_r 0.34489816427230835 loss_kl 0.11043375730514526\n",
      "step 184 loss 0.45765334367752075 loss_r 0.34334617853164673 loss_kl 0.11430717259645462\n",
      "step 185 loss 0.4746229648590088 loss_r 0.35951876640319824 loss_kl 0.11510418355464935\n",
      "step 186 loss 0.45313721895217896 loss_r 0.35413622856140137 loss_kl 0.09900099039077759\n",
      "step 187 loss 0.4379226565361023 loss_r 0.3387216031551361 loss_kl 0.09920103847980499\n",
      "step 188 loss 0.45449239015579224 loss_r 0.33614760637283325 loss_kl 0.11834479868412018\n",
      "step 189 loss 0.44217026233673096 loss_r 0.34944525361061096 loss_kl 0.09272501617670059\n",
      "step 190 loss 0.47571104764938354 loss_r 0.3852202296257019 loss_kl 0.09049080312252045\n",
      "step 191 loss 0.46218791604042053 loss_r 0.37327396869659424 loss_kl 0.08891395479440689\n",
      "step 192 loss 0.4643571078777313 loss_r 0.3781743049621582 loss_kl 0.08618281036615372\n",
      "step 193 loss 0.4647473990917206 loss_r 0.3744010627269745 loss_kl 0.0903463363647461\n",
      "step 194 loss 0.47170957922935486 loss_r 0.3829328119754791 loss_kl 0.08877676725387573\n",
      "step 195 loss 0.46874165534973145 loss_r 0.3719298541545868 loss_kl 0.09681180119514465\n",
      "step 196 loss 0.4507560729980469 loss_r 0.35547319054603577 loss_kl 0.0952828899025917\n",
      "step 197 loss 0.46200692653656006 loss_r 0.36654189229011536 loss_kl 0.09546501934528351\n",
      "step 198 loss 0.4372388422489166 loss_r 0.3389120399951935 loss_kl 0.09832679480314255\n",
      "step 199 loss 0.419948935508728 loss_r 0.3156201243400574 loss_kl 0.10432881861925125\n",
      "step 200 loss 0.43198275566101074 loss_r 0.33326011896133423 loss_kl 0.09872262179851532\n",
      "step 201 loss 0.43309950828552246 loss_r 0.32831597328186035 loss_kl 0.10478353500366211\n",
      "step 202 loss 0.4557723104953766 loss_r 0.3607245981693268 loss_kl 0.0950477123260498\n",
      "step 203 loss 0.40876227617263794 loss_r 0.3044174313545227 loss_kl 0.10434484481811523\n",
      "step 204 loss 0.45140254497528076 loss_r 0.37096527218818665 loss_kl 0.08043725788593292\n",
      "step 205 loss 0.43219465017318726 loss_r 0.35276058316230774 loss_kl 0.07943406701087952\n",
      "step 206 loss 0.4266682267189026 loss_r 0.3360779285430908 loss_kl 0.09059029072523117\n",
      "step 207 loss 0.42162322998046875 loss_r 0.32624176144599915 loss_kl 0.09538145363330841\n",
      "step 208 loss 0.39544007182121277 loss_r 0.3104199171066284 loss_kl 0.08502015471458435\n",
      "step 209 loss 0.4121357798576355 loss_r 0.31226930022239685 loss_kl 0.09986647218465805\n",
      "step 210 loss 0.49251019954681396 loss_r 0.41472023725509644 loss_kl 0.07778995484113693\n",
      "step 211 loss 0.44043493270874023 loss_r 0.35799384117126465 loss_kl 0.08244110643863678\n",
      "step 212 loss 0.47976937890052795 loss_r 0.4039296805858612 loss_kl 0.07583969831466675\n",
      "step 213 loss 0.39960354566574097 loss_r 0.29280421137809753 loss_kl 0.10679932683706284\n",
      "step 214 loss 0.4400548040866852 loss_r 0.35155224800109863 loss_kl 0.08850255608558655\n",
      "step 215 loss 0.4445728659629822 loss_r 0.33060550689697266 loss_kl 0.11396736651659012\n",
      "step 216 loss 0.41154998540878296 loss_r 0.31578999757766724 loss_kl 0.09576000273227692\n",
      "step 217 loss 0.4142869710922241 loss_r 0.3250049650669098 loss_kl 0.08928202092647552\n",
      "step 218 loss 0.4439007639884949 loss_r 0.36430951952934265 loss_kl 0.07959122955799103\n",
      "step 219 loss 0.42173388600349426 loss_r 0.32931652665138245 loss_kl 0.09241736680269241\n",
      "step 220 loss 0.4082067310810089 loss_r 0.3348490595817566 loss_kl 0.07335767894983292\n",
      "step 221 loss 0.41848886013031006 loss_r 0.33596327900886536 loss_kl 0.0825255811214447\n",
      "step 222 loss 0.42880284786224365 loss_r 0.345838725566864 loss_kl 0.08296412229537964\n",
      "step 223 loss 0.4148443341255188 loss_r 0.31572890281677246 loss_kl 0.09911544620990753\n",
      "step 224 loss 0.39404264092445374 loss_r 0.3088749349117279 loss_kl 0.08516770601272583\n",
      "step 225 loss 0.4265899062156677 loss_r 0.34995800256729126 loss_kl 0.07663188874721527\n",
      "step 226 loss 0.41103705763816833 loss_r 0.32200711965560913 loss_kl 0.0890299379825592\n",
      "step 227 loss 0.4360247850418091 loss_r 0.3584543764591217 loss_kl 0.07757039368152618\n",
      "step 228 loss 0.40874576568603516 loss_r 0.32825684547424316 loss_kl 0.08048892766237259\n",
      "step 229 loss 0.39819633960723877 loss_r 0.32324135303497314 loss_kl 0.07495497167110443\n",
      "step 230 loss 0.40325286984443665 loss_r 0.31807950139045715 loss_kl 0.0851733610033989\n",
      "step 231 loss 0.3899450898170471 loss_r 0.292129784822464 loss_kl 0.09781531989574432\n",
      "step 232 loss 0.4192012548446655 loss_r 0.331912636756897 loss_kl 0.08728862553834915\n",
      "step 233 loss 0.42328235507011414 loss_r 0.35434597730636597 loss_kl 0.06893637776374817\n",
      "step 234 loss 0.4254705011844635 loss_r 0.3578491508960724 loss_kl 0.06762135773897171\n",
      "step 235 loss 0.4073854386806488 loss_r 0.32658594846725464 loss_kl 0.08079949766397476\n",
      "step 236 loss 0.4031122028827667 loss_r 0.32798340916633606 loss_kl 0.07512879371643066\n",
      "step 237 loss 0.41011008620262146 loss_r 0.33921682834625244 loss_kl 0.07089325785636902\n",
      "step 238 loss 0.3916449248790741 loss_r 0.30895182490348816 loss_kl 0.08269309997558594\n",
      "step 239 loss 0.4117855429649353 loss_r 0.3346951901912689 loss_kl 0.07709036767482758\n",
      "step 240 loss 0.43914794921875 loss_r 0.37366634607315063 loss_kl 0.06548158824443817\n",
      "step 241 loss 0.41253089904785156 loss_r 0.3352101147174835 loss_kl 0.07732078433036804\n",
      "step 242 loss 0.4538874626159668 loss_r 0.3879636228084564 loss_kl 0.06592382490634918\n",
      "step 243 loss 0.3913179337978363 loss_r 0.3169209957122803 loss_kl 0.07439694553613663\n",
      "step 244 loss 0.39350494742393494 loss_r 0.31833919882774353 loss_kl 0.0751657485961914\n",
      "step 245 loss 0.40142542123794556 loss_r 0.3307911157608032 loss_kl 0.07063430547714233\n",
      "step 246 loss 0.3988665044307709 loss_r 0.3226773142814636 loss_kl 0.07618919014930725\n",
      "step 247 loss 0.3798752427101135 loss_r 0.30603688955307007 loss_kl 0.07383836805820465\n",
      "step 248 loss 0.41479623317718506 loss_r 0.32772567868232727 loss_kl 0.08707056194543839\n",
      "step 249 loss 0.37374287843704224 loss_r 0.2783304452896118 loss_kl 0.09541242569684982\n",
      "step 250 loss 0.4144078493118286 loss_r 0.3479936122894287 loss_kl 0.0664142370223999\n",
      "step 251 loss 0.3926542103290558 loss_r 0.296030730009079 loss_kl 0.0966234803199768\n",
      "step 252 loss 0.41189005970954895 loss_r 0.34419018030166626 loss_kl 0.0676998719573021\n",
      "step 253 loss 0.4298260807991028 loss_r 0.3608197569847107 loss_kl 0.06900632381439209\n",
      "step 254 loss 0.39653611183166504 loss_r 0.3242991864681244 loss_kl 0.07223691046237946\n",
      "step 255 loss 0.3948330581188202 loss_r 0.31838247179985046 loss_kl 0.07645059376955032\n",
      "step 256 loss 0.41222381591796875 loss_r 0.33938372135162354 loss_kl 0.07284007966518402\n",
      "step 257 loss 0.37445300817489624 loss_r 0.3038846552371979 loss_kl 0.07056833803653717\n",
      "step 258 loss 0.37450578808784485 loss_r 0.28437644243240356 loss_kl 0.09012933820486069\n",
      "step 259 loss 0.4374353289604187 loss_r 0.37657904624938965 loss_kl 0.06085629388689995\n",
      "step 260 loss 0.38443076610565186 loss_r 0.3111485242843628 loss_kl 0.07328224182128906\n",
      "step 261 loss 0.3828498125076294 loss_r 0.30760881304740906 loss_kl 0.07524098455905914\n",
      "step 262 loss 0.3904350697994232 loss_r 0.330403208732605 loss_kl 0.060031864792108536\n",
      "step 263 loss 0.3662761449813843 loss_r 0.30864065885543823 loss_kl 0.05763547495007515\n",
      "step 264 loss 0.4356648027896881 loss_r 0.3760514557361603 loss_kl 0.05961335450410843\n",
      "step 265 loss 0.4564024806022644 loss_r 0.3999926447868347 loss_kl 0.05640982836484909\n",
      "step 266 loss 0.40972989797592163 loss_r 0.3444763422012329 loss_kl 0.06525357067584991\n",
      "step 267 loss 0.3796345293521881 loss_r 0.3054453134536743 loss_kl 0.0741892084479332\n",
      "step 268 loss 0.3777649700641632 loss_r 0.2876051664352417 loss_kl 0.09015980362892151\n",
      "step 269 loss 0.3983553946018219 loss_r 0.32933109998703003 loss_kl 0.06902430206537247\n",
      "step 270 loss 0.4215332865715027 loss_r 0.35572341084480286 loss_kl 0.06580986082553864\n",
      "step 271 loss 0.3966928720474243 loss_r 0.3310088515281677 loss_kl 0.06568402796983719\n",
      "step 272 loss 0.38626474142074585 loss_r 0.31500670313835144 loss_kl 0.07125802338123322\n",
      "step 273 loss 0.3807244896888733 loss_r 0.3108350932598114 loss_kl 0.0698893815279007\n",
      "step 274 loss 0.37130972743034363 loss_r 0.30103880167007446 loss_kl 0.07027091830968857\n",
      "step 275 loss 0.35428059101104736 loss_r 0.2665165662765503 loss_kl 0.08776402473449707\n",
      "step 276 loss 0.3962962031364441 loss_r 0.3336081802845001 loss_kl 0.06268802285194397\n",
      "step 277 loss 0.37579813599586487 loss_r 0.31618228554725647 loss_kl 0.059615857899188995\n",
      "step 278 loss 0.39050692319869995 loss_r 0.33662086725234985 loss_kl 0.0538860484957695\n",
      "step 279 loss 0.40677589178085327 loss_r 0.3447248935699463 loss_kl 0.062051013112068176\n",
      "step 280 loss 0.339391827583313 loss_r 0.26081836223602295 loss_kl 0.07857346534729004\n",
      "step 281 loss 0.4096637964248657 loss_r 0.3523808419704437 loss_kl 0.057282954454422\n",
      "step 282 loss 0.372122585773468 loss_r 0.30402109026908875 loss_kl 0.06810150295495987\n",
      "step 283 loss 0.4221484363079071 loss_r 0.3630988597869873 loss_kl 0.0590495839715004\n",
      "step 284 loss 0.355965793132782 loss_r 0.29322347044944763 loss_kl 0.06274233758449554\n",
      "step 285 loss 0.4223303198814392 loss_r 0.3635583817958832 loss_kl 0.058771952986717224\n",
      "step 286 loss 0.3834654688835144 loss_r 0.3042951226234436 loss_kl 0.0791703388094902\n",
      "step 287 loss 0.39486828446388245 loss_r 0.331039696931839 loss_kl 0.06382858753204346\n",
      "step 288 loss 0.41454383730888367 loss_r 0.35125887393951416 loss_kl 0.0632849633693695\n",
      "step 289 loss 0.35747507214546204 loss_r 0.2909776270389557 loss_kl 0.06649744510650635\n",
      "step 290 loss 0.3392111361026764 loss_r 0.26302671432495117 loss_kl 0.07618441432714462\n",
      "step 291 loss 0.3701772093772888 loss_r 0.30179494619369507 loss_kl 0.06838227063417435\n",
      "step 292 loss 0.36085259914398193 loss_r 0.2977236807346344 loss_kl 0.06312893331050873\n",
      "step 293 loss 0.36639365553855896 loss_r 0.302556574344635 loss_kl 0.06383708864450455\n",
      "step 294 loss 0.34782320261001587 loss_r 0.28602221608161926 loss_kl 0.061800986528396606\n",
      "step 295 loss 0.38973748683929443 loss_r 0.33628562092781067 loss_kl 0.05345188081264496\n",
      "step 296 loss 0.34154245257377625 loss_r 0.283691942691803 loss_kl 0.057850517332553864\n",
      "step 297 loss 0.39242106676101685 loss_r 0.33981141448020935 loss_kl 0.0526096485555172\n",
      "step 298 loss 0.3936089277267456 loss_r 0.33999890089035034 loss_kl 0.05361003801226616\n",
      "step 299 loss 0.3684641718864441 loss_r 0.3036815822124481 loss_kl 0.06478258967399597\n",
      "step 300 loss 0.3719226121902466 loss_r 0.29665568470954895 loss_kl 0.07526694238185883\n",
      "step 301 loss 0.37599533796310425 loss_r 0.3100226819515228 loss_kl 0.06597267091274261\n",
      "step 302 loss 0.3831680119037628 loss_r 0.31533926725387573 loss_kl 0.06782874464988708\n",
      "step 303 loss 0.3838353455066681 loss_r 0.3265379071235657 loss_kl 0.05729743838310242\n",
      "step 304 loss 0.3779204487800598 loss_r 0.32151007652282715 loss_kl 0.056410372257232666\n",
      "step 305 loss 0.40725404024124146 loss_r 0.3536757230758667 loss_kl 0.05357831344008446\n",
      "step 306 loss 0.41851532459259033 loss_r 0.3542752265930176 loss_kl 0.06424009799957275\n",
      "step 307 loss 0.37283432483673096 loss_r 0.3131471872329712 loss_kl 0.05968712270259857\n",
      "step 308 loss 0.3424670696258545 loss_r 0.27114617824554443 loss_kl 0.07132089883089066\n",
      "step 309 loss 0.3470301926136017 loss_r 0.2695752680301666 loss_kl 0.07745492458343506\n",
      "step 310 loss 0.37276166677474976 loss_r 0.30718880891799927 loss_kl 0.06557287275791168\n",
      "step 311 loss 0.3524732291698456 loss_r 0.28003668785095215 loss_kl 0.07243654876947403\n",
      "step 312 loss 0.4083750247955322 loss_r 0.35504621267318726 loss_kl 0.05332881957292557\n",
      "step 313 loss 0.35441961884498596 loss_r 0.2897592782974243 loss_kl 0.06466034054756165\n",
      "step 314 loss 0.3613280653953552 loss_r 0.30718138813972473 loss_kl 0.054146673530340195\n",
      "step 315 loss 0.36492565274238586 loss_r 0.310313880443573 loss_kl 0.054611772298812866\n",
      "step 316 loss 0.37387600541114807 loss_r 0.3227728605270386 loss_kl 0.0511031374335289\n",
      "step 317 loss 0.36435046792030334 loss_r 0.31637486815452576 loss_kl 0.04797559231519699\n",
      "step 318 loss 0.37325742840766907 loss_r 0.32072120904922485 loss_kl 0.05253620818257332\n",
      "step 319 loss 0.35674938559532166 loss_r 0.2909032702445984 loss_kl 0.06584611535072327\n",
      "step 320 loss 0.401048868894577 loss_r 0.3420248329639435 loss_kl 0.05902404338121414\n",
      "step 321 loss 0.3624706566333771 loss_r 0.2957633435726166 loss_kl 0.0667073130607605\n",
      "step 322 loss 0.42198747396469116 loss_r 0.3605228662490845 loss_kl 0.06146461144089699\n",
      "step 323 loss 0.4298715591430664 loss_r 0.3741835057735443 loss_kl 0.055688053369522095\n",
      "step 324 loss 0.3599846661090851 loss_r 0.29671767354011536 loss_kl 0.06326700001955032\n",
      "step 325 loss 0.3289650082588196 loss_r 0.25453662872314453 loss_kl 0.07442837953567505\n",
      "step 326 loss 0.34706729650497437 loss_r 0.28451746702194214 loss_kl 0.06254981458187103\n",
      "step 327 loss 0.3497028946876526 loss_r 0.28440427780151367 loss_kl 0.06529861688613892\n",
      "step 328 loss 0.37138235569000244 loss_r 0.3193824589252472 loss_kl 0.05199989676475525\n",
      "step 329 loss 0.3267243504524231 loss_r 0.2567111849784851 loss_kl 0.0700131505727768\n",
      "step 330 loss 0.39480751752853394 loss_r 0.34464526176452637 loss_kl 0.05016224831342697\n",
      "step 331 loss 0.3463187515735626 loss_r 0.29191553592681885 loss_kl 0.05440322682261467\n",
      "step 332 loss 0.36454546451568604 loss_r 0.3120791018009186 loss_kl 0.05246637761592865\n",
      "step 333 loss 0.31987977027893066 loss_r 0.2622224986553192 loss_kl 0.057657256722450256\n",
      "step 334 loss 0.40125682950019836 loss_r 0.35674020648002625 loss_kl 0.044516630470752716\n",
      "step 335 loss 0.3247026801109314 loss_r 0.25322940945625305 loss_kl 0.07147327810525894\n",
      "step 336 loss 0.36725547909736633 loss_r 0.31491148471832275 loss_kl 0.052344001829624176\n",
      "step 337 loss 0.37724196910858154 loss_r 0.32008472084999084 loss_kl 0.057157259434461594\n",
      "step 338 loss 0.3519314229488373 loss_r 0.2860967218875885 loss_kl 0.06583470106124878\n",
      "step 339 loss 0.34186220169067383 loss_r 0.2583175301551819 loss_kl 0.08354467153549194\n",
      "step 340 loss 0.34735965728759766 loss_r 0.28596732020378113 loss_kl 0.06139235198497772\n",
      "step 341 loss 0.35876715183258057 loss_r 0.3009899854660034 loss_kl 0.05777715891599655\n",
      "step 342 loss 0.35414862632751465 loss_r 0.28912585973739624 loss_kl 0.06502276659011841\n",
      "step 343 loss 0.4544821083545685 loss_r 0.40719059109687805 loss_kl 0.04729152098298073\n",
      "step 344 loss 0.37391096353530884 loss_r 0.29968154430389404 loss_kl 0.07422943413257599\n",
      "step 345 loss 0.34181439876556396 loss_r 0.2884981632232666 loss_kl 0.05331623554229736\n",
      "step 346 loss 0.37946948409080505 loss_r 0.32537519931793213 loss_kl 0.05409429222345352\n",
      "step 347 loss 0.33908313512802124 loss_r 0.2832195460796356 loss_kl 0.05586359277367592\n",
      "step 348 loss 0.381128191947937 loss_r 0.32294249534606934 loss_kl 0.05818570777773857\n",
      "step 349 loss 0.30465376377105713 loss_r 0.2366860806941986 loss_kl 0.06796768307685852\n",
      "step 350 loss 0.3903968334197998 loss_r 0.33861783146858215 loss_kl 0.05177900195121765\n",
      "step 351 loss 0.3345511853694916 loss_r 0.27313143014907837 loss_kl 0.06141975522041321\n",
      "step 352 loss 0.4319515824317932 loss_r 0.3776669502258301 loss_kl 0.054284632205963135\n",
      "step 353 loss 0.39988914132118225 loss_r 0.3478275239467621 loss_kl 0.052061617374420166\n",
      "step 354 loss 0.37261420488357544 loss_r 0.31599316000938416 loss_kl 0.05662105977535248\n",
      "step 355 loss 0.3330164849758148 loss_r 0.27185720205307007 loss_kl 0.06115927919745445\n",
      "step 356 loss 0.3288150429725647 loss_r 0.2658584415912628 loss_kl 0.06295658648014069\n",
      "step 357 loss 0.40180501341819763 loss_r 0.3511168360710144 loss_kl 0.05068817362189293\n",
      "step 358 loss 0.3301224708557129 loss_r 0.2649468779563904 loss_kl 0.06517557799816132\n",
      "step 359 loss 0.37554264068603516 loss_r 0.3198888301849365 loss_kl 0.055653806775808334\n",
      "step 360 loss 0.3304945230484009 loss_r 0.27708926796913147 loss_kl 0.053405262529850006\n",
      "step 361 loss 0.3356522023677826 loss_r 0.2705058157444 loss_kl 0.06514638662338257\n",
      "step 362 loss 0.3267733156681061 loss_r 0.27541157603263855 loss_kl 0.05136174336075783\n",
      "step 363 loss 0.33732283115386963 loss_r 0.2861884534358978 loss_kl 0.0511343851685524\n",
      "step 364 loss 0.3430488109588623 loss_r 0.2952781915664673 loss_kl 0.04777061194181442\n",
      "step 365 loss 0.33445900678634644 loss_r 0.2829360067844391 loss_kl 0.05152300372719765\n",
      "step 366 loss 0.44629228115081787 loss_r 0.4035782217979431 loss_kl 0.04271407425403595\n",
      "step 367 loss 0.3686671853065491 loss_r 0.31604865193367004 loss_kl 0.052618540823459625\n",
      "step 368 loss 0.37415778636932373 loss_r 0.3203760087490082 loss_kl 0.053781792521476746\n",
      "step 369 loss 0.3439851999282837 loss_r 0.28175443410873413 loss_kl 0.06223077327013016\n",
      "step 370 loss 0.3875216841697693 loss_r 0.32413095235824585 loss_kl 0.06339072436094284\n",
      "step 371 loss 0.33823254704475403 loss_r 0.27156925201416016 loss_kl 0.06666329503059387\n",
      "step 372 loss 0.3448127508163452 loss_r 0.28260818123817444 loss_kl 0.062204573303461075\n",
      "step 373 loss 0.41662395000457764 loss_r 0.36834102869033813 loss_kl 0.04828290641307831\n",
      "step 374 loss 0.3832134008407593 loss_r 0.3332410752773285 loss_kl 0.04997231438755989\n",
      "step 375 loss 0.3775789737701416 loss_r 0.33166080713272095 loss_kl 0.04591818153858185\n",
      "step 376 loss 0.35842859745025635 loss_r 0.3010881543159485 loss_kl 0.057340458035469055\n",
      "step 377 loss 0.3309803903102875 loss_r 0.25888535380363464 loss_kl 0.07209503650665283\n",
      "step 378 loss 0.3517467975616455 loss_r 0.2972225248813629 loss_kl 0.054524265229701996\n",
      "step 379 loss 0.3230767250061035 loss_r 0.26104041934013367 loss_kl 0.062036313116550446\n",
      "step 380 loss 0.3121306300163269 loss_r 0.2553183436393738 loss_kl 0.05681227147579193\n",
      "step 381 loss 0.38059499859809875 loss_r 0.3353620171546936 loss_kl 0.04523298144340515\n",
      "step 382 loss 0.37812384963035583 loss_r 0.3227638900279999 loss_kl 0.05535994842648506\n",
      "step 383 loss 0.30657240748405457 loss_r 0.24790790677070618 loss_kl 0.058664511889219284\n",
      "step 384 loss 0.3577149510383606 loss_r 0.2985367178916931 loss_kl 0.059178225696086884\n",
      "step 385 loss 0.3064609169960022 loss_r 0.24567919969558716 loss_kl 0.06078171730041504\n",
      "step 386 loss 0.3380469083786011 loss_r 0.28686898946762085 loss_kl 0.05117792636156082\n",
      "step 387 loss 0.33029258251190186 loss_r 0.27377617359161377 loss_kl 0.05651640146970749\n",
      "step 388 loss 0.34195804595947266 loss_r 0.2811644375324249 loss_kl 0.06079362332820892\n",
      "step 389 loss 0.2936391830444336 loss_r 0.2299257069826126 loss_kl 0.06371349096298218\n",
      "step 390 loss 0.335325688123703 loss_r 0.28118354082107544 loss_kl 0.05414213612675667\n",
      "step 391 loss 0.32787761092185974 loss_r 0.27036163210868835 loss_kl 0.057515986263751984\n",
      "step 392 loss 0.3458411693572998 loss_r 0.2986137270927429 loss_kl 0.047227438539266586\n",
      "step 393 loss 0.373661071062088 loss_r 0.32426899671554565 loss_kl 0.04939207062125206\n",
      "step 394 loss 0.3203660249710083 loss_r 0.27303722500801086 loss_kl 0.04732879623770714\n",
      "step 395 loss 0.35618671774864197 loss_r 0.3064945936203003 loss_kl 0.04969213157892227\n",
      "step 396 loss 0.34113702178001404 loss_r 0.2935140132904053 loss_kl 0.047623008489608765\n",
      "step 397 loss 0.2853468656539917 loss_r 0.23195116221904755 loss_kl 0.05339568853378296\n",
      "step 398 loss 0.31996890902519226 loss_r 0.25570401549339294 loss_kl 0.06426489353179932\n",
      "Epoch 1\n",
      "step 0 loss 0.339141309261322 loss_r 0.2900654077529907 loss_kl 0.04907591640949249\n",
      "step 1 loss 0.2825784385204315 loss_r 0.2315782755613327 loss_kl 0.05100015550851822\n",
      "step 2 loss 0.37610772252082825 loss_r 0.3303739130496979 loss_kl 0.04573381319642067\n",
      "step 3 loss 0.3370465934276581 loss_r 0.2863287031650543 loss_kl 0.05071788281202316\n",
      "step 4 loss 0.39321526885032654 loss_r 0.3455643653869629 loss_kl 0.04765090346336365\n",
      "step 5 loss 0.3458987772464752 loss_r 0.2815881371498108 loss_kl 0.06431063264608383\n",
      "step 6 loss 0.37065020203590393 loss_r 0.3178007900714874 loss_kl 0.052849411964416504\n",
      "step 7 loss 0.4463708698749542 loss_r 0.39588260650634766 loss_kl 0.05048826336860657\n",
      "step 8 loss 0.3044081926345825 loss_r 0.2226155698299408 loss_kl 0.08179263770580292\n",
      "step 9 loss 0.3223941922187805 loss_r 0.25044989585876465 loss_kl 0.07194429636001587\n",
      "step 10 loss 0.39155805110931396 loss_r 0.33814147114753723 loss_kl 0.05341658368706703\n",
      "step 11 loss 0.40485119819641113 loss_r 0.349254310131073 loss_kl 0.05559687316417694\n",
      "step 12 loss 0.3190915882587433 loss_r 0.25431695580482483 loss_kl 0.06477463245391846\n",
      "step 13 loss 0.38015663623809814 loss_r 0.3291453421115875 loss_kl 0.05101130157709122\n",
      "step 14 loss 0.2841324508190155 loss_r 0.2330828756093979 loss_kl 0.051049575209617615\n",
      "step 15 loss 0.31801992654800415 loss_r 0.2591474950313568 loss_kl 0.05887244641780853\n",
      "step 16 loss 0.3258952796459198 loss_r 0.27010536193847656 loss_kl 0.055789925158023834\n",
      "step 17 loss 0.3311326801776886 loss_r 0.2838927209377289 loss_kl 0.04723994806408882\n",
      "step 18 loss 0.3485347032546997 loss_r 0.30336838960647583 loss_kl 0.04516630619764328\n",
      "step 19 loss 0.3325495719909668 loss_r 0.2846163511276245 loss_kl 0.04793320596218109\n",
      "step 20 loss 0.32883670926094055 loss_r 0.2789214849472046 loss_kl 0.049915216863155365\n",
      "step 21 loss 0.38640978932380676 loss_r 0.3432604670524597 loss_kl 0.043149322271347046\n",
      "step 22 loss 0.2676657736301422 loss_r 0.20993751287460327 loss_kl 0.05772826075553894\n",
      "step 23 loss 0.3164648115634918 loss_r 0.2739630937576294 loss_kl 0.042501725256443024\n",
      "step 24 loss 0.37151989340782166 loss_r 0.32750433683395386 loss_kl 0.044015564024448395\n",
      "step 25 loss 0.29829296469688416 loss_r 0.24205607175827026 loss_kl 0.056236885488033295\n",
      "step 26 loss 0.35911133885383606 loss_r 0.31318676471710205 loss_kl 0.04592456668615341\n",
      "step 27 loss 0.33411407470703125 loss_r 0.2841227352619171 loss_kl 0.04999134689569473\n",
      "step 28 loss 0.34860000014305115 loss_r 0.29882749915122986 loss_kl 0.04977250099182129\n",
      "step 29 loss 0.3433253765106201 loss_r 0.2909010052680969 loss_kl 0.05242438614368439\n",
      "step 30 loss 0.3115551769733429 loss_r 0.24727651476860046 loss_kl 0.06427865475416183\n",
      "step 31 loss 0.3203155994415283 loss_r 0.2599354684352875 loss_kl 0.06038014590740204\n",
      "step 32 loss 0.3160727322101593 loss_r 0.25895655155181885 loss_kl 0.05711617320775986\n",
      "step 33 loss 0.345973938703537 loss_r 0.297899454832077 loss_kl 0.048074495047330856\n",
      "step 34 loss 0.34951668977737427 loss_r 0.2878272235393524 loss_kl 0.06168947368860245\n",
      "step 35 loss 0.36410456895828247 loss_r 0.3193597197532654 loss_kl 0.04474484920501709\n",
      "step 36 loss 0.3123164772987366 loss_r 0.25975024700164795 loss_kl 0.05256621539592743\n",
      "step 37 loss 0.3254241645336151 loss_r 0.2790243625640869 loss_kl 0.0463997907936573\n",
      "step 38 loss 0.28926581144332886 loss_r 0.2385473996400833 loss_kl 0.05071839690208435\n",
      "step 39 loss 0.31250160932540894 loss_r 0.26392990350723267 loss_kl 0.04857172071933746\n",
      "step 40 loss 0.28367671370506287 loss_r 0.22901523113250732 loss_kl 0.05466149374842644\n",
      "step 41 loss 0.36333850026130676 loss_r 0.32299259305000305 loss_kl 0.040345899760723114\n",
      "step 42 loss 0.29781830310821533 loss_r 0.23811322450637817 loss_kl 0.05970509350299835\n",
      "step 43 loss 0.37697774171829224 loss_r 0.33531203866004944 loss_kl 0.041665710508823395\n",
      "step 44 loss 0.31749969720840454 loss_r 0.27279946208000183 loss_kl 0.04470024257898331\n",
      "step 45 loss 0.35975968837738037 loss_r 0.31621113419532776 loss_kl 0.043548546731472015\n",
      "step 46 loss 0.2697645127773285 loss_r 0.2133069783449173 loss_kl 0.05645753815770149\n",
      "step 47 loss 0.34194597601890564 loss_r 0.2888844907283783 loss_kl 0.05306147411465645\n",
      "step 48 loss 0.3190344274044037 loss_r 0.26161348819732666 loss_kl 0.05742093548178673\n",
      "step 49 loss 0.37409302592277527 loss_r 0.3265034854412079 loss_kl 0.047589533030986786\n",
      "step 50 loss 0.3033408224582672 loss_r 0.24545134603977203 loss_kl 0.05788946896791458\n",
      "step 51 loss 0.33294573426246643 loss_r 0.2849270701408386 loss_kl 0.04801865667104721\n",
      "step 52 loss 0.3032185137271881 loss_r 0.25284114480018616 loss_kl 0.05037737637758255\n",
      "step 53 loss 0.3379252552986145 loss_r 0.2918471097946167 loss_kl 0.046078138053417206\n",
      "step 54 loss 0.37056177854537964 loss_r 0.3297000527381897 loss_kl 0.04086171090602875\n",
      "step 55 loss 0.32853853702545166 loss_r 0.2808336913585663 loss_kl 0.04770484194159508\n",
      "step 56 loss 0.31308743357658386 loss_r 0.26100099086761475 loss_kl 0.05208643525838852\n",
      "step 57 loss 0.3333047926425934 loss_r 0.2874748110771179 loss_kl 0.04582998901605606\n",
      "step 58 loss 0.31139737367630005 loss_r 0.25453421473503113 loss_kl 0.056863151490688324\n",
      "step 59 loss 0.30622386932373047 loss_r 0.24865718185901642 loss_kl 0.057566676288843155\n",
      "step 60 loss 0.3489278554916382 loss_r 0.30274710059165955 loss_kl 0.04618076980113983\n",
      "step 61 loss 0.3647308051586151 loss_r 0.31917324662208557 loss_kl 0.04555756598711014\n",
      "step 62 loss 0.35424062609672546 loss_r 0.3089887499809265 loss_kl 0.04525186866521835\n",
      "step 63 loss 0.2696492373943329 loss_r 0.21658867597579956 loss_kl 0.05306055396795273\n",
      "step 64 loss 0.3036271035671234 loss_r 0.24228259921073914 loss_kl 0.061344511806964874\n",
      "step 65 loss 0.3280065059661865 loss_r 0.28598061203956604 loss_kl 0.04202587902545929\n",
      "step 66 loss 0.29683592915534973 loss_r 0.2510286569595337 loss_kl 0.04580727219581604\n",
      "step 67 loss 0.3598065674304962 loss_r 0.31820204854011536 loss_kl 0.04160452261567116\n",
      "step 68 loss 0.30644482374191284 loss_r 0.26077908277511597 loss_kl 0.04566574841737747\n",
      "step 69 loss 0.2807934880256653 loss_r 0.23005381226539612 loss_kl 0.050739679485559464\n",
      "step 70 loss 0.33849555253982544 loss_r 0.2902001738548279 loss_kl 0.04829537868499756\n",
      "step 71 loss 0.2875729203224182 loss_r 0.23279859125614166 loss_kl 0.05477432534098625\n",
      "step 72 loss 0.3903800845146179 loss_r 0.35059070587158203 loss_kl 0.03978938236832619\n",
      "step 73 loss 0.29228755831718445 loss_r 0.24898503720760345 loss_kl 0.04330252483487129\n",
      "step 74 loss 0.31723687052726746 loss_r 0.2679065465927124 loss_kl 0.04933033138513565\n",
      "step 75 loss 0.3532516360282898 loss_r 0.30753037333488464 loss_kl 0.04572127014398575\n",
      "step 76 loss 0.33451297879219055 loss_r 0.28107768297195435 loss_kl 0.0534353032708168\n",
      "step 77 loss 0.3260771334171295 loss_r 0.27620866894721985 loss_kl 0.049868471920490265\n",
      "step 78 loss 0.3135932981967926 loss_r 0.26677393913269043 loss_kl 0.046819351613521576\n",
      "step 79 loss 0.30641692876815796 loss_r 0.2524760365486145 loss_kl 0.05394090712070465\n",
      "step 80 loss 0.30158036947250366 loss_r 0.25339266657829285 loss_kl 0.04818771779537201\n",
      "step 81 loss 0.2932524085044861 loss_r 0.23505564033985138 loss_kl 0.0581967756152153\n",
      "step 82 loss 0.32357561588287354 loss_r 0.27940255403518677 loss_kl 0.04417305439710617\n",
      "step 83 loss 0.3123328983783722 loss_r 0.26521316170692444 loss_kl 0.04711974412202835\n",
      "step 84 loss 0.2715338468551636 loss_r 0.22205401957035065 loss_kl 0.049479831010103226\n",
      "step 85 loss 0.273980975151062 loss_r 0.22219213843345642 loss_kl 0.05178883671760559\n",
      "step 86 loss 0.43448659777641296 loss_r 0.3992401361465454 loss_kl 0.03524645045399666\n",
      "step 87 loss 0.30593472719192505 loss_r 0.26775801181793213 loss_kl 0.03817672282457352\n",
      "step 88 loss 0.3584267497062683 loss_r 0.3186193108558655 loss_kl 0.039807431399822235\n",
      "step 89 loss 0.3805820047855377 loss_r 0.3358151912689209 loss_kl 0.044766806066036224\n",
      "step 90 loss 0.34020376205444336 loss_r 0.28158849477767944 loss_kl 0.05861527472734451\n",
      "step 91 loss 0.38332152366638184 loss_r 0.3188277781009674 loss_kl 0.06449376046657562\n",
      "step 92 loss 0.36563771963119507 loss_r 0.3062812089920044 loss_kl 0.059356510639190674\n",
      "step 93 loss 0.32048898935317993 loss_r 0.2517205774784088 loss_kl 0.06876840442419052\n",
      "step 94 loss 0.32723745703697205 loss_r 0.2593907415866852 loss_kl 0.06784672290086746\n",
      "step 95 loss 0.3249586224555969 loss_r 0.2637285888195038 loss_kl 0.06123003736138344\n",
      "step 96 loss 0.2902787923812866 loss_r 0.22057555615901947 loss_kl 0.06970325112342834\n",
      "step 97 loss 0.30816471576690674 loss_r 0.2502192556858063 loss_kl 0.05794544517993927\n",
      "step 98 loss 0.3327326774597168 loss_r 0.290181040763855 loss_kl 0.042551636695861816\n",
      "step 99 loss 0.30179765820503235 loss_r 0.2593885064125061 loss_kl 0.04240915924310684\n",
      "step 100 loss 0.3154444396495819 loss_r 0.27284008264541626 loss_kl 0.042604364454746246\n",
      "step 101 loss 0.27306997776031494 loss_r 0.22767512500286102 loss_kl 0.04539485275745392\n",
      "step 102 loss 0.3246188163757324 loss_r 0.28240180015563965 loss_kl 0.04221703112125397\n",
      "step 103 loss 0.35841259360313416 loss_r 0.3179294764995575 loss_kl 0.04048312455415726\n",
      "step 104 loss 0.30990368127822876 loss_r 0.26438698172569275 loss_kl 0.04551668465137482\n",
      "step 105 loss 0.29339203238487244 loss_r 0.23914560675621033 loss_kl 0.05424641817808151\n",
      "step 106 loss 0.3259834051132202 loss_r 0.2694801390171051 loss_kl 0.05650325119495392\n",
      "step 107 loss 0.29183724522590637 loss_r 0.2323884814977646 loss_kl 0.059448760002851486\n",
      "step 108 loss 0.25403735041618347 loss_r 0.19453679025173187 loss_kl 0.0595005601644516\n",
      "step 109 loss 0.33173036575317383 loss_r 0.2812446653842926 loss_kl 0.05048569291830063\n",
      "step 110 loss 0.3539009392261505 loss_r 0.3045828938484192 loss_kl 0.049318041652441025\n",
      "step 111 loss 0.27448731660842896 loss_r 0.22016017138957977 loss_kl 0.05432714521884918\n",
      "step 112 loss 0.33180034160614014 loss_r 0.2869437634944916 loss_kl 0.044856563210487366\n",
      "step 113 loss 0.30657580494880676 loss_r 0.2557501792907715 loss_kl 0.05082562193274498\n",
      "step 114 loss 0.3405713438987732 loss_r 0.28520840406417847 loss_kl 0.055362943559885025\n",
      "step 115 loss 0.3129085898399353 loss_r 0.2623254954814911 loss_kl 0.05058308690786362\n",
      "step 116 loss 0.3078257739543915 loss_r 0.2567295730113983 loss_kl 0.05109621211886406\n",
      "step 117 loss 0.3018225431442261 loss_r 0.2429751455783844 loss_kl 0.05884740129113197\n",
      "step 118 loss 0.33792614936828613 loss_r 0.29239675402641296 loss_kl 0.045529402792453766\n",
      "step 119 loss 0.30405306816101074 loss_r 0.25667986273765564 loss_kl 0.0473732054233551\n",
      "step 120 loss 0.2718561887741089 loss_r 0.21573875844478607 loss_kl 0.05611743405461311\n",
      "step 121 loss 0.2779664993286133 loss_r 0.22652624547481537 loss_kl 0.05144026130437851\n",
      "step 122 loss 0.3055938482284546 loss_r 0.26071301102638245 loss_kl 0.044880837202072144\n",
      "step 123 loss 0.2787976861000061 loss_r 0.2297593504190445 loss_kl 0.0490383505821228\n",
      "step 124 loss 0.3266928791999817 loss_r 0.2844432592391968 loss_kl 0.042249612510204315\n",
      "step 125 loss 0.28968700766563416 loss_r 0.24621818959712982 loss_kl 0.043468818068504333\n",
      "step 126 loss 0.2869979739189148 loss_r 0.2353675663471222 loss_kl 0.051630422472953796\n",
      "step 127 loss 0.31931933760643005 loss_r 0.2723774015903473 loss_kl 0.046941936016082764\n",
      "step 128 loss 0.2692926228046417 loss_r 0.21117962896823883 loss_kl 0.05811299383640289\n",
      "step 129 loss 0.26466768980026245 loss_r 0.2026621252298355 loss_kl 0.06200556457042694\n",
      "step 130 loss 0.28447556495666504 loss_r 0.2329993098974228 loss_kl 0.05147625878453255\n",
      "step 131 loss 0.28618866205215454 loss_r 0.23352955281734467 loss_kl 0.052659109234809875\n",
      "step 132 loss 0.3159784972667694 loss_r 0.2671283185482025 loss_kl 0.0488501712679863\n",
      "step 133 loss 0.4003235101699829 loss_r 0.36402004957199097 loss_kl 0.036303456872701645\n",
      "step 134 loss 0.33030903339385986 loss_r 0.28733718395233154 loss_kl 0.04297184199094772\n",
      "step 135 loss 0.2930961847305298 loss_r 0.23202170431613922 loss_kl 0.061074480414390564\n",
      "step 136 loss 0.27689602971076965 loss_r 0.21936987340450287 loss_kl 0.05752614885568619\n",
      "step 137 loss 0.3606249690055847 loss_r 0.2990961968898773 loss_kl 0.0615287683904171\n",
      "step 138 loss 0.32184749841690063 loss_r 0.2339201271533966 loss_kl 0.08792737126350403\n",
      "step 139 loss 0.2771601676940918 loss_r 0.21121355891227722 loss_kl 0.06594659388065338\n",
      "step 140 loss 0.3136584460735321 loss_r 0.26004472374916077 loss_kl 0.05361371487379074\n",
      "step 141 loss 0.31753674149513245 loss_r 0.2630733251571655 loss_kl 0.054463423788547516\n",
      "step 142 loss 0.279428631067276 loss_r 0.22998058795928955 loss_kl 0.04944804310798645\n",
      "step 143 loss 0.26004207134246826 loss_r 0.20347824692726135 loss_kl 0.056563835591077805\n",
      "step 144 loss 0.3029487729072571 loss_r 0.26281729340553284 loss_kl 0.040131472051143646\n",
      "step 145 loss 0.28409525752067566 loss_r 0.24011948704719543 loss_kl 0.043975766748189926\n",
      "step 146 loss 0.23226121068000793 loss_r 0.18524879217147827 loss_kl 0.04701241850852966\n",
      "step 147 loss 0.39852628111839294 loss_r 0.36082300543785095 loss_kl 0.03770327940583229\n",
      "step 148 loss 0.2657487988471985 loss_r 0.20832976698875427 loss_kl 0.05741902440786362\n",
      "step 149 loss 0.31012803316116333 loss_r 0.2557612657546997 loss_kl 0.05436675250530243\n",
      "step 150 loss 0.29071226716041565 loss_r 0.23706687986850739 loss_kl 0.05364537984132767\n",
      "step 151 loss 0.3523530960083008 loss_r 0.30139681696891785 loss_kl 0.05095629394054413\n",
      "step 152 loss 0.32424265146255493 loss_r 0.27307966351509094 loss_kl 0.05116298794746399\n",
      "step 153 loss 0.318572998046875 loss_r 0.26563942432403564 loss_kl 0.05293356254696846\n",
      "step 154 loss 0.25460031628608704 loss_r 0.19439177215099335 loss_kl 0.060208551585674286\n",
      "step 155 loss 0.2762663960456848 loss_r 0.2275000959634781 loss_kl 0.04876629263162613\n",
      "step 156 loss 0.2832636535167694 loss_r 0.23506101965904236 loss_kl 0.04820263385772705\n",
      "step 157 loss 0.2739751636981964 loss_r 0.22454790771007538 loss_kl 0.049427248537540436\n",
      "step 158 loss 0.31296759843826294 loss_r 0.2599124610424042 loss_kl 0.05305512621998787\n",
      "step 159 loss 0.2773677706718445 loss_r 0.22519025206565857 loss_kl 0.05217752605676651\n",
      "step 160 loss 0.27608099579811096 loss_r 0.2279561460018158 loss_kl 0.04812486097216606\n",
      "step 161 loss 0.29564717411994934 loss_r 0.2514477074146271 loss_kl 0.04419947788119316\n",
      "step 162 loss 0.2900317311286926 loss_r 0.24013414978981018 loss_kl 0.04989759624004364\n",
      "step 163 loss 0.2333836853504181 loss_r 0.1755688637495041 loss_kl 0.0578148290514946\n",
      "step 164 loss 0.25470930337905884 loss_r 0.203131303191185 loss_kl 0.05157799273729324\n",
      "step 165 loss 0.32530462741851807 loss_r 0.2817440927028656 loss_kl 0.04356051981449127\n",
      "step 166 loss 0.22871962189674377 loss_r 0.17689642310142517 loss_kl 0.05182319134473801\n",
      "step 167 loss 0.3383335471153259 loss_r 0.2932099997997284 loss_kl 0.04512355849146843\n",
      "step 168 loss 0.31259685754776 loss_r 0.26448869705200195 loss_kl 0.04810814559459686\n",
      "step 169 loss 0.31381139159202576 loss_r 0.26060450077056885 loss_kl 0.05320689082145691\n",
      "step 170 loss 0.2918778657913208 loss_r 0.2365352064371109 loss_kl 0.0553426630795002\n",
      "step 171 loss 0.28905513882637024 loss_r 0.2368239164352417 loss_kl 0.05223121866583824\n",
      "step 172 loss 0.3227531909942627 loss_r 0.27637791633605957 loss_kl 0.04637525975704193\n",
      "step 173 loss 0.33127111196517944 loss_r 0.2718053460121155 loss_kl 0.059465765953063965\n",
      "step 174 loss 0.32432013750076294 loss_r 0.2554100751876831 loss_kl 0.06891004741191864\n",
      "step 175 loss 0.2767528295516968 loss_r 0.21527943015098572 loss_kl 0.06147340312600136\n",
      "step 176 loss 0.29429885745048523 loss_r 0.2376747578382492 loss_kl 0.056624092161655426\n",
      "step 177 loss 0.26499810814857483 loss_r 0.20305635035037994 loss_kl 0.06194175034761429\n",
      "step 178 loss 0.24651387333869934 loss_r 0.18474744260311127 loss_kl 0.061766427010297775\n",
      "step 179 loss 0.31620559096336365 loss_r 0.27100998163223267 loss_kl 0.04519560933113098\n",
      "step 180 loss 0.3812409043312073 loss_r 0.34104135632514954 loss_kl 0.040199533104896545\n",
      "step 181 loss 0.31054720282554626 loss_r 0.26090800762176514 loss_kl 0.049639198929071426\n",
      "step 182 loss 0.23465827107429504 loss_r 0.18206705152988434 loss_kl 0.0525912269949913\n",
      "step 183 loss 0.3138364553451538 loss_r 0.2732579708099365 loss_kl 0.040578484535217285\n",
      "step 184 loss 0.24143142998218536 loss_r 0.19483748078346252 loss_kl 0.04659394919872284\n",
      "step 185 loss 0.3070231080055237 loss_r 0.26335349678993225 loss_kl 0.043669600039720535\n",
      "step 186 loss 0.3032698631286621 loss_r 0.25667405128479004 loss_kl 0.04659581184387207\n",
      "step 187 loss 0.3042893707752228 loss_r 0.25522172451019287 loss_kl 0.04906763508915901\n",
      "step 188 loss 0.31864023208618164 loss_r 0.26589250564575195 loss_kl 0.052747733891010284\n",
      "step 189 loss 0.2975757122039795 loss_r 0.23599769175052643 loss_kl 0.06157800555229187\n",
      "step 190 loss 0.32365506887435913 loss_r 0.25543275475502014 loss_kl 0.06822230666875839\n",
      "step 191 loss 0.2708092927932739 loss_r 0.19588665664196014 loss_kl 0.07492262125015259\n",
      "step 192 loss 0.27812060713768005 loss_r 0.22397102415561676 loss_kl 0.0541495755314827\n",
      "step 193 loss 0.22508490085601807 loss_r 0.17495064437389374 loss_kl 0.050134263932704926\n",
      "step 194 loss 0.2509761154651642 loss_r 0.18052953481674194 loss_kl 0.07044658064842224\n",
      "step 195 loss 0.25797727704048157 loss_r 0.21285396814346313 loss_kl 0.04512332007288933\n",
      "step 196 loss 0.2434048354625702 loss_r 0.19602923095226288 loss_kl 0.04737561196088791\n",
      "step 197 loss 0.27150973677635193 loss_r 0.22948850691318512 loss_kl 0.04202122986316681\n",
      "step 198 loss 0.2677793502807617 loss_r 0.22644206881523132 loss_kl 0.041337281465530396\n",
      "step 199 loss 0.2600359320640564 loss_r 0.2092655897140503 loss_kl 0.0507703498005867\n",
      "step 200 loss 0.2944885194301605 loss_r 0.247358500957489 loss_kl 0.04713001847267151\n",
      "step 201 loss 0.25204673409461975 loss_r 0.1907375156879425 loss_kl 0.06130921095609665\n",
      "step 202 loss 0.29708078503608704 loss_r 0.24170193076133728 loss_kl 0.055378854274749756\n",
      "step 203 loss 0.2985040545463562 loss_r 0.2478228360414505 loss_kl 0.0506812185049057\n",
      "step 204 loss 0.2981887459754944 loss_r 0.24508410692214966 loss_kl 0.053104642778635025\n",
      "step 205 loss 0.32071179151535034 loss_r 0.27288293838500977 loss_kl 0.04782886058092117\n",
      "step 206 loss 0.25542888045310974 loss_r 0.198863223195076 loss_kl 0.05656565725803375\n",
      "step 207 loss 0.2613569498062134 loss_r 0.20758245885372162 loss_kl 0.05377449840307236\n",
      "step 208 loss 0.23998668789863586 loss_r 0.19624771177768707 loss_kl 0.043738968670368195\n",
      "step 209 loss 0.2717089354991913 loss_r 0.22563700377941132 loss_kl 0.04607192426919937\n",
      "step 210 loss 0.3222702145576477 loss_r 0.28329572081565857 loss_kl 0.03897447884082794\n",
      "step 211 loss 0.24813610315322876 loss_r 0.19986598193645477 loss_kl 0.04827011749148369\n",
      "step 212 loss 0.23294270038604736 loss_r 0.1784127652645111 loss_kl 0.054529935121536255\n",
      "step 213 loss 0.24999135732650757 loss_r 0.20272040367126465 loss_kl 0.04727094992995262\n",
      "step 214 loss 0.3053595721721649 loss_r 0.2619870901107788 loss_kl 0.04337247461080551\n",
      "step 215 loss 0.2270890474319458 loss_r 0.172551229596138 loss_kl 0.0545378141105175\n",
      "step 216 loss 0.35560423135757446 loss_r 0.31775128841400146 loss_kl 0.037852942943573\n",
      "step 217 loss 0.3280552625656128 loss_r 0.28125131130218506 loss_kl 0.04680394381284714\n",
      "step 218 loss 0.2317105233669281 loss_r 0.1744624376296997 loss_kl 0.0572480782866478\n",
      "step 219 loss 0.33055150508880615 loss_r 0.273327499628067 loss_kl 0.057224005460739136\n",
      "step 220 loss 0.3226338326931 loss_r 0.26677149534225464 loss_kl 0.055862344801425934\n",
      "step 221 loss 0.2612176835536957 loss_r 0.19504962861537933 loss_kl 0.06616805493831635\n",
      "step 222 loss 0.2995489239692688 loss_r 0.2405570149421692 loss_kl 0.05899191275238991\n",
      "step 223 loss 0.24672748148441315 loss_r 0.19291985034942627 loss_kl 0.05380763113498688\n",
      "step 224 loss 0.27398502826690674 loss_r 0.22791731357574463 loss_kl 0.04606771469116211\n",
      "step 225 loss 0.339022159576416 loss_r 0.29877156019210815 loss_kl 0.04025060683488846\n",
      "step 226 loss 0.24738077819347382 loss_r 0.20302003622055054 loss_kl 0.04436073824763298\n",
      "step 227 loss 0.26737651228904724 loss_r 0.2213655710220337 loss_kl 0.046010930091142654\n",
      "step 228 loss 0.28549671173095703 loss_r 0.23763808608055115 loss_kl 0.04785863310098648\n",
      "step 229 loss 0.1948806643486023 loss_r 0.13769561052322388 loss_kl 0.057185061275959015\n",
      "step 230 loss 0.29425808787345886 loss_r 0.2552739083766937 loss_kl 0.03898417949676514\n",
      "step 231 loss 0.2692079544067383 loss_r 0.2276979237794876 loss_kl 0.041510023176670074\n",
      "step 232 loss 0.2559901475906372 loss_r 0.2082562893629074 loss_kl 0.04773386940360069\n",
      "step 233 loss 0.31385838985443115 loss_r 0.2751843333244324 loss_kl 0.038674063980579376\n",
      "step 234 loss 0.2364836186170578 loss_r 0.17627480626106262 loss_kl 0.06020880863070488\n",
      "step 235 loss 0.25342756509780884 loss_r 0.1992853283882141 loss_kl 0.05414222180843353\n",
      "step 236 loss 0.26786547899246216 loss_r 0.21149934828281403 loss_kl 0.056366123259067535\n",
      "step 237 loss 0.3493507504463196 loss_r 0.3053945302963257 loss_kl 0.04395622760057449\n",
      "step 238 loss 0.2942911982536316 loss_r 0.23900172114372253 loss_kl 0.05528946593403816\n",
      "step 239 loss 0.28382840752601624 loss_r 0.2284892201423645 loss_kl 0.05533919110894203\n",
      "step 240 loss 0.2630826234817505 loss_r 0.21119312942028046 loss_kl 0.05188947916030884\n",
      "step 241 loss 0.32172638177871704 loss_r 0.28234216570854187 loss_kl 0.039384204894304276\n",
      "step 242 loss 0.2983476519584656 loss_r 0.2561648190021515 loss_kl 0.04218282178044319\n",
      "step 243 loss 0.2727471590042114 loss_r 0.22952064871788025 loss_kl 0.043226517736911774\n",
      "step 244 loss 0.2882416844367981 loss_r 0.23787987232208252 loss_kl 0.05036180466413498\n",
      "step 245 loss 0.26278650760650635 loss_r 0.21078374981880188 loss_kl 0.05200275033712387\n",
      "step 246 loss 0.28344473242759705 loss_r 0.23301252722740173 loss_kl 0.05043220520019531\n",
      "step 247 loss 0.3025272786617279 loss_r 0.2557518184185028 loss_kl 0.046775467693805695\n",
      "step 248 loss 0.26723453402519226 loss_r 0.22025978565216064 loss_kl 0.04697475582361221\n",
      "step 249 loss 0.20319578051567078 loss_r 0.14749707281589508 loss_kl 0.0556987002491951\n",
      "step 250 loss 0.27681949734687805 loss_r 0.21973855793476105 loss_kl 0.057080939412117004\n",
      "step 251 loss 0.2552657127380371 loss_r 0.18903209269046783 loss_kl 0.06623362004756927\n",
      "step 252 loss 0.22846302390098572 loss_r 0.17465275526046753 loss_kl 0.05381027236580849\n",
      "step 253 loss 0.25255075097084045 loss_r 0.19605903327465057 loss_kl 0.056491728872060776\n",
      "step 254 loss 0.38035741448402405 loss_r 0.34429723024368286 loss_kl 0.036060184240341187\n",
      "step 255 loss 0.31183403730392456 loss_r 0.27289894223213196 loss_kl 0.0389351025223732\n",
      "step 256 loss 0.24960541725158691 loss_r 0.18966710567474365 loss_kl 0.05993831157684326\n",
      "step 257 loss 0.26795732975006104 loss_r 0.2168196141719818 loss_kl 0.05113770812749863\n",
      "step 258 loss 0.231694296002388 loss_r 0.1682336926460266 loss_kl 0.06346060335636139\n",
      "step 259 loss 0.24026504158973694 loss_r 0.1767694503068924 loss_kl 0.06349559128284454\n",
      "step 260 loss 0.28559786081314087 loss_r 0.23179955780506134 loss_kl 0.05379829183220863\n",
      "step 261 loss 0.2621980607509613 loss_r 0.20391355454921722 loss_kl 0.05828449875116348\n",
      "step 262 loss 0.19361385703086853 loss_r 0.12411076575517654 loss_kl 0.06950309872627258\n",
      "step 263 loss 0.23530811071395874 loss_r 0.1936148852109909 loss_kl 0.04169321805238724\n",
      "step 264 loss 0.24611012637615204 loss_r 0.19950252771377563 loss_kl 0.0466076023876667\n",
      "step 265 loss 0.2686561346054077 loss_r 0.2291463315486908 loss_kl 0.039509810507297516\n",
      "step 266 loss 0.2736484110355377 loss_r 0.2364632785320282 loss_kl 0.03718512877821922\n",
      "step 267 loss 0.25859785079956055 loss_r 0.2167801707983017 loss_kl 0.041817694902420044\n",
      "step 268 loss 0.2643846869468689 loss_r 0.22291603684425354 loss_kl 0.04146864265203476\n",
      "step 269 loss 0.28609582781791687 loss_r 0.24335989356040955 loss_kl 0.04273593798279762\n",
      "step 270 loss 0.2743566632270813 loss_r 0.20442251861095428 loss_kl 0.06993412971496582\n",
      "step 271 loss 0.34926262497901917 loss_r 0.2996365427970886 loss_kl 0.04962609335780144\n",
      "step 272 loss 0.3132641911506653 loss_r 0.2548149824142456 loss_kl 0.05844920501112938\n",
      "step 273 loss 0.22977083921432495 loss_r 0.16170081496238708 loss_kl 0.06807001680135727\n",
      "step 274 loss 0.26395097374916077 loss_r 0.19623859226703644 loss_kl 0.06771238148212433\n",
      "step 275 loss 0.2249927818775177 loss_r 0.15482360124588013 loss_kl 0.07016918063163757\n",
      "step 276 loss 0.2754351794719696 loss_r 0.22085823118686676 loss_kl 0.05457693710923195\n",
      "step 277 loss 0.3247721493244171 loss_r 0.2788611054420471 loss_kl 0.04591105133295059\n",
      "step 278 loss 0.19964802265167236 loss_r 0.14482972025871277 loss_kl 0.054818298667669296\n",
      "step 279 loss 0.2524648606777191 loss_r 0.20783235132694244 loss_kl 0.04463251680135727\n",
      "step 280 loss 0.27766168117523193 loss_r 0.236988365650177 loss_kl 0.04067332297563553\n",
      "step 281 loss 0.2525591254234314 loss_r 0.21263889968395233 loss_kl 0.03992021828889847\n",
      "step 282 loss 0.27317532896995544 loss_r 0.2295427918434143 loss_kl 0.04363253712654114\n",
      "step 283 loss 0.3528800904750824 loss_r 0.30814096331596375 loss_kl 0.044739119708538055\n",
      "step 284 loss 0.24145637452602386 loss_r 0.17179381847381592 loss_kl 0.06966255605220795\n",
      "step 285 loss 0.31759023666381836 loss_r 0.2600833773612976 loss_kl 0.05750686302781105\n",
      "step 286 loss 0.26225191354751587 loss_r 0.19315284490585327 loss_kl 0.0690990537405014\n",
      "step 287 loss 0.2524748146533966 loss_r 0.18747036159038544 loss_kl 0.06500444561243057\n",
      "step 288 loss 0.23926672339439392 loss_r 0.16494867205619812 loss_kl 0.0743180438876152\n",
      "step 289 loss 0.24587708711624146 loss_r 0.16893155872821808 loss_kl 0.07694552838802338\n",
      "step 290 loss 0.22131401300430298 loss_r 0.15862278640270233 loss_kl 0.06269122660160065\n",
      "step 291 loss 0.2332667112350464 loss_r 0.18245947360992432 loss_kl 0.05080724507570267\n",
      "step 292 loss 0.3525819182395935 loss_r 0.31442102789878845 loss_kl 0.03816087543964386\n",
      "step 293 loss 0.24320942163467407 loss_r 0.19554056227207184 loss_kl 0.04766885191202164\n",
      "step 294 loss 0.23738467693328857 loss_r 0.18418577313423157 loss_kl 0.05319889634847641\n",
      "step 295 loss 0.24598810076713562 loss_r 0.20073065161705017 loss_kl 0.04525744169950485\n",
      "step 296 loss 0.2299402803182602 loss_r 0.18584780395030975 loss_kl 0.04409247636795044\n",
      "step 297 loss 0.25634485483169556 loss_r 0.21767811477184296 loss_kl 0.038666725158691406\n",
      "step 298 loss 0.2896178662776947 loss_r 0.24722251296043396 loss_kl 0.04239536449313164\n",
      "step 299 loss 0.29453399777412415 loss_r 0.24038700759410858 loss_kl 0.05414698272943497\n",
      "step 300 loss 0.27681881189346313 loss_r 0.20896826684474945 loss_kl 0.06785055249929428\n",
      "step 301 loss 0.389332115650177 loss_r 0.32885634899139404 loss_kl 0.06047577038407326\n",
      "step 302 loss 0.24130749702453613 loss_r 0.1621614396572113 loss_kl 0.07914605736732483\n",
      "step 303 loss 0.3008032441139221 loss_r 0.23056496679782867 loss_kl 0.07023826986551285\n",
      "step 304 loss 0.22152918577194214 loss_r 0.13839541375637054 loss_kl 0.08313377946615219\n",
      "step 305 loss 0.2714449465274811 loss_r 0.20785890519618988 loss_kl 0.0635860338807106\n",
      "step 306 loss 0.2594977915287018 loss_r 0.20728358626365662 loss_kl 0.052214205265045166\n",
      "step 307 loss 0.34776952862739563 loss_r 0.3053162693977356 loss_kl 0.04245326668024063\n",
      "step 308 loss 0.25915712118148804 loss_r 0.21580274403095245 loss_kl 0.04335438832640648\n",
      "step 309 loss 0.2544475793838501 loss_r 0.20596006512641907 loss_kl 0.04848750680685043\n",
      "step 310 loss 0.3004342317581177 loss_r 0.25245267152786255 loss_kl 0.04798156023025513\n",
      "step 311 loss 0.1799914538860321 loss_r 0.12232968956232071 loss_kl 0.05766177177429199\n",
      "step 312 loss 0.28630825877189636 loss_r 0.2434343844652176 loss_kl 0.04287387803196907\n",
      "step 313 loss 0.22511106729507446 loss_r 0.16916804015636444 loss_kl 0.055943019688129425\n",
      "step 314 loss 0.29257646203041077 loss_r 0.24371488392353058 loss_kl 0.04886157810688019\n",
      "step 315 loss 0.33277273178100586 loss_r 0.2815413177013397 loss_kl 0.05123141035437584\n",
      "step 316 loss 0.21392062306404114 loss_r 0.16418606042861938 loss_kl 0.049734555184841156\n",
      "step 317 loss 0.27588942646980286 loss_r 0.22337988018989563 loss_kl 0.05250954627990723\n",
      "step 318 loss 0.23342613875865936 loss_r 0.15852096676826477 loss_kl 0.07490517199039459\n",
      "step 319 loss 0.19711244106292725 loss_r 0.12493472546339035 loss_kl 0.0721777155995369\n",
      "step 320 loss 0.2488180696964264 loss_r 0.19536423683166504 loss_kl 0.05345384031534195\n",
      "step 321 loss 0.1901746690273285 loss_r 0.13026274740695953 loss_kl 0.05991192162036896\n",
      "step 322 loss 0.22908399999141693 loss_r 0.1765444576740265 loss_kl 0.05253954231739044\n",
      "step 323 loss 0.22707565128803253 loss_r 0.17377448081970215 loss_kl 0.05330117046833038\n",
      "step 324 loss 0.23848535120487213 loss_r 0.19960826635360718 loss_kl 0.038877081125974655\n",
      "step 325 loss 0.2881070673465729 loss_r 0.24870815873146057 loss_kl 0.03939890116453171\n",
      "step 326 loss 0.2627859115600586 loss_r 0.224932461977005 loss_kl 0.037853434681892395\n",
      "step 327 loss 0.2657444179058075 loss_r 0.22626137733459473 loss_kl 0.03948302939534187\n",
      "step 328 loss 0.2527031898498535 loss_r 0.2106899619102478 loss_kl 0.042013224214315414\n",
      "step 329 loss 0.21492686867713928 loss_r 0.16011928021907806 loss_kl 0.05480758100748062\n",
      "step 330 loss 0.22238080203533173 loss_r 0.1715245395898819 loss_kl 0.05085626244544983\n",
      "step 331 loss 0.2311524748802185 loss_r 0.17766553163528442 loss_kl 0.05348695069551468\n",
      "step 332 loss 0.24837493896484375 loss_r 0.18384094536304474 loss_kl 0.06453398615121841\n",
      "step 333 loss 0.21167562901973724 loss_r 0.15363387763500214 loss_kl 0.058041755110025406\n",
      "step 334 loss 0.27409422397613525 loss_r 0.23019804060459137 loss_kl 0.04389618709683418\n",
      "step 335 loss 0.2519252896308899 loss_r 0.20262829959392548 loss_kl 0.049296993762254715\n",
      "step 336 loss 0.21249228715896606 loss_r 0.159888356924057 loss_kl 0.05260392278432846\n",
      "step 337 loss 0.205708310008049 loss_r 0.15366147458553314 loss_kl 0.05204683542251587\n",
      "step 338 loss 0.21040299534797668 loss_r 0.16054394841194153 loss_kl 0.04985904321074486\n",
      "step 339 loss 0.27452221512794495 loss_r 0.2337224781513214 loss_kl 0.04079974442720413\n",
      "step 340 loss 0.1891421377658844 loss_r 0.11662768572568893 loss_kl 0.07251444458961487\n",
      "step 341 loss 0.19427230954170227 loss_r 0.13712115585803986 loss_kl 0.05715114623308182\n",
      "step 342 loss 0.21797165274620056 loss_r 0.17219217121601105 loss_kl 0.04577947407960892\n",
      "step 343 loss 0.20817887783050537 loss_r 0.16654229164123535 loss_kl 0.04163658991456032\n",
      "step 344 loss 0.3452704846858978 loss_r 0.3104708790779114 loss_kl 0.03479959815740585\n",
      "step 345 loss 0.24292200803756714 loss_r 0.19834588468074799 loss_kl 0.044576115906238556\n",
      "step 346 loss 0.2657315731048584 loss_r 0.21462254226207733 loss_kl 0.05110903084278107\n",
      "step 347 loss 0.27779069542884827 loss_r 0.22963450849056244 loss_kl 0.048156194388866425\n",
      "step 348 loss 0.2795007824897766 loss_r 0.2281724065542221 loss_kl 0.05132836848497391\n",
      "step 349 loss 0.25598853826522827 loss_r 0.18825596570968628 loss_kl 0.06773258745670319\n",
      "step 350 loss 0.2626556158065796 loss_r 0.1966811865568161 loss_kl 0.06597442179918289\n",
      "step 351 loss 0.26895028352737427 loss_r 0.21744784712791443 loss_kl 0.05150245130062103\n",
      "step 352 loss 0.2545555531978607 loss_r 0.1981542706489563 loss_kl 0.05640128254890442\n",
      "step 353 loss 0.2063753306865692 loss_r 0.1542201191186905 loss_kl 0.05215521156787872\n",
      "step 354 loss 0.22214746475219727 loss_r 0.17261846363544464 loss_kl 0.04952899366617203\n",
      "step 355 loss 0.3103288412094116 loss_r 0.274983286857605 loss_kl 0.035345565527677536\n",
      "step 356 loss 0.22254568338394165 loss_r 0.17854414880275726 loss_kl 0.04400152713060379\n",
      "step 357 loss 0.2818618416786194 loss_r 0.2470632940530777 loss_kl 0.034798555076122284\n",
      "step 358 loss 0.28832894563674927 loss_r 0.25182798504829407 loss_kl 0.0365009531378746\n",
      "step 359 loss 0.2032449096441269 loss_r 0.15551283955574036 loss_kl 0.047732070088386536\n",
      "step 360 loss 0.25989076495170593 loss_r 0.21459588408470154 loss_kl 0.04529489204287529\n",
      "step 361 loss 0.27902698516845703 loss_r 0.22535763680934906 loss_kl 0.053669337183237076\n",
      "step 362 loss 0.2752891778945923 loss_r 0.22962650656700134 loss_kl 0.04566267132759094\n",
      "step 363 loss 0.26545244455337524 loss_r 0.21364957094192505 loss_kl 0.05180288478732109\n",
      "step 364 loss 0.21215802431106567 loss_r 0.14027373492717743 loss_kl 0.07188429683446884\n",
      "step 365 loss 0.245388001203537 loss_r 0.1755407750606537 loss_kl 0.0698472335934639\n",
      "step 366 loss 0.2453450858592987 loss_r 0.18584050238132477 loss_kl 0.059504587203264236\n",
      "step 367 loss 0.24578624963760376 loss_r 0.1849781721830368 loss_kl 0.060808077454566956\n",
      "step 368 loss 0.28135520219802856 loss_r 0.2335081398487091 loss_kl 0.04784707352519035\n",
      "step 369 loss 0.24498093128204346 loss_r 0.19183042645454407 loss_kl 0.05315050855278969\n",
      "step 370 loss 0.23473994433879852 loss_r 0.17251019179821014 loss_kl 0.06222974881529808\n",
      "step 371 loss 0.3318710923194885 loss_r 0.2866896390914917 loss_kl 0.04518146067857742\n",
      "step 372 loss 0.184695765376091 loss_r 0.1263759881258011 loss_kl 0.05831977725028992\n",
      "step 373 loss 0.24585475027561188 loss_r 0.19846637547016144 loss_kl 0.04738837480545044\n",
      "step 374 loss 0.28252118825912476 loss_r 0.24413709342479706 loss_kl 0.0383840873837471\n",
      "step 375 loss 0.20427772402763367 loss_r 0.155360609292984 loss_kl 0.048917122185230255\n",
      "step 376 loss 0.23891660571098328 loss_r 0.1896716058254242 loss_kl 0.049244996160268784\n",
      "step 377 loss 0.2494201809167862 loss_r 0.2039269059896469 loss_kl 0.04549327492713928\n",
      "step 378 loss 0.21686094999313354 loss_r 0.1637963056564331 loss_kl 0.05306463688611984\n",
      "step 379 loss 0.2511831521987915 loss_r 0.19993020594120026 loss_kl 0.05125293880701065\n",
      "step 380 loss 0.26773878931999207 loss_r 0.2211420089006424 loss_kl 0.04659677669405937\n",
      "step 381 loss 0.2346118539571762 loss_r 0.18418146669864655 loss_kl 0.05043038725852966\n",
      "step 382 loss 0.31637266278266907 loss_r 0.2751733362674713 loss_kl 0.041199322789907455\n",
      "step 383 loss 0.21534991264343262 loss_r 0.16124336421489716 loss_kl 0.05410655587911606\n",
      "step 384 loss 0.21927066147327423 loss_r 0.17712441086769104 loss_kl 0.04214625060558319\n",
      "step 385 loss 0.2745210528373718 loss_r 0.22925055027008057 loss_kl 0.04527051001787186\n",
      "step 386 loss 0.22763605415821075 loss_r 0.18116262555122375 loss_kl 0.0464734323322773\n",
      "step 387 loss 0.21705126762390137 loss_r 0.16394060850143433 loss_kl 0.05311065912246704\n",
      "step 388 loss 0.17411261796951294 loss_r 0.12849128246307373 loss_kl 0.04562132805585861\n",
      "step 389 loss 0.15688733756542206 loss_r 0.097966268658638 loss_kl 0.05892106890678406\n",
      "step 390 loss 0.23679722845554352 loss_r 0.19297456741333008 loss_kl 0.04382266476750374\n",
      "step 391 loss 0.20837555825710297 loss_r 0.1584869772195816 loss_kl 0.04988858103752136\n",
      "step 392 loss 0.2328346073627472 loss_r 0.187373086810112 loss_kl 0.04546152427792549\n",
      "step 393 loss 0.21999591588974 loss_r 0.17846941947937012 loss_kl 0.041526488959789276\n",
      "step 394 loss 0.24667713046073914 loss_r 0.2062876969575882 loss_kl 0.04038942605257034\n",
      "step 395 loss 0.18485127389431 loss_r 0.14097772538661957 loss_kl 0.04387354850769043\n",
      "step 396 loss 0.18442921340465546 loss_r 0.13868176937103271 loss_kl 0.04574744775891304\n",
      "step 397 loss 0.21136236190795898 loss_r 0.15833789110183716 loss_kl 0.05302446335554123\n",
      "step 398 loss 0.2400931417942047 loss_r 0.19757364690303802 loss_kl 0.04251949116587639\n",
      "Epoch 2\n",
      "step 0 loss 0.2699953019618988 loss_r 0.22718268632888794 loss_kl 0.04281262308359146\n",
      "step 1 loss 0.3222825229167938 loss_r 0.2771908640861511 loss_kl 0.045091670006513596\n",
      "step 2 loss 0.23622719943523407 loss_r 0.1839476078748703 loss_kl 0.05227959528565407\n",
      "step 3 loss 0.2396078258752823 loss_r 0.18461672961711884 loss_kl 0.05499109625816345\n",
      "step 4 loss 0.26445651054382324 loss_r 0.2113109976053238 loss_kl 0.053145524114370346\n",
      "step 5 loss 0.19870778918266296 loss_r 0.1349695920944214 loss_kl 0.06373819708824158\n",
      "step 6 loss 0.18713127076625824 loss_r 0.11738703399896622 loss_kl 0.06974423676729202\n",
      "step 7 loss 0.30383628606796265 loss_r 0.24901089072227478 loss_kl 0.054825399070978165\n",
      "step 8 loss 0.16590595245361328 loss_r 0.108307845890522 loss_kl 0.05759810656309128\n",
      "step 9 loss 0.21907946467399597 loss_r 0.15740621089935303 loss_kl 0.06167324632406235\n",
      "step 10 loss 0.2206258624792099 loss_r 0.17860908806324005 loss_kl 0.04201677814126015\n",
      "step 11 loss 0.23594863712787628 loss_r 0.18748095631599426 loss_kl 0.04846768081188202\n",
      "step 12 loss 0.2202608287334442 loss_r 0.17938460409641266 loss_kl 0.04087623208761215\n",
      "step 13 loss 0.25740522146224976 loss_r 0.21773666143417358 loss_kl 0.03966854512691498\n",
      "step 14 loss 0.2941262423992157 loss_r 0.25325456261634827 loss_kl 0.04087167978286743\n",
      "step 15 loss 0.22138209640979767 loss_r 0.17175497114658356 loss_kl 0.04962712526321411\n",
      "step 16 loss 0.2787363529205322 loss_r 0.22480352222919464 loss_kl 0.05393281579017639\n",
      "step 17 loss 0.23519401252269745 loss_r 0.17657366394996643 loss_kl 0.058620352298021317\n",
      "step 18 loss 0.27299821376800537 loss_r 0.2117363065481186 loss_kl 0.061261892318725586\n",
      "step 19 loss 0.25746187567710876 loss_r 0.19582441449165344 loss_kl 0.06163745000958443\n",
      "step 20 loss 0.23586653172969818 loss_r 0.1741906851530075 loss_kl 0.061675846576690674\n",
      "step 21 loss 0.2639140486717224 loss_r 0.20609652996063232 loss_kl 0.05781753361225128\n",
      "step 22 loss 0.20766416192054749 loss_r 0.15202952921390533 loss_kl 0.05563464015722275\n",
      "step 23 loss 0.25247281789779663 loss_r 0.20816670358181 loss_kl 0.04430609941482544\n",
      "step 24 loss 0.25074633955955505 loss_r 0.20485509932041168 loss_kl 0.04589124023914337\n",
      "step 25 loss 0.2443258911371231 loss_r 0.20177829265594482 loss_kl 0.04254760220646858\n",
      "step 26 loss 0.2623365521430969 loss_r 0.22812364995479584 loss_kl 0.03421289101243019\n",
      "step 27 loss 0.20519359409809113 loss_r 0.16014286875724792 loss_kl 0.0450507290661335\n",
      "step 28 loss 0.3156221807003021 loss_r 0.26080650091171265 loss_kl 0.05481567978858948\n",
      "step 29 loss 0.2833724021911621 loss_r 0.24464178085327148 loss_kl 0.03873061761260033\n",
      "step 30 loss 0.2601039707660675 loss_r 0.20885424315929413 loss_kl 0.05124972015619278\n",
      "step 31 loss 0.2664460241794586 loss_r 0.20096904039382935 loss_kl 0.06547697633504868\n",
      "step 32 loss 0.22440369427204132 loss_r 0.14300473034381866 loss_kl 0.08139896392822266\n",
      "step 33 loss 0.3125072419643402 loss_r 0.2428707778453827 loss_kl 0.06963647156953812\n",
      "step 34 loss 0.1853957623243332 loss_r 0.09416940063238144 loss_kl 0.09122636169195175\n",
      "step 35 loss 0.15094402432441711 loss_r 0.08164539188146591 loss_kl 0.0692986249923706\n",
      "step 36 loss 0.17942598462104797 loss_r 0.11338606476783752 loss_kl 0.06603992730379105\n",
      "step 37 loss 0.26839086413383484 loss_r 0.22514395415782928 loss_kl 0.04324690252542496\n",
      "step 38 loss 0.27655836939811707 loss_r 0.22901122272014618 loss_kl 0.04754715412855148\n",
      "step 39 loss 0.2311965674161911 loss_r 0.19045519828796387 loss_kl 0.04074137285351753\n",
      "step 40 loss 0.2212679386138916 loss_r 0.17373991012573242 loss_kl 0.04752802848815918\n",
      "step 41 loss 0.34876105189323425 loss_r 0.3104831278324127 loss_kl 0.038277916610240936\n",
      "step 42 loss 0.36361902952194214 loss_r 0.32559460401535034 loss_kl 0.03802444040775299\n",
      "step 43 loss 0.18441012501716614 loss_r 0.1280335634946823 loss_kl 0.056376561522483826\n",
      "step 44 loss 0.25180521607398987 loss_r 0.1916303038597107 loss_kl 0.06017492339015007\n",
      "step 45 loss 0.2025579959154129 loss_r 0.13283759355545044 loss_kl 0.06972040235996246\n",
      "step 46 loss 0.27076804637908936 loss_r 0.21587836742401123 loss_kl 0.054889678955078125\n",
      "step 47 loss 0.2125314176082611 loss_r 0.15523654222488403 loss_kl 0.057294879108667374\n",
      "step 48 loss 0.2515178620815277 loss_r 0.20082134008407593 loss_kl 0.05069651082158089\n",
      "step 49 loss 0.22728073596954346 loss_r 0.17693425714969635 loss_kl 0.05034647136926651\n",
      "step 50 loss 0.2142670601606369 loss_r 0.1642807424068451 loss_kl 0.04998631775379181\n",
      "step 51 loss 0.19972196221351624 loss_r 0.15054109692573547 loss_kl 0.04918087273836136\n",
      "step 52 loss 0.24718427658081055 loss_r 0.2046167552471161 loss_kl 0.04256751388311386\n",
      "step 53 loss 0.1734338402748108 loss_r 0.12542562186717987 loss_kl 0.04800821840763092\n",
      "step 54 loss 0.17257724702358246 loss_r 0.12209046632051468 loss_kl 0.05048677697777748\n",
      "step 55 loss 0.15031026303768158 loss_r 0.089692123234272 loss_kl 0.060618139803409576\n",
      "step 56 loss 0.29785096645355225 loss_r 0.25985854864120483 loss_kl 0.03799241781234741\n",
      "step 57 loss 0.16152231395244598 loss_r 0.10879363119602203 loss_kl 0.05272868275642395\n",
      "step 58 loss 0.22926566004753113 loss_r 0.16867393255233765 loss_kl 0.06059173122048378\n",
      "step 59 loss 0.22096548974514008 loss_r 0.17302322387695312 loss_kl 0.04794226586818695\n",
      "step 60 loss 0.2064884752035141 loss_r 0.14269912242889404 loss_kl 0.06378935277462006\n",
      "step 61 loss 0.2174162119626999 loss_r 0.16611191630363464 loss_kl 0.051304299384355545\n",
      "step 62 loss 0.2612208127975464 loss_r 0.20994418859481812 loss_kl 0.051276639103889465\n",
      "step 63 loss 0.23394761979579926 loss_r 0.18517275154590607 loss_kl 0.04877486452460289\n",
      "step 64 loss 0.23495416343212128 loss_r 0.18523652851581573 loss_kl 0.04971763864159584\n",
      "step 65 loss 0.17347006499767303 loss_r 0.11549261212348938 loss_kl 0.057977452874183655\n",
      "step 66 loss 0.16933198273181915 loss_r 0.09711628407239914 loss_kl 0.07221569865942001\n",
      "step 67 loss 0.24331697821617126 loss_r 0.19370944797992706 loss_kl 0.0496075376868248\n",
      "step 68 loss 0.1989864557981491 loss_r 0.14654825627803802 loss_kl 0.052438199520111084\n",
      "step 69 loss 0.19045604765415192 loss_r 0.13937486708164215 loss_kl 0.051081180572509766\n",
      "step 70 loss 0.30212125182151794 loss_r 0.26279687881469727 loss_kl 0.039324384182691574\n",
      "step 71 loss 0.22073273360729218 loss_r 0.18046866357326508 loss_kl 0.0402640737593174\n",
      "step 72 loss 0.22480258345603943 loss_r 0.17510643601417542 loss_kl 0.049696143716573715\n",
      "step 73 loss 0.2141045778989792 loss_r 0.1617276966571808 loss_kl 0.0523768775165081\n",
      "step 74 loss 0.18324372172355652 loss_r 0.12708760797977448 loss_kl 0.05615611746907234\n",
      "step 75 loss 0.1502794623374939 loss_r 0.08691656589508057 loss_kl 0.06336290389299393\n",
      "step 76 loss 0.20028842985630035 loss_r 0.14449666440486908 loss_kl 0.055791765451431274\n",
      "step 77 loss 0.16860775649547577 loss_r 0.10299669206142426 loss_kl 0.06561106443405151\n",
      "step 78 loss 0.16823720932006836 loss_r 0.1119619756937027 loss_kl 0.05627522990107536\n",
      "step 79 loss 0.1529693305492401 loss_r 0.09376303851604462 loss_kl 0.05920629948377609\n",
      "step 80 loss 0.27502205967903137 loss_r 0.2315441220998764 loss_kl 0.04347793012857437\n",
      "step 81 loss 0.26234447956085205 loss_r 0.2188596874475479 loss_kl 0.04348479583859444\n",
      "step 82 loss 0.23635704815387726 loss_r 0.19515667855739594 loss_kl 0.04120036959648132\n",
      "step 83 loss 0.14963313937187195 loss_r 0.10194628685712814 loss_kl 0.047686852514743805\n",
      "step 84 loss 0.23672717809677124 loss_r 0.20099391043186188 loss_kl 0.03573327139019966\n",
      "step 85 loss 0.1573701649904251 loss_r 0.10480929911136627 loss_kl 0.05256086587905884\n",
      "step 86 loss 0.17202122509479523 loss_r 0.1275067925453186 loss_kl 0.044514428824186325\n",
      "step 87 loss 0.17522364854812622 loss_r 0.12799376249313354 loss_kl 0.04722989350557327\n",
      "step 88 loss 0.24401786923408508 loss_r 0.20259569585323334 loss_kl 0.041422173380851746\n",
      "step 89 loss 0.21280187368392944 loss_r 0.1632867306470871 loss_kl 0.049515146762132645\n",
      "step 90 loss 0.21855059266090393 loss_r 0.15977118909358978 loss_kl 0.058779407292604446\n",
      "step 91 loss 0.24925141036510468 loss_r 0.19860154390335083 loss_kl 0.050649870187044144\n",
      "step 92 loss 0.21241751313209534 loss_r 0.1661406010389328 loss_kl 0.04627691209316254\n",
      "step 93 loss 0.1590368151664734 loss_r 0.10687525570392609 loss_kl 0.0521615631878376\n",
      "step 94 loss 0.1912534385919571 loss_r 0.145939901471138 loss_kl 0.04531353339552879\n",
      "step 95 loss 0.21239668130874634 loss_r 0.1661883443593979 loss_kl 0.04620832949876785\n",
      "step 96 loss 0.25437623262405396 loss_r 0.20804709196090698 loss_kl 0.04632912576198578\n",
      "step 97 loss 0.1627606451511383 loss_r 0.11338965594768524 loss_kl 0.04937099665403366\n",
      "step 98 loss 0.20452484488487244 loss_r 0.1482466757297516 loss_kl 0.05627816170454025\n",
      "step 99 loss 0.2268611490726471 loss_r 0.1806444525718689 loss_kl 0.0462166965007782\n",
      "step 100 loss 0.270846962928772 loss_r 0.22299732267856598 loss_kl 0.04784964397549629\n",
      "step 101 loss 0.13761171698570251 loss_r 0.07762011140584946 loss_kl 0.05999159812927246\n",
      "step 102 loss 0.17448551952838898 loss_r 0.12090890109539032 loss_kl 0.053576622158288956\n",
      "step 103 loss 0.1937660276889801 loss_r 0.14099039137363434 loss_kl 0.052775636315345764\n",
      "step 104 loss 0.14181877672672272 loss_r 0.08269570767879486 loss_kl 0.059123069047927856\n",
      "step 105 loss 0.2421989142894745 loss_r 0.1977437138557434 loss_kl 0.04445519298315048\n",
      "step 106 loss 0.15845917165279388 loss_r 0.11018667370080948 loss_kl 0.048272501677274704\n",
      "step 107 loss 0.2265091836452484 loss_r 0.18409883975982666 loss_kl 0.042410340160131454\n",
      "step 108 loss 0.2652568817138672 loss_r 0.21993179619312286 loss_kl 0.045325081795454025\n",
      "step 109 loss 0.25478866696357727 loss_r 0.21139957010746002 loss_kl 0.04338908940553665\n",
      "step 110 loss 0.2537389397621155 loss_r 0.20757749676704407 loss_kl 0.04616142809391022\n",
      "step 111 loss 0.2523786127567291 loss_r 0.1911928951740265 loss_kl 0.06118571385741234\n",
      "step 112 loss 0.22051066160202026 loss_r 0.16427263617515564 loss_kl 0.056238025426864624\n",
      "step 113 loss 0.2518311142921448 loss_r 0.18764843046665192 loss_kl 0.06418266892433167\n",
      "step 114 loss 0.23314499855041504 loss_r 0.16690461337566376 loss_kl 0.06624037772417068\n",
      "step 115 loss 0.3418038785457611 loss_r 0.29065775871276855 loss_kl 0.051146119832992554\n",
      "step 116 loss 0.2378566861152649 loss_r 0.17153069376945496 loss_kl 0.06632598489522934\n",
      "step 117 loss 0.18035413324832916 loss_r 0.12059703469276428 loss_kl 0.05975709855556488\n",
      "step 118 loss 0.1866753101348877 loss_r 0.12568140029907227 loss_kl 0.06099391728639603\n",
      "step 119 loss 0.20435652136802673 loss_r 0.14625626802444458 loss_kl 0.05810026079416275\n",
      "step 120 loss 0.15631350874900818 loss_r 0.09888973832130432 loss_kl 0.05742376297712326\n",
      "step 121 loss 0.216586172580719 loss_r 0.16984373331069946 loss_kl 0.046742431819438934\n",
      "step 122 loss 0.2469676434993744 loss_r 0.20473653078079224 loss_kl 0.04223111644387245\n",
      "step 123 loss 0.23247486352920532 loss_r 0.19116611778736115 loss_kl 0.04130873829126358\n",
      "step 124 loss 0.16444151103496552 loss_r 0.11879726499319077 loss_kl 0.04564424976706505\n",
      "step 125 loss 0.19717678427696228 loss_r 0.13609546422958374 loss_kl 0.06108132004737854\n",
      "step 126 loss 0.2121334820985794 loss_r 0.15165267884731293 loss_kl 0.06048080697655678\n",
      "step 127 loss 0.18522247672080994 loss_r 0.12466087192296982 loss_kl 0.06056159734725952\n",
      "step 128 loss 0.19000303745269775 loss_r 0.12614494562149048 loss_kl 0.06385809183120728\n",
      "step 129 loss 0.20974786579608917 loss_r 0.14559733867645264 loss_kl 0.06415052711963654\n",
      "step 130 loss 0.2362745702266693 loss_r 0.18625397980213165 loss_kl 0.050020597875118256\n",
      "step 131 loss 0.24637097120285034 loss_r 0.20071682333946228 loss_kl 0.045654140412807465\n",
      "step 132 loss 0.1918550431728363 loss_r 0.1476133167743683 loss_kl 0.044241733849048615\n",
      "step 133 loss 0.23946279287338257 loss_r 0.20077292621135712 loss_kl 0.03868987038731575\n",
      "step 134 loss 0.1793508529663086 loss_r 0.13085317611694336 loss_kl 0.04849768429994583\n",
      "step 135 loss 0.2243371456861496 loss_r 0.17254497110843658 loss_kl 0.051792170852422714\n",
      "step 136 loss 0.17892658710479736 loss_r 0.13348017632961273 loss_kl 0.04544641822576523\n",
      "step 137 loss 0.14831937849521637 loss_r 0.10015149414539337 loss_kl 0.048167884349823\n",
      "step 138 loss 0.23268574476242065 loss_r 0.17726577818393707 loss_kl 0.05541996657848358\n",
      "step 139 loss 0.17950159311294556 loss_r 0.11425597220659256 loss_kl 0.0652456283569336\n",
      "step 140 loss 0.20120510458946228 loss_r 0.1480451077222824 loss_kl 0.05315999314188957\n",
      "step 141 loss 0.19317995011806488 loss_r 0.14732863008975983 loss_kl 0.04585132375359535\n",
      "step 142 loss 0.19430981576442719 loss_r 0.14102351665496826 loss_kl 0.053286295384168625\n",
      "step 143 loss 0.2239140123128891 loss_r 0.17666861414909363 loss_kl 0.04724539816379547\n",
      "step 144 loss 0.1670006364583969 loss_r 0.10950546711683273 loss_kl 0.05749516934156418\n",
      "step 145 loss 0.2505958378314972 loss_r 0.20206531882286072 loss_kl 0.04853051155805588\n",
      "step 146 loss 0.3300159275531769 loss_r 0.2877238690853119 loss_kl 0.04229206591844559\n",
      "step 147 loss 0.1063094288110733 loss_r 0.040991220623254776 loss_kl 0.06531820446252823\n",
      "step 148 loss 0.14727048575878143 loss_r 0.08473408222198486 loss_kl 0.06253640353679657\n",
      "step 149 loss 0.1605934500694275 loss_r 0.1006646528840065 loss_kl 0.05992878973484039\n",
      "step 150 loss 0.24823474884033203 loss_r 0.19570063054561615 loss_kl 0.05253412574529648\n",
      "step 151 loss 0.14278540015220642 loss_r 0.07963241636753082 loss_kl 0.063152976334095\n",
      "step 152 loss 0.11252635717391968 loss_r 0.0525711253285408 loss_kl 0.059955231845378876\n",
      "step 153 loss 0.20265817642211914 loss_r 0.14727270603179932 loss_kl 0.05538546293973923\n",
      "step 154 loss 0.2973494827747345 loss_r 0.26183632016181946 loss_kl 0.03551316633820534\n",
      "step 155 loss 0.2837068438529968 loss_r 0.2430662214756012 loss_kl 0.040640607476234436\n",
      "step 156 loss 0.229108989238739 loss_r 0.18088750541210175 loss_kl 0.04822147637605667\n",
      "step 157 loss 0.17193588614463806 loss_r 0.1180826723575592 loss_kl 0.05385321378707886\n",
      "step 158 loss 0.18121342360973358 loss_r 0.1301751434803009 loss_kl 0.05103828385472298\n",
      "step 159 loss 0.22790738940238953 loss_r 0.17604757845401764 loss_kl 0.05185981094837189\n",
      "step 160 loss 0.23355582356452942 loss_r 0.18289925158023834 loss_kl 0.05065657198429108\n",
      "step 161 loss 0.24850758910179138 loss_r 0.19317173957824707 loss_kl 0.055335842072963715\n",
      "step 162 loss 0.24509994685649872 loss_r 0.19111433625221252 loss_kl 0.053985610604286194\n",
      "step 163 loss 0.2868899703025818 loss_r 0.22571060061454773 loss_kl 0.061179354786872864\n",
      "step 164 loss 0.142706960439682 loss_r 0.05906951054930687 loss_kl 0.08363744616508484\n",
      "step 165 loss 0.21584750711917877 loss_r 0.15600942075252533 loss_kl 0.05983808636665344\n",
      "step 166 loss 0.17941299080848694 loss_r 0.11838021129369736 loss_kl 0.061032772064208984\n",
      "step 167 loss 0.2618660628795624 loss_r 0.21226206421852112 loss_kl 0.04960400611162186\n",
      "step 168 loss 0.1490778774023056 loss_r 0.09592273831367493 loss_kl 0.05315513536334038\n",
      "step 169 loss 0.21569401025772095 loss_r 0.1687859296798706 loss_kl 0.046908073127269745\n",
      "step 170 loss 0.27981892228126526 loss_r 0.24196656048297882 loss_kl 0.03785235807299614\n",
      "step 171 loss 0.19099652767181396 loss_r 0.14956210553646088 loss_kl 0.04143441841006279\n",
      "step 172 loss 0.2292465716600418 loss_r 0.1911003589630127 loss_kl 0.038146208971738815\n",
      "step 173 loss 0.18998436629772186 loss_r 0.14316365122795105 loss_kl 0.04682071506977081\n",
      "step 174 loss 0.29766905307769775 loss_r 0.2561259865760803 loss_kl 0.04154307022690773\n",
      "step 175 loss 0.21943724155426025 loss_r 0.17165425419807434 loss_kl 0.047782979905605316\n",
      "step 176 loss 0.2042202204465866 loss_r 0.13629160821437836 loss_kl 0.06792861223220825\n",
      "step 177 loss 0.18690115213394165 loss_r 0.12575195729732513 loss_kl 0.06114920228719711\n",
      "step 178 loss 0.21620184183120728 loss_r 0.15055307745933533 loss_kl 0.06564876437187195\n",
      "step 179 loss 0.18718358874320984 loss_r 0.12870776653289795 loss_kl 0.05847581848502159\n",
      "step 180 loss 0.18252885341644287 loss_r 0.12105558067560196 loss_kl 0.06147328019142151\n",
      "step 181 loss 0.33616650104522705 loss_r 0.29451924562454224 loss_kl 0.04164724424481392\n",
      "step 182 loss 0.24152830243110657 loss_r 0.1935734897851944 loss_kl 0.04795482009649277\n",
      "step 183 loss 0.1317802369594574 loss_r 0.07926787436008453 loss_kl 0.05251235514879227\n",
      "step 184 loss 0.20727011561393738 loss_r 0.15612925589084625 loss_kl 0.05114085227251053\n",
      "step 185 loss 0.14660397171974182 loss_r 0.09214484691619873 loss_kl 0.05445913225412369\n",
      "step 186 loss 0.16925294697284698 loss_r 0.11706520617008209 loss_kl 0.05218774452805519\n",
      "step 187 loss 0.2482336312532425 loss_r 0.2081478089094162 loss_kl 0.040085822343826294\n",
      "step 188 loss 0.13989566266536713 loss_r 0.08710642158985138 loss_kl 0.05278923735022545\n",
      "step 189 loss 0.2833290994167328 loss_r 0.24181434512138367 loss_kl 0.04151476174592972\n",
      "step 190 loss 0.21487094461917877 loss_r 0.16478276252746582 loss_kl 0.05008818209171295\n",
      "step 191 loss 0.21307913959026337 loss_r 0.15431036055088043 loss_kl 0.058768775314092636\n",
      "step 192 loss 0.23348741233348846 loss_r 0.1842435896396637 loss_kl 0.049243826419115067\n",
      "step 193 loss 0.21082165837287903 loss_r 0.1552157998085022 loss_kl 0.05560585483908653\n",
      "step 194 loss 0.1417241245508194 loss_r 0.08818969875574112 loss_kl 0.05353442579507828\n",
      "step 195 loss 0.18206527829170227 loss_r 0.1301964819431305 loss_kl 0.051868803799152374\n",
      "step 196 loss 0.21126720309257507 loss_r 0.16150978207588196 loss_kl 0.049757424741983414\n",
      "step 197 loss 0.20255324244499207 loss_r 0.15470226109027863 loss_kl 0.04785098508000374\n",
      "step 198 loss 0.2074948400259018 loss_r 0.1543390303850174 loss_kl 0.0531558096408844\n",
      "step 199 loss 0.19856107234954834 loss_r 0.14669930934906006 loss_kl 0.05186176300048828\n",
      "step 200 loss 0.2301538586616516 loss_r 0.17861182987689972 loss_kl 0.05154203623533249\n",
      "step 201 loss 0.11780024319887161 loss_r 0.0578324981033802 loss_kl 0.05996774509549141\n",
      "step 202 loss 0.24143284559249878 loss_r 0.20148923993110657 loss_kl 0.03994361311197281\n",
      "step 203 loss 0.24615216255187988 loss_r 0.2032891809940338 loss_kl 0.04286297783255577\n",
      "step 204 loss 0.16646593809127808 loss_r 0.11495024710893631 loss_kl 0.05151569843292236\n",
      "step 205 loss 0.19493341445922852 loss_r 0.14983560144901276 loss_kl 0.04509781301021576\n",
      "step 206 loss 0.161937415599823 loss_r 0.10746210813522339 loss_kl 0.054475314915180206\n",
      "step 207 loss 0.2060844749212265 loss_r 0.15889838337898254 loss_kl 0.04718609154224396\n",
      "step 208 loss 0.16299013793468475 loss_r 0.11743982881307602 loss_kl 0.045550309121608734\n",
      "step 209 loss 0.21570907533168793 loss_r 0.17158855497837067 loss_kl 0.04412052035331726\n",
      "step 210 loss 0.17861653864383698 loss_r 0.12349953502416611 loss_kl 0.05511699989438057\n",
      "step 211 loss 0.210256427526474 loss_r 0.15615452826023102 loss_kl 0.05410189926624298\n",
      "step 212 loss 0.19153398275375366 loss_r 0.1409214287996292 loss_kl 0.050612546503543854\n",
      "step 213 loss 0.2506551146507263 loss_r 0.1901550590991974 loss_kl 0.06050004065036774\n",
      "step 214 loss 0.1800721287727356 loss_r 0.11463494598865509 loss_kl 0.0654371827840805\n",
      "step 215 loss 0.169242724776268 loss_r 0.10607624799013138 loss_kl 0.06316647678613663\n",
      "step 216 loss 0.29891330003738403 loss_r 0.2535300850868225 loss_kl 0.04538322240114212\n",
      "step 217 loss 0.1607169210910797 loss_r 0.09469016641378403 loss_kl 0.06602674722671509\n",
      "step 218 loss 0.15799769759178162 loss_r 0.0987209901213646 loss_kl 0.05927671492099762\n",
      "step 219 loss 0.28966498374938965 loss_r 0.24368201196193695 loss_kl 0.04598298668861389\n",
      "step 220 loss 0.2190118134021759 loss_r 0.17395459115505219 loss_kl 0.04505722224712372\n",
      "step 221 loss 0.3584679663181305 loss_r 0.3114963173866272 loss_kl 0.046971645206213\n",
      "step 222 loss 0.18482384085655212 loss_r 0.13620291650295258 loss_kl 0.048620931804180145\n",
      "step 223 loss 0.16966474056243896 loss_r 0.11162364482879639 loss_kl 0.05804109200835228\n",
      "step 224 loss 0.19493606686592102 loss_r 0.13457123935222626 loss_kl 0.06036482751369476\n",
      "step 225 loss 0.1787901520729065 loss_r 0.1014014333486557 loss_kl 0.0773887187242508\n",
      "step 226 loss 0.2726161479949951 loss_r 0.21077653765678406 loss_kl 0.06183961033821106\n",
      "step 227 loss 0.30642998218536377 loss_r 0.2547150254249573 loss_kl 0.05171496421098709\n",
      "step 228 loss 0.1578822135925293 loss_r 0.08737482130527496 loss_kl 0.07050739973783493\n",
      "step 229 loss 0.19070060551166534 loss_r 0.12494195997714996 loss_kl 0.06575864553451538\n",
      "step 230 loss 0.2233383059501648 loss_r 0.1694960594177246 loss_kl 0.05384225398302078\n",
      "step 231 loss 0.1283079981803894 loss_r 0.06715158373117447 loss_kl 0.06115641072392464\n",
      "step 232 loss 0.29709240794181824 loss_r 0.25003835558891296 loss_kl 0.04705405235290527\n",
      "step 233 loss 0.17571134865283966 loss_r 0.12396285682916641 loss_kl 0.05174849182367325\n",
      "step 234 loss 0.2511123716831207 loss_r 0.2080085277557373 loss_kl 0.04310384765267372\n",
      "step 235 loss 0.2169526219367981 loss_r 0.16614341735839844 loss_kl 0.05080920830368996\n",
      "step 236 loss 0.2693445682525635 loss_r 0.21858397126197815 loss_kl 0.05076059699058533\n",
      "step 237 loss 0.18756774067878723 loss_r 0.11645675450563431 loss_kl 0.07111098617315292\n",
      "step 238 loss 0.20913001894950867 loss_r 0.1352774053812027 loss_kl 0.07385262101888657\n",
      "step 239 loss 0.14981243014335632 loss_r 0.047901224344968796 loss_kl 0.10191120207309723\n",
      "step 240 loss 0.27648818492889404 loss_r 0.21204355359077454 loss_kl 0.0644446313381195\n",
      "step 241 loss 0.09663019329309464 loss_r 0.01998378336429596 loss_kl 0.07664640992879868\n",
      "step 242 loss 0.23065707087516785 loss_r 0.17060308158397675 loss_kl 0.060053981840610504\n",
      "step 243 loss 0.1598300337791443 loss_r 0.1090487390756607 loss_kl 0.05078130215406418\n",
      "step 244 loss 0.16069629788398743 loss_r 0.1053861677646637 loss_kl 0.05531013756990433\n",
      "step 245 loss 0.19279468059539795 loss_r 0.15105552971363068 loss_kl 0.041739143431186676\n",
      "step 246 loss 0.20260165631771088 loss_r 0.16272759437561035 loss_kl 0.03987406566739082\n",
      "step 247 loss 0.17278724908828735 loss_r 0.13055124878883362 loss_kl 0.042236000299453735\n",
      "step 248 loss 0.1432097852230072 loss_r 0.09966087341308594 loss_kl 0.04354891926050186\n",
      "step 249 loss 0.11509332060813904 loss_r 0.06028852239251137 loss_kl 0.05480480194091797\n",
      "step 250 loss 0.11241276562213898 loss_r 0.05153776332736015 loss_kl 0.06087500602006912\n",
      "step 251 loss 0.19777977466583252 loss_r 0.14520052075386047 loss_kl 0.05257926136255264\n",
      "step 252 loss 0.16788847744464874 loss_r 0.09554130584001541 loss_kl 0.07234717160463333\n",
      "step 253 loss 0.1496570110321045 loss_r 0.08908606320619583 loss_kl 0.06057095527648926\n",
      "step 254 loss 0.17300580441951752 loss_r 0.11966927349567413 loss_kl 0.053336530923843384\n",
      "step 255 loss 0.16166718304157257 loss_r 0.11194098740816116 loss_kl 0.049726199358701706\n",
      "step 256 loss 0.11456264555454254 loss_r 0.058083776384592056 loss_kl 0.056478872895240784\n",
      "step 257 loss 0.1855563223361969 loss_r 0.13681207597255707 loss_kl 0.04874425381422043\n",
      "step 258 loss 0.09082522243261337 loss_r 0.024518826976418495 loss_kl 0.06630639731884003\n",
      "step 259 loss 0.20336100459098816 loss_r 0.1560753583908081 loss_kl 0.047285646200180054\n",
      "step 260 loss 0.1679607331752777 loss_r 0.12260410934686661 loss_kl 0.045356616377830505\n",
      "step 261 loss 0.22535431385040283 loss_r 0.18202094733715057 loss_kl 0.04333335906267166\n",
      "step 262 loss 0.245352640748024 loss_r 0.20084168016910553 loss_kl 0.04451096057891846\n",
      "step 263 loss 0.18356502056121826 loss_r 0.1372145265340805 loss_kl 0.04635050147771835\n",
      "step 264 loss 0.24820631742477417 loss_r 0.1906009465456009 loss_kl 0.05760537087917328\n",
      "step 265 loss 0.2094959318637848 loss_r 0.146753191947937 loss_kl 0.06274273246526718\n",
      "step 266 loss 0.16510412096977234 loss_r 0.09345309436321259 loss_kl 0.07165103405714035\n",
      "step 267 loss 0.24336765706539154 loss_r 0.180635005235672 loss_kl 0.06273265182971954\n",
      "step 268 loss 0.2353573739528656 loss_r 0.17833338677883148 loss_kl 0.057023994624614716\n",
      "step 269 loss 0.21326813101768494 loss_r 0.16007408499717712 loss_kl 0.05319405347108841\n",
      "step 270 loss 0.10968567430973053 loss_r 0.04957545921206474 loss_kl 0.06011021509766579\n",
      "step 271 loss 0.13735169172286987 loss_r 0.08139900863170624 loss_kl 0.05595267936587334\n",
      "step 272 loss 0.07293591648340225 loss_r 0.017608866095542908 loss_kl 0.055327050387859344\n",
      "step 273 loss 0.195558100938797 loss_r 0.15603742003440857 loss_kl 0.03952068090438843\n",
      "step 274 loss 0.2136240303516388 loss_r 0.17139454185962677 loss_kl 0.04222948104143143\n",
      "step 275 loss 0.1809178590774536 loss_r 0.13854141533374786 loss_kl 0.04237644374370575\n",
      "step 276 loss 0.18700282275676727 loss_r 0.14626401662826538 loss_kl 0.04073880612850189\n",
      "step 277 loss 0.12971583008766174 loss_r 0.07910393178462982 loss_kl 0.05061190575361252\n",
      "step 278 loss 0.15804393589496613 loss_r 0.11709287762641907 loss_kl 0.04095106199383736\n",
      "step 279 loss 0.1985246241092682 loss_r 0.15176720917224884 loss_kl 0.046757422387599945\n",
      "step 280 loss 0.21434089541435242 loss_r 0.14896519482135773 loss_kl 0.06537570059299469\n",
      "step 281 loss 0.12381299585103989 loss_r 0.0349641814827919 loss_kl 0.08884881436824799\n",
      "step 282 loss 0.21649089455604553 loss_r 0.15149420499801636 loss_kl 0.06499668955802917\n",
      "step 283 loss 0.23456338047981262 loss_r 0.1672162413597107 loss_kl 0.06734713912010193\n",
      "step 284 loss 0.1564793735742569 loss_r 0.07239118218421936 loss_kl 0.08408819139003754\n",
      "step 285 loss 0.23367851972579956 loss_r 0.1818026453256607 loss_kl 0.051875874400138855\n",
      "step 286 loss 0.18404151499271393 loss_r 0.13234224915504456 loss_kl 0.051699262112379074\n",
      "step 287 loss 0.20177429914474487 loss_r 0.15451085567474365 loss_kl 0.04726345092058182\n",
      "step 288 loss 0.19075213372707367 loss_r 0.14225172996520996 loss_kl 0.04850040376186371\n",
      "step 289 loss 0.21342113614082336 loss_r 0.17273102700710297 loss_kl 0.0406901091337204\n",
      "step 290 loss 0.14128868281841278 loss_r 0.09237682819366455 loss_kl 0.04891185462474823\n",
      "step 291 loss 0.137381911277771 loss_r 0.09063423424959183 loss_kl 0.04674766957759857\n",
      "step 292 loss 0.16235411167144775 loss_r 0.10265523195266724 loss_kl 0.05969887971878052\n",
      "step 293 loss 0.11551003158092499 loss_r 0.05011418089270592 loss_kl 0.06539584696292877\n",
      "step 294 loss 0.2557418644428253 loss_r 0.2102440893650055 loss_kl 0.045497775077819824\n",
      "step 295 loss 0.16881465911865234 loss_r 0.10494651645421982 loss_kl 0.06386814266443253\n",
      "step 296 loss 0.17925387620925903 loss_r 0.10825102031230927 loss_kl 0.07100286334753036\n",
      "step 297 loss 0.1572386622428894 loss_r 0.09273245930671692 loss_kl 0.06450621038675308\n",
      "step 298 loss 0.2477172613143921 loss_r 0.19047921895980835 loss_kl 0.05723803490400314\n",
      "step 299 loss 0.08905496448278427 loss_r 0.02648807503283024 loss_kl 0.06256689131259918\n",
      "step 300 loss 0.1746893674135208 loss_r 0.1151103526353836 loss_kl 0.05957901105284691\n",
      "step 301 loss 0.22770094871520996 loss_r 0.1790832281112671 loss_kl 0.048617713153362274\n",
      "step 302 loss 0.2031930536031723 loss_r 0.14871414005756378 loss_kl 0.05447891727089882\n",
      "step 303 loss 0.09725846350193024 loss_r 0.04040722921490669 loss_kl 0.056851230561733246\n",
      "step 304 loss 0.15574607253074646 loss_r 0.11386875808238983 loss_kl 0.04187730699777603\n",
      "step 305 loss 0.12668243050575256 loss_r 0.0780596062541008 loss_kl 0.04862281680107117\n",
      "step 306 loss 0.1775013506412506 loss_r 0.1335710734128952 loss_kl 0.043930284678936005\n",
      "step 307 loss 0.15184348821640015 loss_r 0.10890454053878784 loss_kl 0.04293894022703171\n",
      "step 308 loss 0.1645965278148651 loss_r 0.12621253728866577 loss_kl 0.03838399797677994\n",
      "step 309 loss 0.1774393916130066 loss_r 0.13410203158855438 loss_kl 0.04333735257387161\n",
      "step 310 loss 0.15355877578258514 loss_r 0.10471168160438538 loss_kl 0.04884709417819977\n",
      "step 311 loss 0.14454083144664764 loss_r 0.08756056427955627 loss_kl 0.05698026716709137\n",
      "step 312 loss 0.13475874066352844 loss_r 0.08037914335727692 loss_kl 0.054379597306251526\n",
      "step 313 loss 0.14124339818954468 loss_r 0.08461014926433563 loss_kl 0.056633248925209045\n",
      "step 314 loss 0.22963987290859222 loss_r 0.18016093969345093 loss_kl 0.049478929489851\n",
      "step 315 loss 0.17843089997768402 loss_r 0.12332068383693695 loss_kl 0.05511021614074707\n",
      "step 316 loss 0.19128736853599548 loss_r 0.1374463140964508 loss_kl 0.05384105443954468\n",
      "step 317 loss 0.16643929481506348 loss_r 0.11319839954376221 loss_kl 0.05324089154601097\n",
      "step 318 loss 0.12631070613861084 loss_r 0.06580318510532379 loss_kl 0.06050751358270645\n",
      "step 319 loss 0.12672850489616394 loss_r 0.07457122206687927 loss_kl 0.052157290279865265\n",
      "step 320 loss 0.13062918186187744 loss_r 0.07991155982017517 loss_kl 0.050717614591121674\n",
      "step 321 loss 0.19126807153224945 loss_r 0.14777511358261108 loss_kl 0.04349295422434807\n",
      "step 322 loss 0.23894211649894714 loss_r 0.19985997676849365 loss_kl 0.03908213973045349\n",
      "step 323 loss 0.24322399497032166 loss_r 0.203124538064003 loss_kl 0.04009944945573807\n",
      "step 324 loss 0.20600217580795288 loss_r 0.16298800706863403 loss_kl 0.043014172464609146\n",
      "step 325 loss 0.1843038648366928 loss_r 0.13886231184005737 loss_kl 0.04544155299663544\n",
      "step 326 loss 0.17135056853294373 loss_r 0.12697438895702362 loss_kl 0.04437617212533951\n",
      "step 327 loss 0.1891636848449707 loss_r 0.13740995526313782 loss_kl 0.05175372213125229\n",
      "step 328 loss 0.28155001997947693 loss_r 0.22957386076450348 loss_kl 0.05197615548968315\n",
      "step 329 loss 0.13214141130447388 loss_r 0.06658382713794708 loss_kl 0.0655575841665268\n",
      "step 330 loss 0.12077122926712036 loss_r 0.04508233442902565 loss_kl 0.07568889856338501\n",
      "step 331 loss 0.1261242926120758 loss_r 0.06538259238004684 loss_kl 0.06074170023202896\n",
      "step 332 loss 0.23903727531433105 loss_r 0.18569692969322205 loss_kl 0.05334034934639931\n",
      "step 333 loss 0.20335383713245392 loss_r 0.1431499719619751 loss_kl 0.06020386889576912\n",
      "step 334 loss 0.1443740427494049 loss_r 0.09092272818088531 loss_kl 0.053451310843229294\n",
      "step 335 loss 0.17051154375076294 loss_r 0.12333659082651138 loss_kl 0.047174952924251556\n",
      "step 336 loss 0.12222173064947128 loss_r 0.07465483993291855 loss_kl 0.047566890716552734\n",
      "step 337 loss 0.19347049295902252 loss_r 0.15188483893871307 loss_kl 0.04158565029501915\n",
      "step 338 loss 0.1945425271987915 loss_r 0.1574947088956833 loss_kl 0.03704782575368881\n",
      "step 339 loss 0.14383545517921448 loss_r 0.10241085290908813 loss_kl 0.04142460599541664\n",
      "step 340 loss 0.09814347326755524 loss_r 0.04543682932853699 loss_kl 0.05270664021372795\n",
      "step 341 loss 0.20197857916355133 loss_r 0.15837273001670837 loss_kl 0.04360584542155266\n",
      "step 342 loss 0.17341741919517517 loss_r 0.1250334233045578 loss_kl 0.04838400334119797\n",
      "step 343 loss 0.1780381202697754 loss_r 0.12519650161266327 loss_kl 0.05284161493182182\n",
      "step 344 loss 0.1408483386039734 loss_r 0.06809472292661667 loss_kl 0.07275360822677612\n",
      "step 345 loss 0.16434040665626526 loss_r 0.1116708517074585 loss_kl 0.052669547498226166\n",
      "step 346 loss 0.1885511875152588 loss_r 0.1364336758852005 loss_kl 0.05211751163005829\n",
      "step 347 loss 0.0791536197066307 loss_r 0.0154045969247818 loss_kl 0.06374902278184891\n",
      "step 348 loss 0.18248486518859863 loss_r 0.13025327026844025 loss_kl 0.05223160237073898\n",
      "step 349 loss 0.2205922156572342 loss_r 0.16493269801139832 loss_kl 0.055659517645835876\n",
      "step 350 loss 0.1255861520767212 loss_r 0.052568405866622925 loss_kl 0.07301774621009827\n",
      "step 351 loss 0.12132808566093445 loss_r 0.061507657170295715 loss_kl 0.059820424765348434\n",
      "step 352 loss 0.21429912745952606 loss_r 0.14888109266757965 loss_kl 0.06541803479194641\n",
      "step 353 loss 0.15383923053741455 loss_r 0.0917700007557869 loss_kl 0.06206923723220825\n",
      "step 354 loss 0.14354319870471954 loss_r 0.09109746664762497 loss_kl 0.05244573578238487\n",
      "step 355 loss 0.17526865005493164 loss_r 0.1329393833875656 loss_kl 0.04232926666736603\n",
      "step 356 loss 0.2117396742105484 loss_r 0.15955540537834167 loss_kl 0.052184268832206726\n",
      "step 357 loss 0.17008940875530243 loss_r 0.12131375074386597 loss_kl 0.048775654286146164\n",
      "step 358 loss 0.22407951951026917 loss_r 0.18036802113056183 loss_kl 0.043711498379707336\n",
      "step 359 loss 0.18051758408546448 loss_r 0.12032513320446014 loss_kl 0.06019245833158493\n",
      "step 360 loss 0.12821587920188904 loss_r 0.05960764363408089 loss_kl 0.06860823929309845\n",
      "step 361 loss 0.17621180415153503 loss_r 0.12899351119995117 loss_kl 0.04721829295158386\n",
      "step 362 loss 0.20040901005268097 loss_r 0.15529973804950714 loss_kl 0.04510926827788353\n",
      "step 363 loss 0.1830853968858719 loss_r 0.13226507604122162 loss_kl 0.05082032084465027\n",
      "step 364 loss 0.14756721258163452 loss_r 0.10076111555099487 loss_kl 0.04680609703063965\n",
      "step 365 loss 0.1785593330860138 loss_r 0.12766502797603607 loss_kl 0.05089430883526802\n",
      "step 366 loss 0.11761704087257385 loss_r 0.06492850929498672 loss_kl 0.05268852785229683\n",
      "step 367 loss 0.11239629238843918 loss_r 0.06295650452375412 loss_kl 0.04943978786468506\n",
      "step 368 loss 0.13365228474140167 loss_r 0.09353411197662354 loss_kl 0.04011817276477814\n",
      "step 369 loss 0.10556589066982269 loss_r 0.0628315806388855 loss_kl 0.042734310030937195\n",
      "step 370 loss 0.09908629953861237 loss_r 0.04627374932169914 loss_kl 0.05281255394220352\n",
      "step 371 loss 0.279531329870224 loss_r 0.23133914172649384 loss_kl 0.048192188143730164\n",
      "step 372 loss 0.14488399028778076 loss_r 0.07065649330615997 loss_kl 0.07422749698162079\n",
      "step 373 loss 0.19164666533470154 loss_r 0.12598556280136108 loss_kl 0.06566110253334045\n",
      "step 374 loss 0.12228920310735703 loss_r 0.05225330591201782 loss_kl 0.0700358971953392\n",
      "step 375 loss 0.2388552725315094 loss_r 0.17943748831748962 loss_kl 0.05941779166460037\n",
      "step 376 loss 0.19390778243541718 loss_r 0.12700103223323822 loss_kl 0.06690675020217896\n",
      "step 377 loss 0.13465921580791473 loss_r 0.07834775745868683 loss_kl 0.056311458349227905\n",
      "step 378 loss 0.12054158002138138 loss_r 0.06308843940496445 loss_kl 0.05745314061641693\n",
      "step 379 loss 0.11415968835353851 loss_r 0.06305810064077377 loss_kl 0.05110158771276474\n",
      "step 380 loss 0.13267800211906433 loss_r 0.08332500606775284 loss_kl 0.04935300350189209\n",
      "step 381 loss 0.09435727447271347 loss_r 0.04594229534268379 loss_kl 0.04841497913002968\n",
      "step 382 loss 0.2168273776769638 loss_r 0.17671413719654083 loss_kl 0.040113240480422974\n",
      "step 383 loss 0.21247181296348572 loss_r 0.15864197909832 loss_kl 0.05382983386516571\n",
      "step 384 loss 0.13931897282600403 loss_r 0.07184714078903198 loss_kl 0.06747183203697205\n",
      "step 385 loss 0.08848439902067184 loss_r 0.02837260439991951 loss_kl 0.060111794620752335\n",
      "step 386 loss 0.07545463740825653 loss_r 0.012908496893942356 loss_kl 0.06254614144563675\n",
      "step 387 loss 0.09220236539840698 loss_r 0.031957533210515976 loss_kl 0.060244835913181305\n",
      "step 388 loss 0.08585785329341888 loss_r 0.02657630667090416 loss_kl 0.05928155034780502\n",
      "step 389 loss 0.16036280989646912 loss_r 0.11169362813234329 loss_kl 0.048669178038835526\n",
      "step 390 loss 0.1832127571105957 loss_r 0.1379162222146988 loss_kl 0.04529654234647751\n",
      "step 391 loss 0.12320449203252792 loss_r 0.07029199600219727 loss_kl 0.05291249603033066\n",
      "step 392 loss 0.11651080846786499 loss_r 0.07116132229566574 loss_kl 0.04534948617219925\n",
      "step 393 loss 0.22829456627368927 loss_r 0.18709290027618408 loss_kl 0.041201669722795486\n",
      "step 394 loss 0.0824713408946991 loss_r 0.022920243442058563 loss_kl 0.05955110117793083\n",
      "step 395 loss 0.12769998610019684 loss_r 0.07245636731386185 loss_kl 0.05524361878633499\n",
      "step 396 loss 0.21800675988197327 loss_r 0.17213140428066254 loss_kl 0.04587535560131073\n",
      "step 397 loss 0.1997201293706894 loss_r 0.14676830172538757 loss_kl 0.05295182764530182\n",
      "step 398 loss 0.1927102953195572 loss_r 0.13444030284881592 loss_kl 0.05826999247074127\n",
      "Epoch 3\n",
      "step 0 loss 0.1300525814294815 loss_r 0.050389986485242844 loss_kl 0.07966259121894836\n",
      "step 1 loss 0.1871042549610138 loss_r 0.11340370774269104 loss_kl 0.07370054721832275\n",
      "step 2 loss 0.20333024859428406 loss_r 0.14388683438301086 loss_kl 0.059443410485982895\n",
      "step 3 loss 0.10178495943546295 loss_r 0.03325657546520233 loss_kl 0.06852838397026062\n",
      "step 4 loss 0.18486708402633667 loss_r 0.12248078733682632 loss_kl 0.062386296689510345\n",
      "step 5 loss 0.3297688066959381 loss_r 0.2800937592983246 loss_kl 0.04967503994703293\n",
      "step 6 loss 0.15555405616760254 loss_r 0.08504923433065414 loss_kl 0.0705048218369484\n",
      "step 7 loss 0.15818995237350464 loss_r 0.09702178835868835 loss_kl 0.06116817146539688\n",
      "step 8 loss 0.1578790545463562 loss_r 0.10067944973707199 loss_kl 0.05719959735870361\n",
      "step 9 loss 0.14928928017616272 loss_r 0.08734138309955597 loss_kl 0.061947889626026154\n",
      "step 10 loss 0.16822904348373413 loss_r 0.11739445477724075 loss_kl 0.05083458125591278\n",
      "step 11 loss 0.1517370492219925 loss_r 0.10236689448356628 loss_kl 0.04937015473842621\n",
      "step 12 loss 0.10937057435512543 loss_r 0.06198494881391525 loss_kl 0.047385625541210175\n",
      "step 13 loss 0.07553320378065109 loss_r 0.023273799568414688 loss_kl 0.052259404212236404\n",
      "step 14 loss 0.11759887635707855 loss_r 0.06627380102872849 loss_kl 0.051325079053640366\n",
      "step 15 loss 0.1490267813205719 loss_r 0.10567964613437653 loss_kl 0.04334712773561478\n",
      "step 16 loss 0.1940414011478424 loss_r 0.1546851247549057 loss_kl 0.0393562838435173\n",
      "step 17 loss 0.042817141860723495 loss_r -0.003119433531537652 loss_kl 0.04593657702207565\n",
      "step 18 loss 0.18658694624900818 loss_r 0.13734644651412964 loss_kl 0.04924050346016884\n",
      "step 19 loss 0.21679407358169556 loss_r 0.1657269299030304 loss_kl 0.051067136228084564\n",
      "step 20 loss 0.05808468163013458 loss_r -0.026236822828650475 loss_kl 0.0843215063214302\n",
      "step 21 loss 0.2242985963821411 loss_r 0.151709645986557 loss_kl 0.0725889503955841\n",
      "step 22 loss 0.07987464964389801 loss_r -0.013542364351451397 loss_kl 0.09341701120138168\n",
      "step 23 loss 0.15175186097621918 loss_r 0.089214026927948 loss_kl 0.06253783404827118\n",
      "step 24 loss 0.17173972725868225 loss_r 0.11094672232866287 loss_kl 0.06079300120472908\n",
      "step 25 loss 0.06626085937023163 loss_r 0.004217620939016342 loss_kl 0.062043242156505585\n",
      "step 26 loss 0.10914139449596405 loss_r 0.04395265877246857 loss_kl 0.06518873572349548\n",
      "step 27 loss 0.1812543123960495 loss_r 0.13200804591178894 loss_kl 0.04924626275897026\n",
      "step 28 loss 0.19154314696788788 loss_r 0.1484781801700592 loss_kl 0.043064966797828674\n",
      "step 29 loss 0.08195269107818604 loss_r 0.028952524065971375 loss_kl 0.05300016328692436\n",
      "step 30 loss 0.09248348325490952 loss_r 0.035892657935619354 loss_kl 0.05659082531929016\n",
      "step 31 loss 0.2380739450454712 loss_r 0.19803287088871002 loss_kl 0.040041081607341766\n",
      "step 32 loss 0.24444067478179932 loss_r 0.19925540685653687 loss_kl 0.04518527537584305\n",
      "step 33 loss 0.13472405076026917 loss_r 0.08307962864637375 loss_kl 0.05164442956447601\n",
      "step 34 loss 0.1564878523349762 loss_r 0.10201233625411987 loss_kl 0.05447551608085632\n",
      "step 35 loss 0.13005205988883972 loss_r 0.06907206028699875 loss_kl 0.06098000332713127\n",
      "step 36 loss 0.13406863808631897 loss_r 0.06975461542606354 loss_kl 0.06431401520967484\n",
      "step 37 loss 0.14738987386226654 loss_r 0.08602297306060791 loss_kl 0.06136690080165863\n",
      "step 38 loss 0.10175813734531403 loss_r 0.03720327094197273 loss_kl 0.06455487012863159\n",
      "step 39 loss 0.07377505302429199 loss_r 0.014705898240208626 loss_kl 0.059069156646728516\n",
      "step 40 loss 0.2008647322654724 loss_r 0.1497868448495865 loss_kl 0.051077891141176224\n",
      "step 41 loss 0.1456744372844696 loss_r 0.09491259604692459 loss_kl 0.05076184123754501\n",
      "step 42 loss 0.17182663083076477 loss_r 0.12073592096567154 loss_kl 0.05109070986509323\n",
      "step 43 loss 0.18721330165863037 loss_r 0.14322039484977722 loss_kl 0.04399291053414345\n",
      "step 44 loss 0.12151296436786652 loss_r 0.07175575196743011 loss_kl 0.0497572124004364\n",
      "step 45 loss 0.1095966100692749 loss_r 0.061078451573848724 loss_kl 0.04851815849542618\n",
      "step 46 loss 0.054770659655332565 loss_r 0.0054629179649055 loss_kl 0.04930774122476578\n",
      "step 47 loss 0.24622273445129395 loss_r 0.2058495581150055 loss_kl 0.04037317633628845\n",
      "step 48 loss 0.1526886522769928 loss_r 0.10135145485401154 loss_kl 0.05133719742298126\n",
      "step 49 loss 0.1812654435634613 loss_r 0.11706605553627014 loss_kl 0.06419938057661057\n",
      "step 50 loss 0.13107722997665405 loss_r 0.05028161406517029 loss_kl 0.08079562336206436\n",
      "step 51 loss 0.23654931783676147 loss_r 0.17423611879348755 loss_kl 0.06231319159269333\n",
      "step 52 loss 0.11634320765733719 loss_r 0.05005430430173874 loss_kl 0.06628890335559845\n",
      "step 53 loss 0.10877969861030579 loss_r 0.03766961395740509 loss_kl 0.0711100846529007\n",
      "step 54 loss 0.16763651371002197 loss_r 0.10878518968820572 loss_kl 0.058851320296525955\n",
      "step 55 loss 0.10042234510183334 loss_r 0.03818783164024353 loss_kl 0.06223451346158981\n",
      "step 56 loss 0.08632424473762512 loss_r 0.040451616048812866 loss_kl 0.045872628688812256\n",
      "step 57 loss 0.13355888426303864 loss_r 0.08831129968166351 loss_kl 0.04524758458137512\n",
      "step 58 loss 0.04274176061153412 loss_r -0.03167903423309326 loss_kl 0.07442079484462738\n",
      "step 59 loss 0.1495194137096405 loss_r 0.10196815431118011 loss_kl 0.04755125194787979\n",
      "step 60 loss 0.09891127049922943 loss_r 0.04322851821780205 loss_kl 0.05568275600671768\n",
      "step 61 loss 0.19838395714759827 loss_r 0.15334495902061462 loss_kl 0.045038990676403046\n",
      "step 62 loss 0.07901564240455627 loss_r 0.017551239579916 loss_kl 0.06146440654993057\n",
      "step 63 loss 0.2245592623949051 loss_r 0.17987515032291412 loss_kl 0.04468411207199097\n",
      "step 64 loss 0.11745285987854004 loss_r 0.05893205478787422 loss_kl 0.05852080136537552\n",
      "step 65 loss 0.1298605501651764 loss_r 0.08134414255619049 loss_kl 0.0485164076089859\n",
      "step 66 loss 0.18766888976097107 loss_r 0.13631395995616913 loss_kl 0.05135493725538254\n",
      "step 67 loss 0.15176165103912354 loss_r 0.09498388320207596 loss_kl 0.056777775287628174\n",
      "step 68 loss 0.0995442196726799 loss_r 0.0399344377219677 loss_kl 0.059609781950712204\n",
      "step 69 loss 0.1568986475467682 loss_r 0.10917741805315018 loss_kl 0.04772122576832771\n",
      "step 70 loss 0.10401260852813721 loss_r 0.051176637411117554 loss_kl 0.05283597111701965\n",
      "step 71 loss 0.1948147863149643 loss_r 0.15227405726909637 loss_kl 0.04254072904586792\n",
      "step 72 loss 0.11910398304462433 loss_r 0.07190778106451035 loss_kl 0.04719620198011398\n",
      "step 73 loss 0.10385048389434814 loss_r 0.05952420458197594 loss_kl 0.04432627558708191\n",
      "step 74 loss 0.12604586780071259 loss_r 0.07326676696538925 loss_kl 0.052779097110033035\n",
      "step 75 loss 0.14618101716041565 loss_r 0.10315380990505219 loss_kl 0.04302721470594406\n",
      "step 76 loss 0.23192550241947174 loss_r 0.189944788813591 loss_kl 0.04198071360588074\n",
      "step 77 loss 0.21132248640060425 loss_r 0.15712083876132965 loss_kl 0.0542016476392746\n",
      "step 78 loss 0.09689950197935104 loss_r 0.04202760383486748 loss_kl 0.054871898144483566\n",
      "step 79 loss 0.09556016325950623 loss_r 0.03297998383641243 loss_kl 0.0625801831483841\n",
      "step 80 loss 0.10864083468914032 loss_r 0.04278478026390076 loss_kl 0.06585605442523956\n",
      "step 81 loss 0.025542568415403366 loss_r -0.042442645877599716 loss_kl 0.06798521429300308\n",
      "step 82 loss 0.1269863247871399 loss_r 0.06854436546564102 loss_kl 0.058441951870918274\n",
      "step 83 loss 0.16522708535194397 loss_r 0.11231786757707596 loss_kl 0.05290921404957771\n",
      "step 84 loss 0.19952332973480225 loss_r 0.14838853478431702 loss_kl 0.05113479122519493\n",
      "step 85 loss 0.10361279547214508 loss_r 0.05147631838917732 loss_kl 0.05213647335767746\n",
      "step 86 loss 0.06439371407032013 loss_r 0.00933776143938303 loss_kl 0.055055949836969376\n",
      "step 87 loss 0.33234426379203796 loss_r 0.29692956805229187 loss_kl 0.0354146882891655\n",
      "step 88 loss 0.08883828669786453 loss_r 0.03814908489584923 loss_kl 0.050689201802015305\n",
      "step 89 loss 0.1466735452413559 loss_r 0.09808337688446045 loss_kl 0.04859016835689545\n",
      "step 90 loss 0.14218780398368835 loss_r 0.08204802125692368 loss_kl 0.060139790177345276\n",
      "step 91 loss 0.2426670491695404 loss_r 0.19071082770824432 loss_kl 0.05195622891187668\n",
      "step 92 loss 0.24833369255065918 loss_r 0.1888532042503357 loss_kl 0.05948048457503319\n",
      "step 93 loss 0.1855364441871643 loss_r 0.11265536397695541 loss_kl 0.07288108766078949\n",
      "step 94 loss 0.15747544169425964 loss_r 0.07614218443632126 loss_kl 0.08133325725793839\n",
      "step 95 loss 0.15213948488235474 loss_r 0.08050325512886047 loss_kl 0.07163622975349426\n",
      "step 96 loss 0.19091194868087769 loss_r 0.12550833821296692 loss_kl 0.06540361046791077\n",
      "step 97 loss 0.2277529537677765 loss_r 0.1661449819803238 loss_kl 0.0616079643368721\n",
      "step 98 loss 0.1340426504611969 loss_r 0.07500993460416794 loss_kl 0.05903272330760956\n",
      "step 99 loss 0.12042617052793503 loss_r 0.05512676388025284 loss_kl 0.06529940664768219\n",
      "step 100 loss 0.14287784695625305 loss_r 0.09449391812086105 loss_kl 0.048383928835392\n",
      "step 101 loss 0.15924176573753357 loss_r 0.11528428643941879 loss_kl 0.043957486748695374\n",
      "step 102 loss 0.11276186257600784 loss_r 0.06152806431055069 loss_kl 0.05123379826545715\n",
      "step 103 loss 0.20997300744056702 loss_r 0.1666044443845749 loss_kl 0.043368563055992126\n",
      "step 104 loss 0.16937246918678284 loss_r 0.12504640221595764 loss_kl 0.044326066970825195\n",
      "step 105 loss 0.09643282741308212 loss_r 0.03920002654194832 loss_kl 0.057232800871133804\n",
      "step 106 loss 0.13995066285133362 loss_r 0.09193327277898788 loss_kl 0.04801739752292633\n",
      "step 107 loss 0.1348268836736679 loss_r 0.08215808868408203 loss_kl 0.052668794989585876\n",
      "step 108 loss 0.13772645592689514 loss_r 0.08313935995101929 loss_kl 0.05458708852529526\n",
      "step 109 loss 0.18311229348182678 loss_r 0.12786853313446045 loss_kl 0.055243752896785736\n",
      "step 110 loss 0.10148251056671143 loss_r 0.04083486273884773 loss_kl 0.06064764782786369\n",
      "step 111 loss 0.09622711688280106 loss_r 0.03921970725059509 loss_kl 0.05700740963220596\n",
      "step 112 loss 0.13356059789657593 loss_r 0.08680877834558487 loss_kl 0.04675181955099106\n",
      "step 113 loss 0.21532146632671356 loss_r 0.1648121327161789 loss_kl 0.05050932988524437\n",
      "step 114 loss 0.09900837391614914 loss_r 0.02625836431980133 loss_kl 0.07275000959634781\n",
      "step 115 loss 0.15533536672592163 loss_r 0.09430953860282898 loss_kl 0.06102583557367325\n",
      "step 116 loss 0.16945502161979675 loss_r 0.10934903472661972 loss_kl 0.060105979442596436\n",
      "step 117 loss 0.13986176252365112 loss_r 0.06977591663599014 loss_kl 0.07008585333824158\n",
      "step 118 loss 0.16498887538909912 loss_r 0.10841227322816849 loss_kl 0.056576598435640335\n",
      "step 119 loss 0.09535937011241913 loss_r 0.03991606459021568 loss_kl 0.05544330179691315\n",
      "step 120 loss 0.11559828370809555 loss_r 0.06145972013473511 loss_kl 0.05413856357336044\n",
      "step 121 loss 0.09703372418880463 loss_r 0.04219130799174309 loss_kl 0.05484241992235184\n",
      "step 122 loss 0.2079308182001114 loss_r 0.16647064685821533 loss_kl 0.04146016761660576\n",
      "step 123 loss 0.08892663568258286 loss_r 0.03787832707166672 loss_kl 0.05104830861091614\n",
      "step 124 loss 0.18840725719928741 loss_r 0.1389445811510086 loss_kl 0.04946267977356911\n",
      "step 125 loss 0.18555934727191925 loss_r 0.13139687478542328 loss_kl 0.05416247248649597\n",
      "step 126 loss 0.13964304327964783 loss_r 0.08667647838592529 loss_kl 0.052966564893722534\n",
      "step 127 loss 0.10356076806783676 loss_r 0.04124188795685768 loss_kl 0.06231888011097908\n",
      "step 128 loss 0.10028445720672607 loss_r 0.035243529826402664 loss_kl 0.06504093110561371\n",
      "step 129 loss 0.21138688921928406 loss_r 0.15358783304691315 loss_kl 0.05779905617237091\n",
      "step 130 loss 0.11824275553226471 loss_r 0.046089958399534225 loss_kl 0.07215279340744019\n",
      "step 131 loss 0.20192119479179382 loss_r 0.15180985629558563 loss_kl 0.050111331045627594\n",
      "step 132 loss 0.1319805085659027 loss_r 0.0732729360461235 loss_kl 0.0587075799703598\n",
      "step 133 loss 0.10668899118900299 loss_r 0.04625048115849495 loss_kl 0.06043851375579834\n",
      "step 134 loss 0.11624568700790405 loss_r 0.04730711504817009 loss_kl 0.06893857568502426\n",
      "step 135 loss 0.13855554163455963 loss_r 0.08998502790927887 loss_kl 0.04857050999999046\n",
      "step 136 loss 0.04400021955370903 loss_r -0.01803528144955635 loss_kl 0.06203550100326538\n",
      "step 137 loss 0.05412113666534424 loss_r -0.010317967273294926 loss_kl 0.06443910300731659\n",
      "step 138 loss 0.0754488930106163 loss_r 0.012010759674012661 loss_kl 0.06343813240528107\n",
      "step 139 loss 0.08581127226352692 loss_r 0.028353840112686157 loss_kl 0.05745742842555046\n",
      "step 140 loss 0.18408849835395813 loss_r 0.13676536083221436 loss_kl 0.047323137521743774\n",
      "step 141 loss 0.10464298725128174 loss_r 0.05230104923248291 loss_kl 0.05234194174408913\n",
      "step 142 loss 0.1203797310590744 loss_r 0.05546405166387558 loss_kl 0.06491567939519882\n",
      "step 143 loss 0.019983042031526566 loss_r -0.05017077550292015 loss_kl 0.07015381753444672\n",
      "step 144 loss 0.023591432720422745 loss_r -0.04533671960234642 loss_kl 0.06892815232276917\n",
      "step 145 loss 0.08330260962247849 loss_r 0.03306179493665695 loss_kl 0.05024081468582153\n",
      "step 146 loss 0.0670255571603775 loss_r 0.008309164084494114 loss_kl 0.05871639400720596\n",
      "step 147 loss 0.12406591325998306 loss_r 0.0849413275718689 loss_kl 0.039124585688114166\n",
      "step 148 loss 0.18318812549114227 loss_r 0.14343911409378052 loss_kl 0.039749011397361755\n",
      "step 149 loss 0.2169005572795868 loss_r 0.17439670860767365 loss_kl 0.04250384867191315\n",
      "step 150 loss 0.0012822449207305908 loss_r -0.05289335548877716 loss_kl 0.05417560040950775\n",
      "step 151 loss 0.10479912161827087 loss_r 0.04854012653231621 loss_kl 0.056258998811244965\n",
      "step 152 loss 0.18539276719093323 loss_r 0.12069510668516159 loss_kl 0.06469765305519104\n",
      "step 153 loss 0.11380881071090698 loss_r 0.04484826326370239 loss_kl 0.06896054744720459\n",
      "step 154 loss 0.20092777907848358 loss_r 0.1308615505695343 loss_kl 0.07006622850894928\n",
      "step 155 loss 0.18498766422271729 loss_r 0.12327329814434052 loss_kl 0.06171437352895737\n",
      "step 156 loss 0.21995937824249268 loss_r 0.15674588084220886 loss_kl 0.06321350485086441\n",
      "step 157 loss 0.12996640801429749 loss_r 0.05761803314089775 loss_kl 0.07234837859869003\n",
      "step 158 loss 0.23443368077278137 loss_r 0.16991843283176422 loss_kl 0.06451525539159775\n",
      "step 159 loss 0.04669050872325897 loss_r -0.021945836022496223 loss_kl 0.06863634288311005\n",
      "step 160 loss 0.12926511466503143 loss_r 0.06790617853403091 loss_kl 0.06135893613100052\n",
      "step 161 loss 0.08845257759094238 loss_r 0.03420520946383476 loss_kl 0.05424736812710762\n",
      "step 162 loss 0.07888520509004593 loss_r 0.022034646943211555 loss_kl 0.056850556284189224\n",
      "step 163 loss 0.37547311186790466 loss_r 0.33455196022987366 loss_kl 0.0409211590886116\n",
      "step 164 loss 0.24819141626358032 loss_r 0.202912375330925 loss_kl 0.04527904465794563\n",
      "step 165 loss 0.12129075825214386 loss_r 0.06099381670355797 loss_kl 0.06029694527387619\n",
      "step 166 loss 0.062135279178619385 loss_r 0.0002710461849346757 loss_kl 0.06186423450708389\n",
      "step 167 loss 0.14675700664520264 loss_r 0.07850291579961777 loss_kl 0.06825409829616547\n",
      "step 168 loss 0.10764250159263611 loss_r 0.040450289845466614 loss_kl 0.0671922117471695\n",
      "step 169 loss 0.16974394023418427 loss_r 0.11065419018268585 loss_kl 0.05908975005149841\n",
      "step 170 loss 0.18464501202106476 loss_r 0.1269458383321762 loss_kl 0.05769917368888855\n",
      "step 171 loss 0.14004437625408173 loss_r 0.08026739954948425 loss_kl 0.059776972979307175\n",
      "step 172 loss 0.09924851357936859 loss_r 0.040335189551115036 loss_kl 0.058913327753543854\n",
      "step 173 loss 0.16989532113075256 loss_r 0.11883945018053055 loss_kl 0.051055870950222015\n",
      "step 174 loss 0.16396644711494446 loss_r 0.11539392173290253 loss_kl 0.048572517931461334\n",
      "step 175 loss 0.18355943262577057 loss_r 0.1321166604757309 loss_kl 0.051442768424749374\n",
      "step 176 loss 0.08385172486305237 loss_r 0.03563109412789345 loss_kl 0.04822062700986862\n",
      "step 177 loss 0.03153393417596817 loss_r -0.01436474360525608 loss_kl 0.0458986796438694\n",
      "step 178 loss 0.10097511112689972 loss_r 0.05406767129898071 loss_kl 0.046907439827919006\n",
      "step 179 loss 0.21825864911079407 loss_r 0.1774323284626007 loss_kl 0.04082631319761276\n",
      "step 180 loss 0.10333825647830963 loss_r 0.05537668243050575 loss_kl 0.04796157032251358\n",
      "step 181 loss 0.12436629086732864 loss_r 0.07276998460292816 loss_kl 0.05159630626440048\n",
      "step 182 loss 0.06144960969686508 loss_r -0.00809909962117672 loss_kl 0.06954871118068695\n",
      "step 183 loss 0.0974302813410759 loss_r 0.04065845161676407 loss_kl 0.05677182972431183\n",
      "step 184 loss 0.16889545321464539 loss_r 0.11433088779449463 loss_kl 0.05456456542015076\n",
      "step 185 loss 0.13853321969509125 loss_r 0.07566438615322113 loss_kl 0.06286883354187012\n",
      "step 186 loss 0.2351451814174652 loss_r 0.1825515180826187 loss_kl 0.052593670785427094\n",
      "step 187 loss 0.10789386183023453 loss_r 0.04364944249391556 loss_kl 0.06424441933631897\n",
      "step 188 loss 0.16173914074897766 loss_r 0.10021604597568512 loss_kl 0.06152310222387314\n",
      "step 189 loss 0.03458254039287567 loss_r -0.04096134752035141 loss_kl 0.07554388791322708\n",
      "step 190 loss 0.10522390902042389 loss_r 0.043712977319955826 loss_kl 0.06151093542575836\n",
      "step 191 loss 0.10290054231882095 loss_r 0.03258118778467178 loss_kl 0.07031935453414917\n",
      "step 192 loss 0.05294011905789375 loss_r -0.009764413349330425 loss_kl 0.06270453333854675\n",
      "step 193 loss 0.15656410157680511 loss_r 0.09994817525148392 loss_kl 0.0566159226000309\n",
      "step 194 loss 0.12266271561384201 loss_r 0.06827886402606964 loss_kl 0.05438385158777237\n",
      "step 195 loss 0.1291319578886032 loss_r 0.07032574713230133 loss_kl 0.05880621075630188\n",
      "step 196 loss 0.10918457061052322 loss_r 0.05645276978611946 loss_kl 0.05273180082440376\n",
      "step 197 loss 0.16898924112319946 loss_r 0.11486880481243134 loss_kl 0.054120440036058426\n",
      "step 198 loss 0.09782139956951141 loss_r 0.029397234320640564 loss_kl 0.06842416524887085\n",
      "step 199 loss 0.09184689819812775 loss_r 0.02335488609969616 loss_kl 0.06849201023578644\n",
      "step 200 loss 0.05008786916732788 loss_r -0.012064908631145954 loss_kl 0.06215277686715126\n",
      "step 201 loss 0.17079532146453857 loss_r 0.12048698216676712 loss_kl 0.050308339297771454\n",
      "step 202 loss 0.09070815145969391 loss_r 0.01713709346950054 loss_kl 0.07357105612754822\n",
      "step 203 loss 0.05149052292108536 loss_r -0.008700142614543438 loss_kl 0.06019066646695137\n",
      "step 204 loss 0.079948291182518 loss_r 0.028540190309286118 loss_kl 0.051408104598522186\n",
      "step 205 loss 0.22833281755447388 loss_r 0.1776508092880249 loss_kl 0.050682008266448975\n",
      "step 206 loss 0.15706351399421692 loss_r 0.09497040510177612 loss_kl 0.062093108892440796\n",
      "step 207 loss 0.09862228482961655 loss_r 0.02805423177778721 loss_kl 0.07056805491447449\n",
      "step 208 loss 0.1420939564704895 loss_r 0.07235047221183777 loss_kl 0.06974348425865173\n",
      "step 209 loss 0.13994017243385315 loss_r 0.07345334440469742 loss_kl 0.06648682802915573\n",
      "step 210 loss 0.07507740706205368 loss_r 0.012899557128548622 loss_kl 0.06217785179615021\n",
      "step 211 loss 0.10529544949531555 loss_r 0.04499759152531624 loss_kl 0.06029786169528961\n",
      "step 212 loss 0.12014801800251007 loss_r 0.06769160181283951 loss_kl 0.05245641991496086\n",
      "step 213 loss 0.03759029507637024 loss_r -0.025120705366134644 loss_kl 0.06271100044250488\n",
      "step 214 loss 0.031039059162139893 loss_r -0.01819460839033127 loss_kl 0.04923366755247116\n",
      "step 215 loss 0.10389621555805206 loss_r 0.04242418333888054 loss_kl 0.061472028493881226\n",
      "step 216 loss 0.12676823139190674 loss_r 0.07734545320272446 loss_kl 0.04942277818918228\n",
      "step 217 loss 0.06906015425920486 loss_r 0.017764966934919357 loss_kl 0.05129518732428551\n",
      "step 218 loss 0.03859540820121765 loss_r -0.0210049320012331 loss_kl 0.0596003383398056\n",
      "step 219 loss 0.10050786286592484 loss_r 0.04782779514789581 loss_kl 0.05268006771802902\n",
      "step 220 loss 0.11232568323612213 loss_r 0.06297478079795837 loss_kl 0.04935090243816376\n",
      "step 221 loss 0.04321493208408356 loss_r -0.015429208055138588 loss_kl 0.058644142001867294\n",
      "step 222 loss 0.19835732877254486 loss_r 0.14880488812923431 loss_kl 0.04955243691802025\n",
      "step 223 loss 0.20377293229103088 loss_r 0.15227164328098297 loss_kl 0.05150129646062851\n",
      "step 224 loss 0.1885930597782135 loss_r 0.13597384095191956 loss_kl 0.05261921137571335\n",
      "step 225 loss 0.0034910589456558228 loss_r -0.07491079717874527 loss_kl 0.07840185612440109\n",
      "step 226 loss 0.1969606876373291 loss_r 0.14132331311702728 loss_kl 0.05563737824559212\n",
      "step 227 loss 0.10150102525949478 loss_r 0.030404673889279366 loss_kl 0.07109635323286057\n",
      "step 228 loss 0.1547873169183731 loss_r 0.09814465045928955 loss_kl 0.05664266645908356\n",
      "step 229 loss 0.09609480202198029 loss_r 0.034121353179216385 loss_kl 0.0619734525680542\n",
      "step 230 loss 0.05587349086999893 loss_r -0.0022555741015821695 loss_kl 0.05812906473875046\n",
      "step 231 loss 0.12838467955589294 loss_r 0.07837357372045517 loss_kl 0.050011105835437775\n",
      "step 232 loss 0.14013035595417023 loss_r 0.09502542018890381 loss_kl 0.04510493576526642\n",
      "step 233 loss 0.12687331438064575 loss_r 0.07839129865169525 loss_kl 0.048482008278369904\n",
      "step 234 loss 0.1387353241443634 loss_r 0.09107265621423721 loss_kl 0.04766267538070679\n",
      "step 235 loss 0.1601179838180542 loss_r 0.11209958046674728 loss_kl 0.04801841080188751\n",
      "step 236 loss 0.03409768268465996 loss_r -0.021946344524621964 loss_kl 0.05604402720928192\n",
      "step 237 loss 0.23297171294689178 loss_r 0.17989270389080048 loss_kl 0.05307900533080101\n",
      "step 238 loss 0.12056317925453186 loss_r 0.04599470645189285 loss_kl 0.07456847280263901\n",
      "step 239 loss 0.07143543660640717 loss_r -0.029072005301713943 loss_kl 0.10050743818283081\n",
      "step 240 loss 0.16393837332725525 loss_r 0.08499862253665924 loss_kl 0.07893975079059601\n",
      "step 241 loss 0.12195931375026703 loss_r 0.03339298442006111 loss_kl 0.08856633305549622\n",
      "step 242 loss 0.07947658747434616 loss_r -0.0018454574747011065 loss_kl 0.08132204413414001\n",
      "step 243 loss 0.14397317171096802 loss_r 0.07360991835594177 loss_kl 0.07036325335502625\n",
      "step 244 loss 0.15836726129055023 loss_r 0.10201855003833771 loss_kl 0.056348707526922226\n",
      "step 245 loss 0.044882964342832565 loss_r -0.01740260049700737 loss_kl 0.062285564839839935\n",
      "step 246 loss 0.058346204459667206 loss_r -0.003053421853110194 loss_kl 0.06139962747693062\n",
      "step 247 loss 0.05212947726249695 loss_r -0.006358325015753508 loss_kl 0.05848780274391174\n",
      "step 248 loss 0.07709209620952606 loss_r 0.01632312312722206 loss_kl 0.0607689768075943\n",
      "step 249 loss 0.10139470547437668 loss_r 0.04756581410765648 loss_kl 0.0538288913667202\n",
      "step 250 loss 0.13498850166797638 loss_r 0.08074937015771866 loss_kl 0.05423913151025772\n",
      "step 251 loss 0.05358974635601044 loss_r -0.0023305972572416067 loss_kl 0.05592034384608269\n",
      "step 252 loss -0.0028116554021835327 loss_r -0.08754105865955353 loss_kl 0.08472940325737\n",
      "step 253 loss 0.16631528735160828 loss_r 0.11465011537075043 loss_kl 0.05166517198085785\n",
      "step 254 loss 0.0640747994184494 loss_r 0.006639840546995401 loss_kl 0.05743496119976044\n",
      "step 255 loss 0.007336534559726715 loss_r -0.07486921548843384 loss_kl 0.08220575004816055\n",
      "step 256 loss 0.09716236591339111 loss_r 0.03826925903558731 loss_kl 0.0588931068778038\n",
      "step 257 loss 0.08829750120639801 loss_r 0.03003672882914543 loss_kl 0.05826077610254288\n",
      "step 258 loss 0.1315399706363678 loss_r 0.0735841616988182 loss_kl 0.05795580893754959\n",
      "step 259 loss 0.005972627550363541 loss_r -0.05173297971487045 loss_kl 0.057705607265233994\n",
      "step 260 loss 0.2191922962665558 loss_r 0.1655644178390503 loss_kl 0.05362788587808609\n",
      "step 261 loss 0.07066091895103455 loss_r 0.003951893653720617 loss_kl 0.06670902669429779\n",
      "step 262 loss 0.1387713998556137 loss_r 0.07916326075792313 loss_kl 0.05960814282298088\n",
      "step 263 loss 0.03524652123451233 loss_r -0.033249929547309875 loss_kl 0.0684964507818222\n",
      "step 264 loss 0.1372559368610382 loss_r 0.07600723952054977 loss_kl 0.06124868988990784\n",
      "step 265 loss 0.14318066835403442 loss_r 0.07868701964616776 loss_kl 0.06449364870786667\n",
      "step 266 loss 0.06325839459896088 loss_r -0.01419823057949543 loss_kl 0.07745662331581116\n",
      "step 267 loss 0.06337053328752518 loss_r -0.019935868680477142 loss_kl 0.08330640196800232\n",
      "step 268 loss 0.008854217827320099 loss_r -0.07350204885005951 loss_kl 0.08235626667737961\n",
      "step 269 loss 0.14986053109169006 loss_r 0.08286505937576294 loss_kl 0.06699547916650772\n",
      "step 270 loss 0.03739370033144951 loss_r -0.03770748898386955 loss_kl 0.07510118931531906\n",
      "step 271 loss 0.07613705843687057 loss_r 0.004669551271945238 loss_kl 0.07146750390529633\n",
      "step 272 loss 0.02082451805472374 loss_r -0.05045023933053017 loss_kl 0.0712747573852539\n",
      "step 273 loss 0.09143803268671036 loss_r 0.02357643097639084 loss_kl 0.06786160171031952\n",
      "step 274 loss 0.07588686048984528 loss_r 0.0018374612554907799 loss_kl 0.07404939830303192\n",
      "step 275 loss 0.1286582201719284 loss_r 0.0607910193502903 loss_kl 0.0678672045469284\n",
      "step 276 loss 0.07236500084400177 loss_r -0.011862042360007763 loss_kl 0.08422704041004181\n",
      "step 277 loss 0.10400208085775375 loss_r 0.026725223287940025 loss_kl 0.07727685570716858\n",
      "step 278 loss 0.10752725601196289 loss_r 0.02683313563466072 loss_kl 0.08069412410259247\n",
      "step 279 loss 0.06522580236196518 loss_r -0.025066575035452843 loss_kl 0.09029237926006317\n",
      "step 280 loss -0.02483532577753067 loss_r -0.12403655797243118 loss_kl 0.09920123219490051\n",
      "step 281 loss 0.055790528655052185 loss_r -0.03765888512134552 loss_kl 0.0934494137763977\n",
      "step 282 loss 0.21471048891544342 loss_r 0.13427940011024475 loss_kl 0.08043108880519867\n",
      "step 283 loss 0.15704956650733948 loss_r 0.06076701730489731 loss_kl 0.09628254175186157\n",
      "step 284 loss 0.04495440423488617 loss_r -0.05948910117149353 loss_kl 0.1044435054063797\n",
      "step 285 loss 0.05587153136730194 loss_r -0.054244622588157654 loss_kl 0.1101161539554596\n",
      "step 286 loss -0.04278102517127991 loss_r -0.1790931522846222 loss_kl 0.13631212711334229\n",
      "step 287 loss 0.1540697067975998 loss_r 0.04283770173788071 loss_kl 0.11123200505971909\n",
      "step 288 loss -0.12059573829174042 loss_r -0.2610433101654053 loss_kl 0.14044757187366486\n",
      "step 289 loss 0.07068177312612534 loss_r -0.05113058537244797 loss_kl 0.1218123584985733\n",
      "step 290 loss -0.03622649610042572 loss_r -0.16884712874889374 loss_kl 0.13262063264846802\n",
      "step 291 loss -0.05145694315433502 loss_r -0.18359903991222382 loss_kl 0.1321420967578888\n",
      "step 292 loss 0.12021789699792862 loss_r 0.005764382425695658 loss_kl 0.1144535169005394\n",
      "step 293 loss -0.11675401031970978 loss_r -0.2533005475997925 loss_kl 0.1365465372800827\n",
      "step 294 loss -0.043847523629665375 loss_r -0.16802260279655457 loss_kl 0.12417507916688919\n",
      "step 295 loss 0.058327727019786835 loss_r -0.07055359333753586 loss_kl 0.1288813203573227\n",
      "step 296 loss -0.01509849727153778 loss_r -0.13840779662132263 loss_kl 0.12330929934978485\n",
      "step 297 loss 0.050332434475421906 loss_r -0.07051056623458862 loss_kl 0.12084300071001053\n",
      "step 298 loss -0.02205565571784973 loss_r -0.1547282338142395 loss_kl 0.13267257809638977\n",
      "step 299 loss -0.09260667860507965 loss_r -0.22802188992500305 loss_kl 0.1354152113199234\n",
      "step 300 loss -0.004239171743392944 loss_r -0.12972168624401093 loss_kl 0.12548251450061798\n",
      "step 301 loss 0.008709698915481567 loss_r -0.12562532722949982 loss_kl 0.13433502614498138\n",
      "step 302 loss -0.48983991146087646 loss_r -0.6318323612213135 loss_kl 0.1419924646615982\n",
      "step 303 loss -0.12462478876113892 loss_r -0.2552211880683899 loss_kl 0.13059639930725098\n",
      "step 304 loss -0.08531580865383148 loss_r -0.22348792850971222 loss_kl 0.13817211985588074\n",
      "step 305 loss -0.37588196992874146 loss_r -0.5273979902267456 loss_kl 0.15151602029800415\n",
      "step 306 loss -0.3057541847229004 loss_r -0.4325315058231354 loss_kl 0.1267773061990738\n",
      "step 307 loss -0.0517725944519043 loss_r -0.1751493215560913 loss_kl 0.12337672710418701\n",
      "step 308 loss -0.44167569279670715 loss_r -0.5658913850784302 loss_kl 0.12421569228172302\n",
      "step 309 loss -0.2939293682575226 loss_r -0.44733187556266785 loss_kl 0.15340250730514526\n",
      "step 310 loss -0.12242592871189117 loss_r -0.26683273911476135 loss_kl 0.14440681040287018\n",
      "step 311 loss -0.09615518152713776 loss_r -0.2345738261938095 loss_kl 0.13841864466667175\n",
      "step 312 loss -0.12645193934440613 loss_r -0.2787620723247528 loss_kl 0.15231013298034668\n",
      "step 313 loss -0.06714344024658203 loss_r -0.20241080224514008 loss_kl 0.13526736199855804\n",
      "step 314 loss -0.7043191194534302 loss_r -0.8576215505599976 loss_kl 0.15330246090888977\n",
      "step 315 loss -0.06505551934242249 loss_r -0.20610037446022034 loss_kl 0.14104485511779785\n",
      "step 316 loss -0.35509470105171204 loss_r -0.48470088839530945 loss_kl 0.1296061873435974\n",
      "step 317 loss 0.023064136505126953 loss_r -0.09416267275810242 loss_kl 0.11722680926322937\n",
      "step 318 loss -0.34692180156707764 loss_r -0.4732798933982849 loss_kl 0.12635807693004608\n",
      "step 319 loss -0.025707200169563293 loss_r -0.14744895696640015 loss_kl 0.12174175679683685\n",
      "step 320 loss -0.18409186601638794 loss_r -0.290883332490921 loss_kl 0.10679146647453308\n",
      "step 321 loss -0.062087759375572205 loss_r -0.17879706621170044 loss_kl 0.11670930683612823\n",
      "step 322 loss -0.62851881980896 loss_r -0.7644705772399902 loss_kl 0.13595178723335266\n",
      "step 323 loss 0.023058898746967316 loss_r -0.09583006054162979 loss_kl 0.1188889592885971\n",
      "step 324 loss -0.06305398046970367 loss_r -0.18680882453918457 loss_kl 0.1237548440694809\n",
      "step 325 loss -0.21056270599365234 loss_r -0.3330768644809723 loss_kl 0.12251416593790054\n",
      "step 326 loss -0.042793139815330505 loss_r -0.18150852620601654 loss_kl 0.13871538639068604\n",
      "step 327 loss -0.21312931180000305 loss_r -0.3415195643901825 loss_kl 0.12839025259017944\n",
      "step 328 loss -0.21879959106445312 loss_r -0.34906309843063354 loss_kl 0.13026350736618042\n",
      "step 329 loss -0.013562411069869995 loss_r -0.1332886517047882 loss_kl 0.11972624063491821\n",
      "step 330 loss -0.2893526554107666 loss_r -0.41307008266448975 loss_kl 0.12371742725372314\n",
      "step 331 loss -0.08900462090969086 loss_r -0.20608150959014893 loss_kl 0.11707688868045807\n",
      "step 332 loss -0.42226600646972656 loss_r -0.5450826287269592 loss_kl 0.12281660735607147\n",
      "step 333 loss -0.31914222240448 loss_r -0.4453732967376709 loss_kl 0.12623105943202972\n",
      "step 334 loss -0.1162073165178299 loss_r -0.23810648918151855 loss_kl 0.12189917266368866\n",
      "step 335 loss -0.2727417051792145 loss_r -0.396452933549881 loss_kl 0.12371122092008591\n",
      "step 336 loss -0.11858853697776794 loss_r -0.24017055332660675 loss_kl 0.1215820163488388\n",
      "step 337 loss -0.3657831847667694 loss_r -0.5050761699676514 loss_kl 0.13929298520088196\n",
      "step 338 loss -0.17612916231155396 loss_r -0.2954930067062378 loss_kl 0.11936383694410324\n",
      "step 339 loss -0.31808486580848694 loss_r -0.44092610478401184 loss_kl 0.1228412389755249\n",
      "step 340 loss -0.1645444631576538 loss_r -0.30791905522346497 loss_kl 0.14337459206581116\n",
      "step 341 loss -0.09436781704425812 loss_r -0.23004098236560822 loss_kl 0.1356731653213501\n",
      "step 342 loss -0.37891432642936707 loss_r -0.5318953990936279 loss_kl 0.15298107266426086\n",
      "step 343 loss -0.11108672618865967 loss_r -0.2549474537372589 loss_kl 0.14386072754859924\n",
      "step 344 loss -0.7466616630554199 loss_r -0.9064239859580994 loss_kl 0.15976230800151825\n",
      "step 345 loss -0.602173924446106 loss_r -0.7777714133262634 loss_kl 0.17559745907783508\n",
      "step 346 loss -0.28983697295188904 loss_r -0.43675562739372253 loss_kl 0.1469186544418335\n",
      "step 347 loss -0.41144993901252747 loss_r -0.5603774785995483 loss_kl 0.14892753958702087\n",
      "step 348 loss -0.2260565459728241 loss_r -0.3640579879283905 loss_kl 0.1380014419555664\n",
      "step 349 loss -0.9069641828536987 loss_r -1.044529914855957 loss_kl 0.1375657171010971\n",
      "step 350 loss -0.2858189046382904 loss_r -0.4206841289997101 loss_kl 0.13486522436141968\n",
      "step 351 loss -0.2525232136249542 loss_r -0.37823763489723206 loss_kl 0.12571442127227783\n",
      "step 352 loss -0.31224170327186584 loss_r -0.4269047677516937 loss_kl 0.11466307193040848\n",
      "step 353 loss 0.060901083052158356 loss_r -0.04193364828824997 loss_kl 0.10283473134040833\n",
      "step 354 loss -0.0354570597410202 loss_r -0.1408345252275467 loss_kl 0.10537746548652649\n",
      "step 355 loss -0.23801161348819733 loss_r -0.3475084602832794 loss_kl 0.10949684679508209\n",
      "step 356 loss -0.18673565983772278 loss_r -0.3103950023651123 loss_kl 0.12365934252738953\n",
      "step 357 loss -0.25083625316619873 loss_r -0.371810644865036 loss_kl 0.12097439169883728\n",
      "step 358 loss -0.09851521998643875 loss_r -0.21971729397773743 loss_kl 0.12120207399129868\n",
      "step 359 loss -0.03305596113204956 loss_r -0.1561204493045807 loss_kl 0.12306448817253113\n",
      "step 360 loss -0.03447507321834564 loss_r -0.1606268286705017 loss_kl 0.12615175545215607\n",
      "step 361 loss -0.4428310692310333 loss_r -0.5768164396286011 loss_kl 0.13398537039756775\n",
      "step 362 loss -0.7763502597808838 loss_r -0.9140328764915466 loss_kl 0.13768258690834045\n",
      "step 363 loss -0.33601170778274536 loss_r -0.4857081472873688 loss_kl 0.1496964395046234\n",
      "step 364 loss -0.42077577114105225 loss_r -0.5614907741546631 loss_kl 0.14071498811244965\n",
      "step 365 loss -0.29242467880249023 loss_r -0.4474939703941345 loss_kl 0.15506930649280548\n",
      "step 366 loss -0.5400204658508301 loss_r -0.6764545440673828 loss_kl 0.13643404841423035\n",
      "step 367 loss -0.3727557063102722 loss_r -0.5052840709686279 loss_kl 0.1325283646583557\n",
      "step 368 loss -0.3186417818069458 loss_r -0.4617541432380676 loss_kl 0.14311236143112183\n",
      "step 369 loss -0.3095884919166565 loss_r -0.4435827434062958 loss_kl 0.13399425148963928\n",
      "step 370 loss -0.3066507577896118 loss_r -0.4493042528629303 loss_kl 0.14265349507331848\n",
      "step 371 loss -0.39284875988960266 loss_r -0.5261965990066528 loss_kl 0.13334783911705017\n",
      "step 372 loss -0.429889976978302 loss_r -0.5616409778594971 loss_kl 0.13175100088119507\n",
      "step 373 loss -0.11085876822471619 loss_r -0.23579439520835876 loss_kl 0.12493562698364258\n",
      "step 374 loss -0.6626313924789429 loss_r -0.798314094543457 loss_kl 0.13568270206451416\n",
      "step 375 loss -0.3024958372116089 loss_r -0.4280979335308075 loss_kl 0.12560208141803741\n",
      "step 376 loss -0.6582744717597961 loss_r -0.7993437647819519 loss_kl 0.14106927812099457\n",
      "step 377 loss -0.672211766242981 loss_r -0.8201374411582947 loss_kl 0.14792567491531372\n",
      "step 378 loss -0.1397671103477478 loss_r -0.2650928795337677 loss_kl 0.1253257691860199\n",
      "step 379 loss 0.047880809754133224 loss_r -0.062056925147771835 loss_kl 0.10993773490190506\n",
      "step 380 loss -0.2090717852115631 loss_r -0.3316572904586792 loss_kl 0.12258549779653549\n",
      "step 381 loss -0.41766685247421265 loss_r -0.5534929633140564 loss_kl 0.13582611083984375\n",
      "step 382 loss -0.13349324464797974 loss_r -0.25905975699424744 loss_kl 0.1255665123462677\n",
      "step 383 loss -0.500641942024231 loss_r -0.6370850205421448 loss_kl 0.136443093419075\n",
      "step 384 loss -0.36647501587867737 loss_r -0.49918147921562195 loss_kl 0.13270646333694458\n",
      "step 385 loss -0.35771799087524414 loss_r -0.4861184060573578 loss_kl 0.12840041518211365\n",
      "step 386 loss -0.4112403392791748 loss_r -0.539169192314148 loss_kl 0.12792886793613434\n",
      "step 387 loss -0.019030168652534485 loss_r -0.13815397024154663 loss_kl 0.11912380158901215\n",
      "step 388 loss -0.13743111491203308 loss_r -0.2660576403141022 loss_kl 0.1286265254020691\n",
      "step 389 loss -0.13304489850997925 loss_r -0.26796507835388184 loss_kl 0.1349201798439026\n",
      "step 390 loss -0.20430757105350494 loss_r -0.3409786522388458 loss_kl 0.13667108118534088\n",
      "step 391 loss -0.13287672400474548 loss_r -0.27112850546836853 loss_kl 0.13825178146362305\n",
      "step 392 loss -0.13083776831626892 loss_r -0.26313284039497375 loss_kl 0.13229507207870483\n",
      "step 393 loss -0.3639577627182007 loss_r -0.5092329978942871 loss_kl 0.14527525007724762\n",
      "step 394 loss -0.30012407898902893 loss_r -0.4296923279762268 loss_kl 0.12956824898719788\n",
      "step 395 loss -0.21840709447860718 loss_r -0.35709625482559204 loss_kl 0.13868916034698486\n",
      "step 396 loss -0.35103219747543335 loss_r -0.4858013689517975 loss_kl 0.13476915657520294\n",
      "step 397 loss -0.5462836623191833 loss_r -0.6779279708862305 loss_kl 0.13164430856704712\n",
      "step 398 loss 0.10730743408203125 loss_r -0.0068908934481441975 loss_kl 0.11419832706451416\n",
      "Epoch 4\n",
      "step 0 loss -0.3674134612083435 loss_r -0.5072677731513977 loss_kl 0.1398543268442154\n",
      "step 1 loss -0.2531754672527313 loss_r -0.39432352781295776 loss_kl 0.14114806056022644\n",
      "step 2 loss -0.5138635635375977 loss_r -0.6541628837585449 loss_kl 0.14029935002326965\n",
      "step 3 loss -0.5193908214569092 loss_r -0.6635810732841492 loss_kl 0.1441902220249176\n",
      "step 4 loss -0.22063493728637695 loss_r -0.3515661954879761 loss_kl 0.13093125820159912\n",
      "step 5 loss -0.21909460425376892 loss_r -0.36569708585739136 loss_kl 0.14660248160362244\n",
      "step 6 loss -0.5402481555938721 loss_r -0.6746476888656616 loss_kl 0.13439951837062836\n",
      "step 7 loss -0.49516040086746216 loss_r -0.6344878077507019 loss_kl 0.13932739198207855\n",
      "step 8 loss -0.5796113610267639 loss_r -0.7183783054351807 loss_kl 0.13876694440841675\n",
      "step 9 loss -0.3027339279651642 loss_r -0.4234755337238312 loss_kl 0.1207415983080864\n",
      "step 10 loss -0.11574121564626694 loss_r -0.2388397753238678 loss_kl 0.12309855967760086\n",
      "step 11 loss -0.20464074611663818 loss_r -0.3304145634174347 loss_kl 0.1257738173007965\n",
      "step 12 loss -0.2463218718767166 loss_r -0.3713779151439667 loss_kl 0.12505604326725006\n",
      "step 13 loss -0.36684292554855347 loss_r -0.4880885183811188 loss_kl 0.12124559283256531\n",
      "step 14 loss -0.44759485125541687 loss_r -0.5808919668197632 loss_kl 0.1332971155643463\n",
      "step 15 loss -0.08692316710948944 loss_r -0.20848959684371948 loss_kl 0.12156642973423004\n",
      "step 16 loss -0.375870019197464 loss_r -0.5029429197311401 loss_kl 0.12707290053367615\n",
      "step 17 loss -0.39279425144195557 loss_r -0.5159876942634583 loss_kl 0.12319345027208328\n",
      "step 18 loss -0.5670254826545715 loss_r -0.7080672383308411 loss_kl 0.14104175567626953\n",
      "step 19 loss -0.47912436723709106 loss_r -0.6204690337181091 loss_kl 0.14134466648101807\n",
      "step 20 loss -0.5857853889465332 loss_r -0.7236829400062561 loss_kl 0.1378975212574005\n",
      "step 21 loss -0.115238718688488 loss_r -0.23656818270683289 loss_kl 0.12132946401834488\n",
      "step 22 loss -0.27905669808387756 loss_r -0.39923059940338135 loss_kl 0.12017390877008438\n",
      "step 23 loss -0.4129059314727783 loss_r -0.5567557215690613 loss_kl 0.14384979009628296\n",
      "step 24 loss -0.039580345153808594 loss_r -0.16385968029499054 loss_kl 0.12427933514118195\n",
      "step 25 loss -0.6101912260055542 loss_r -0.7504047155380249 loss_kl 0.1402134895324707\n",
      "step 26 loss -0.2858576774597168 loss_r -0.4177277982234955 loss_kl 0.1318701207637787\n",
      "step 27 loss -0.4038355350494385 loss_r -0.5387377738952637 loss_kl 0.1349022388458252\n",
      "step 28 loss -0.595715343952179 loss_r -0.7247533798217773 loss_kl 0.1290380209684372\n",
      "step 29 loss -0.8207371234893799 loss_r -0.9671754837036133 loss_kl 0.146438330411911\n",
      "step 30 loss -0.3822539448738098 loss_r -0.5090582966804504 loss_kl 0.12680435180664062\n",
      "step 31 loss -0.34518828988075256 loss_r -0.4725392758846283 loss_kl 0.12735098600387573\n",
      "step 32 loss -0.2565244436264038 loss_r -0.3826969265937805 loss_kl 0.1261724978685379\n",
      "step 33 loss -0.330252468585968 loss_r -0.4573915898799896 loss_kl 0.1271391212940216\n",
      "step 34 loss -0.4578709900379181 loss_r -0.5972562432289124 loss_kl 0.13938525319099426\n",
      "step 35 loss -0.6350996494293213 loss_r -0.7769691944122314 loss_kl 0.14186951518058777\n",
      "step 36 loss -0.18201914429664612 loss_r -0.308275043964386 loss_kl 0.12625589966773987\n",
      "step 37 loss -0.674155056476593 loss_r -0.8148802518844604 loss_kl 0.14072521030902863\n",
      "step 38 loss -0.675156831741333 loss_r -0.8292962908744812 loss_kl 0.1541394591331482\n",
      "step 39 loss -0.32412588596343994 loss_r -0.45848169922828674 loss_kl 0.1343558132648468\n",
      "step 40 loss -0.38754910230636597 loss_r -0.5279168486595154 loss_kl 0.14036774635314941\n",
      "step 41 loss -0.3450464606285095 loss_r -0.4805251657962799 loss_kl 0.13547872006893158\n",
      "step 42 loss -0.35905909538269043 loss_r -0.49028581380844116 loss_kl 0.13122673332691193\n",
      "step 43 loss -0.813746988773346 loss_r -0.9464340806007385 loss_kl 0.13268707692623138\n",
      "step 44 loss -0.18523085117340088 loss_r -0.31189391016960144 loss_kl 0.12666305899620056\n",
      "step 45 loss -0.5714201927185059 loss_r -0.7043031454086304 loss_kl 0.1328829675912857\n",
      "step 46 loss -0.6320958137512207 loss_r -0.7595964074134827 loss_kl 0.12750059366226196\n",
      "step 47 loss -0.4886775612831116 loss_r -0.6288812756538391 loss_kl 0.14020371437072754\n",
      "step 48 loss -0.5739827156066895 loss_r -0.7239137291908264 loss_kl 0.14993104338645935\n",
      "step 49 loss -0.5911552906036377 loss_r -0.7243741750717163 loss_kl 0.13321885466575623\n",
      "step 50 loss -0.1740601509809494 loss_r -0.3122827708721161 loss_kl 0.1382226198911667\n",
      "step 51 loss -0.3180962800979614 loss_r -0.4609511196613312 loss_kl 0.14285483956336975\n",
      "step 52 loss -0.5745790004730225 loss_r -0.7276967167854309 loss_kl 0.15311768651008606\n",
      "step 53 loss -0.2417677640914917 loss_r -0.38477522134780884 loss_kl 0.14300745725631714\n",
      "step 54 loss -0.5131808519363403 loss_r -0.659856915473938 loss_kl 0.14667606353759766\n",
      "step 55 loss -0.48812466859817505 loss_r -0.6317912340164185 loss_kl 0.1436665654182434\n",
      "step 56 loss -0.18847689032554626 loss_r -0.336288720369339 loss_kl 0.14781183004379272\n",
      "step 57 loss -0.7092145681381226 loss_r -0.8509828448295593 loss_kl 0.14176829159259796\n",
      "step 58 loss -0.2496335208415985 loss_r -0.4003778100013733 loss_kl 0.15074428915977478\n",
      "step 59 loss -0.4057152569293976 loss_r -0.5389098525047302 loss_kl 0.13319459557533264\n",
      "step 60 loss -0.494706392288208 loss_r -0.6410328149795532 loss_kl 0.14632642269134521\n",
      "step 61 loss -0.3729473650455475 loss_r -0.5064309239387512 loss_kl 0.13348355889320374\n",
      "step 62 loss -0.7596617937088013 loss_r -0.9095779657363892 loss_kl 0.1499161720275879\n",
      "step 63 loss -0.26780402660369873 loss_r -0.41736847162246704 loss_kl 0.14956443011760712\n",
      "step 64 loss -0.8102136850357056 loss_r -0.9593820571899414 loss_kl 0.14916837215423584\n",
      "step 65 loss -0.17895352840423584 loss_r -0.3055110275745392 loss_kl 0.12655749917030334\n",
      "step 66 loss -0.24518805742263794 loss_r -0.3841104805469513 loss_kl 0.13892242312431335\n",
      "step 67 loss -0.5164469480514526 loss_r -0.6639566421508789 loss_kl 0.14750967919826508\n",
      "step 68 loss -0.41894596815109253 loss_r -0.5571067929267883 loss_kl 0.1381608247756958\n",
      "step 69 loss -0.6950994729995728 loss_r -0.8568097352981567 loss_kl 0.1617102324962616\n",
      "step 70 loss -0.37068963050842285 loss_r -0.50481116771698 loss_kl 0.13412153720855713\n",
      "step 71 loss -0.2423495352268219 loss_r -0.36699211597442627 loss_kl 0.12464258074760437\n",
      "step 72 loss -0.2681954801082611 loss_r -0.40887483954429626 loss_kl 0.14067935943603516\n",
      "step 73 loss -0.0327240526676178 loss_r -0.1561848223209381 loss_kl 0.12346076965332031\n",
      "step 74 loss -0.708917498588562 loss_r -0.8692641258239746 loss_kl 0.16034665703773499\n",
      "step 75 loss -0.43808048963546753 loss_r -0.6036137938499451 loss_kl 0.16553328931331635\n",
      "step 76 loss -0.6258258819580078 loss_r -0.7847192287445068 loss_kl 0.15889331698417664\n",
      "step 77 loss -0.9673193097114563 loss_r -1.1389381885528564 loss_kl 0.17161886394023895\n",
      "step 78 loss -0.04048837721347809 loss_r -0.17105771601200104 loss_kl 0.13056933879852295\n",
      "step 79 loss -0.262753427028656 loss_r -0.4023672342300415 loss_kl 0.1396138072013855\n",
      "step 80 loss -0.1609926074743271 loss_r -0.29677245020866394 loss_kl 0.13577984273433685\n",
      "step 81 loss -0.5868780016899109 loss_r -0.7305971384048462 loss_kl 0.1437191367149353\n",
      "step 82 loss -0.23018482327461243 loss_r -0.3605632185935974 loss_kl 0.13037839531898499\n",
      "step 83 loss -0.30348634719848633 loss_r -0.4438551366329193 loss_kl 0.14036880433559418\n",
      "step 84 loss -0.4138309359550476 loss_r -0.5524805784225464 loss_kl 0.13864964246749878\n",
      "step 85 loss -0.2911701500415802 loss_r -0.4149675667285919 loss_kl 0.12379741668701172\n",
      "step 86 loss -0.4413348436355591 loss_r -0.5922113060951233 loss_kl 0.15087644755840302\n",
      "step 87 loss -0.6323543190956116 loss_r -0.770160973072052 loss_kl 0.13780665397644043\n",
      "step 88 loss -0.0807298868894577 loss_r -0.2063143402338028 loss_kl 0.1255844533443451\n",
      "step 89 loss -0.4094783663749695 loss_r -0.5450441837310791 loss_kl 0.13556580245494843\n",
      "step 90 loss -0.3347063958644867 loss_r -0.4866788685321808 loss_kl 0.1519724726676941\n",
      "step 91 loss -0.36038798093795776 loss_r -0.5123159885406494 loss_kl 0.15192802250385284\n",
      "step 92 loss -0.5908177495002747 loss_r -0.7406361103057861 loss_kl 0.14981837570667267\n",
      "step 93 loss -0.16571281850337982 loss_r -0.2951816916465759 loss_kl 0.1294688731431961\n",
      "step 94 loss -0.85084068775177 loss_r -0.9972046613693237 loss_kl 0.1463639736175537\n",
      "step 95 loss -0.48436349630355835 loss_r -0.6145366430282593 loss_kl 0.13017316162586212\n",
      "step 96 loss -0.5237398147583008 loss_r -0.6642916798591614 loss_kl 0.1405518501996994\n",
      "step 97 loss -0.45411431789398193 loss_r -0.5989910364151001 loss_kl 0.14487670361995697\n",
      "step 98 loss -0.3233267366886139 loss_r -0.465921550989151 loss_kl 0.1425948143005371\n",
      "step 99 loss -0.5544452667236328 loss_r -0.7006620764732361 loss_kl 0.14621683955192566\n",
      "step 100 loss -0.5817843079566956 loss_r -0.7183877825737 loss_kl 0.1366034746170044\n",
      "step 101 loss -0.2626604437828064 loss_r -0.399310827255249 loss_kl 0.13665039837360382\n",
      "step 102 loss -0.7743251919746399 loss_r -0.9203928112983704 loss_kl 0.14606761932373047\n",
      "step 103 loss -0.28818273544311523 loss_r -0.4208516776561737 loss_kl 0.13266894221305847\n",
      "step 104 loss 0.03523367643356323 loss_r -0.07583613693714142 loss_kl 0.11106981337070465\n",
      "step 105 loss -0.3335074186325073 loss_r -0.4559793174266815 loss_kl 0.1224718987941742\n",
      "step 106 loss -0.7772773504257202 loss_r -0.9178164601325989 loss_kl 0.14053913950920105\n",
      "step 107 loss -0.5226315855979919 loss_r -0.6517943143844604 loss_kl 0.1291627287864685\n",
      "step 108 loss -0.4458780288696289 loss_r -0.5841836333274841 loss_kl 0.13830560445785522\n",
      "step 109 loss -0.415332555770874 loss_r -0.5596411824226379 loss_kl 0.14430862665176392\n",
      "step 110 loss -0.2963835597038269 loss_r -0.4252356290817261 loss_kl 0.12885205447673798\n",
      "step 111 loss -0.407397598028183 loss_r -0.5578843951225281 loss_kl 0.1504867970943451\n",
      "step 112 loss -0.46838679909706116 loss_r -0.6223754286766052 loss_kl 0.15398862957954407\n",
      "step 113 loss -0.4917748272418976 loss_r -0.6220447421073914 loss_kl 0.13026991486549377\n",
      "step 114 loss -0.5837492942810059 loss_r -0.7407397627830505 loss_kl 0.1569904386997223\n",
      "step 115 loss -1.0282739400863647 loss_r -1.1734719276428223 loss_kl 0.1451980173587799\n",
      "step 116 loss -0.6686176657676697 loss_r -0.817096471786499 loss_kl 0.14847882091999054\n",
      "step 117 loss -0.336718887090683 loss_r -0.46489018201828003 loss_kl 0.12817129492759705\n",
      "step 118 loss -0.7050873041152954 loss_r -0.8461554646492004 loss_kl 0.14106813073158264\n",
      "step 119 loss -0.7342695593833923 loss_r -0.8757784962654114 loss_kl 0.14150893688201904\n",
      "step 120 loss -0.24405622482299805 loss_r -0.37124109268188477 loss_kl 0.12718486785888672\n",
      "step 121 loss -0.8805527091026306 loss_r -1.012056589126587 loss_kl 0.1315038800239563\n",
      "step 122 loss -0.31113165616989136 loss_r -0.4518345892429352 loss_kl 0.14070294797420502\n",
      "step 123 loss -0.8506775498390198 loss_r -0.9750838279724121 loss_kl 0.12440626323223114\n",
      "step 124 loss -0.47624558210372925 loss_r -0.6080003380775452 loss_kl 0.13175474107265472\n",
      "step 125 loss -0.4025682806968689 loss_r -0.5292387008666992 loss_kl 0.12667042016983032\n",
      "step 126 loss -0.28336766362190247 loss_r -0.40321508049964905 loss_kl 0.11984741687774658\n",
      "step 127 loss -0.4690392017364502 loss_r -0.6194657683372498 loss_kl 0.15042656660079956\n",
      "step 128 loss -0.4004068672657013 loss_r -0.5641950964927673 loss_kl 0.16378822922706604\n",
      "step 129 loss -0.6024733185768127 loss_r -0.7438784241676331 loss_kl 0.1414051055908203\n",
      "step 130 loss -0.6174544095993042 loss_r -0.768491804599762 loss_kl 0.15103740990161896\n",
      "step 131 loss -1.3141964673995972 loss_r -1.4789620637893677 loss_kl 0.1647656261920929\n",
      "step 132 loss -0.3401913642883301 loss_r -0.4962461590766907 loss_kl 0.1560548096895218\n",
      "step 133 loss -0.2913071811199188 loss_r -0.44933420419692993 loss_kl 0.1580270230770111\n",
      "step 134 loss -0.35711175203323364 loss_r -0.5063583254814148 loss_kl 0.14924657344818115\n",
      "step 135 loss -0.15914463996887207 loss_r -0.2966548800468445 loss_kl 0.1375102400779724\n",
      "step 136 loss -0.29962432384490967 loss_r -0.4452473223209381 loss_kl 0.14562301337718964\n",
      "step 137 loss -0.49925217032432556 loss_r -0.6448190212249756 loss_kl 0.14556685090065002\n",
      "step 138 loss -0.9549514651298523 loss_r -1.091033935546875 loss_kl 0.1360824704170227\n",
      "step 139 loss -0.553062915802002 loss_r -0.6920758485794067 loss_kl 0.1390129029750824\n",
      "step 140 loss -0.3241991400718689 loss_r -0.45646244287490845 loss_kl 0.13226330280303955\n",
      "step 141 loss -0.3873310387134552 loss_r -0.5192096829414368 loss_kl 0.13187864422798157\n",
      "step 142 loss -0.9291885495185852 loss_r -1.060247778892517 loss_kl 0.13105924427509308\n",
      "step 143 loss -0.3123370110988617 loss_r -0.4481102228164673 loss_kl 0.1357732117176056\n",
      "step 144 loss -0.6750799417495728 loss_r -0.8343348503112793 loss_kl 0.15925493836402893\n",
      "step 145 loss -0.516798734664917 loss_r -0.6704936027526855 loss_kl 0.15369485318660736\n",
      "step 146 loss -0.7873362302780151 loss_r -0.9443924427032471 loss_kl 0.15705621242523193\n",
      "step 147 loss -0.5924543738365173 loss_r -0.7460170984268188 loss_kl 0.1535627394914627\n",
      "step 148 loss -0.5885053873062134 loss_r -0.7416203022003174 loss_kl 0.1531149446964264\n",
      "step 149 loss -1.0361623764038086 loss_r -1.1887739896774292 loss_kl 0.1526115983724594\n",
      "step 150 loss -0.45218244194984436 loss_r -0.5847154259681702 loss_kl 0.1325329840183258\n",
      "step 151 loss -1.4462437629699707 loss_r -1.6052272319793701 loss_kl 0.15898346900939941\n",
      "step 152 loss -0.2595127820968628 loss_r -0.38889989256858826 loss_kl 0.12938711047172546\n",
      "step 153 loss -0.28009381890296936 loss_r -0.40237364172935486 loss_kl 0.1222798228263855\n",
      "step 154 loss -0.6584244966506958 loss_r -0.7946810126304626 loss_kl 0.13625654578208923\n",
      "step 155 loss -0.5054771900177002 loss_r -0.6263822317123413 loss_kl 0.12090504169464111\n",
      "step 156 loss -0.4609273076057434 loss_r -0.5952555537223816 loss_kl 0.13432824611663818\n",
      "step 157 loss -0.5301896333694458 loss_r -0.6752316951751709 loss_kl 0.1450420618057251\n",
      "step 158 loss -0.6819452047348022 loss_r -0.8221497535705566 loss_kl 0.1402045637369156\n",
      "step 159 loss -0.13553398847579956 loss_r -0.2629707455635071 loss_kl 0.12743675708770752\n",
      "step 160 loss -0.5536269545555115 loss_r -0.7052016854286194 loss_kl 0.1515747457742691\n",
      "step 161 loss -0.25994348526000977 loss_r -0.4032413065433502 loss_kl 0.14329782128334045\n",
      "step 162 loss -0.37464267015457153 loss_r -0.5462326407432556 loss_kl 0.17158997058868408\n",
      "step 163 loss -0.3081248700618744 loss_r -0.4751066267490387 loss_kl 0.1669817566871643\n",
      "step 164 loss -0.48733460903167725 loss_r -0.6372055411338806 loss_kl 0.14987091720104218\n",
      "step 165 loss -0.6721869111061096 loss_r -0.8375508189201355 loss_kl 0.16536390781402588\n",
      "step 166 loss -0.44357970356941223 loss_r -0.6032651662826538 loss_kl 0.15968546271324158\n",
      "step 167 loss -0.18849492073059082 loss_r -0.33165204524993896 loss_kl 0.14315712451934814\n",
      "step 168 loss -0.5443500876426697 loss_r -0.7002886533737183 loss_kl 0.15593856573104858\n",
      "step 169 loss -0.4926607310771942 loss_r -0.6356428861618042 loss_kl 0.14298215508460999\n",
      "step 170 loss -1.0542882680892944 loss_r -1.1997976303100586 loss_kl 0.14550934731960297\n",
      "step 171 loss -0.2292497158050537 loss_r -0.3566303253173828 loss_kl 0.1273806095123291\n",
      "step 172 loss -1.0084437131881714 loss_r -1.1444450616836548 loss_kl 0.1360013782978058\n",
      "step 173 loss -0.64686119556427 loss_r -0.7779896259307861 loss_kl 0.13112840056419373\n",
      "step 174 loss -0.2905482351779938 loss_r -0.4102994501590729 loss_kl 0.1197512075304985\n",
      "step 175 loss -0.3588964343070984 loss_r -0.49658873677253723 loss_kl 0.13769228756427765\n",
      "step 176 loss -0.33434876799583435 loss_r -0.45522189140319824 loss_kl 0.12087312340736389\n",
      "step 177 loss -0.49339205026626587 loss_r -0.6243986487388611 loss_kl 0.13100658357143402\n",
      "step 178 loss -0.4889048933982849 loss_r -0.6273496747016907 loss_kl 0.13844479620456696\n",
      "step 179 loss -0.5393788814544678 loss_r -0.6785579919815063 loss_kl 0.13917911052703857\n",
      "step 180 loss -0.44093167781829834 loss_r -0.6058951616287231 loss_kl 0.164963498711586\n",
      "step 181 loss -0.837894082069397 loss_r -0.9829837679862976 loss_kl 0.14508968591690063\n",
      "step 182 loss -0.46307194232940674 loss_r -0.6176552176475525 loss_kl 0.15458329021930695\n",
      "step 183 loss -0.6245931386947632 loss_r -0.7822368741035461 loss_kl 0.15764370560646057\n",
      "step 184 loss -0.8035072684288025 loss_r -0.968903660774231 loss_kl 0.16539637744426727\n",
      "step 185 loss -0.22770185768604279 loss_r -0.374369740486145 loss_kl 0.14666788280010223\n",
      "step 186 loss -0.6548144817352295 loss_r -0.8130276203155518 loss_kl 0.15821313858032227\n",
      "step 187 loss -0.3642110824584961 loss_r -0.5130152106285095 loss_kl 0.14880414307117462\n",
      "step 188 loss -0.5065635442733765 loss_r -0.6611558794975281 loss_kl 0.15459230542182922\n",
      "step 189 loss -0.833855152130127 loss_r -0.9893150925636292 loss_kl 0.1554599553346634\n",
      "step 190 loss -0.18665699660778046 loss_r -0.31719672679901123 loss_kl 0.13053973019123077\n",
      "step 191 loss -0.19013532996177673 loss_r -0.3037859797477722 loss_kl 0.11365064978599548\n",
      "step 192 loss -0.5886446237564087 loss_r -0.725293755531311 loss_kl 0.13664913177490234\n",
      "step 193 loss -0.5754871368408203 loss_r -0.6999258399009705 loss_kl 0.12443867325782776\n",
      "step 194 loss -0.4168078601360321 loss_r -0.554028332233429 loss_kl 0.13722047209739685\n",
      "step 195 loss -0.48208776116371155 loss_r -0.6049383282661438 loss_kl 0.12285056710243225\n",
      "step 196 loss -0.6373592615127563 loss_r -0.7660563588142395 loss_kl 0.12869708240032196\n",
      "step 197 loss -0.8911815285682678 loss_r -1.0227429866790771 loss_kl 0.13156144320964813\n",
      "step 198 loss -0.29617980122566223 loss_r -0.4288465976715088 loss_kl 0.13266679644584656\n",
      "step 199 loss -1.214782953262329 loss_r -1.3662331104278564 loss_kl 0.15145015716552734\n",
      "step 200 loss -0.7707070708274841 loss_r -0.9063295722007751 loss_kl 0.13562248647212982\n",
      "step 201 loss -0.3895147442817688 loss_r -0.5235270857810974 loss_kl 0.1340123414993286\n",
      "step 202 loss -0.4290088415145874 loss_r -0.569176435470581 loss_kl 0.14016759395599365\n",
      "step 203 loss -0.3226855397224426 loss_r -0.4563080072402954 loss_kl 0.13362246751785278\n",
      "step 204 loss -0.4759379029273987 loss_r -0.6144586801528931 loss_kl 0.13852077722549438\n",
      "step 205 loss -0.4211356043815613 loss_r -0.5688018202781677 loss_kl 0.14766621589660645\n",
      "step 206 loss -0.3421161472797394 loss_r -0.4825262725353241 loss_kl 0.14041012525558472\n",
      "step 207 loss -0.5842138528823853 loss_r -0.7356494665145874 loss_kl 0.15143561363220215\n",
      "step 208 loss -0.8736550807952881 loss_r -1.0334699153900146 loss_kl 0.15981486439704895\n",
      "step 209 loss -0.42479488253593445 loss_r -0.5816967487335205 loss_kl 0.15690186619758606\n",
      "step 210 loss -0.4953974485397339 loss_r -0.6466965079307556 loss_kl 0.15129904448986053\n",
      "step 211 loss -0.3781428933143616 loss_r -0.5109561085700989 loss_kl 0.1328132152557373\n",
      "step 212 loss -0.7234549522399902 loss_r -0.8708430528640747 loss_kl 0.14738813042640686\n",
      "step 213 loss -0.4540603756904602 loss_r -0.6235578656196594 loss_kl 0.16949748992919922\n",
      "step 214 loss -0.8209776878356934 loss_r -0.970259428024292 loss_kl 0.14928171038627625\n",
      "step 215 loss -0.29449471831321716 loss_r -0.42312318086624146 loss_kl 0.1286284625530243\n",
      "step 216 loss -0.7666207551956177 loss_r -0.9067354202270508 loss_kl 0.14011463522911072\n",
      "step 217 loss -0.7389897108078003 loss_r -0.8783934712409973 loss_kl 0.13940376043319702\n",
      "step 218 loss -0.48621028661727905 loss_r -0.6153296232223511 loss_kl 0.12911933660507202\n",
      "step 219 loss -0.5777523517608643 loss_r -0.6997487545013428 loss_kl 0.12199638038873672\n",
      "step 220 loss -0.48755693435668945 loss_r -0.6171223521232605 loss_kl 0.12956540286540985\n",
      "step 221 loss -0.9178010821342468 loss_r -1.067613959312439 loss_kl 0.14981287717819214\n",
      "step 222 loss -1.313437581062317 loss_r -1.462001919746399 loss_kl 0.1485643833875656\n",
      "step 223 loss -0.2308705896139145 loss_r -0.3671460747718811 loss_kl 0.1362754851579666\n",
      "step 224 loss -0.35068532824516296 loss_r -0.5076366066932678 loss_kl 0.15695127844810486\n",
      "step 225 loss -0.6640068292617798 loss_r -0.8055600523948669 loss_kl 0.14155322313308716\n",
      "step 226 loss -0.2587435245513916 loss_r -0.3895499110221863 loss_kl 0.13080637156963348\n",
      "step 227 loss -0.6036968231201172 loss_r -0.7436714768409729 loss_kl 0.13997463881969452\n",
      "step 228 loss -0.6359130144119263 loss_r -0.7824270129203796 loss_kl 0.14651399850845337\n",
      "step 229 loss -0.779548168182373 loss_r -0.9108276963233948 loss_kl 0.13127949833869934\n",
      "step 230 loss -0.6612371206283569 loss_r -0.8071509599685669 loss_kl 0.14591383934020996\n",
      "step 231 loss -0.5695348381996155 loss_r -0.7163627743721008 loss_kl 0.14682793617248535\n",
      "step 232 loss -0.784880518913269 loss_r -0.9304605722427368 loss_kl 0.14558008313179016\n",
      "step 233 loss -0.9635564088821411 loss_r -1.1187785863876343 loss_kl 0.15522217750549316\n",
      "step 234 loss -0.6401568651199341 loss_r -0.7804843783378601 loss_kl 0.14032748341560364\n",
      "step 235 loss -0.45135778188705444 loss_r -0.5957318544387817 loss_kl 0.1443740576505661\n",
      "step 236 loss -0.6056349277496338 loss_r -0.7487912774085999 loss_kl 0.14315633475780487\n",
      "step 237 loss -0.3886999487876892 loss_r -0.5295457243919373 loss_kl 0.14084576070308685\n",
      "step 238 loss -0.23489226400852203 loss_r -0.36987701058387756 loss_kl 0.13498474657535553\n",
      "step 239 loss -0.6280011534690857 loss_r -0.7562425136566162 loss_kl 0.12824136018753052\n",
      "step 240 loss -0.7383216619491577 loss_r -0.8846474289894104 loss_kl 0.14632576704025269\n",
      "step 241 loss -0.22752675414085388 loss_r -0.349959135055542 loss_kl 0.12243238091468811\n",
      "step 242 loss -0.3746938109397888 loss_r -0.5125385522842407 loss_kl 0.1378447562456131\n",
      "step 243 loss -0.506101667881012 loss_r -0.6308395862579346 loss_kl 0.12473791092634201\n",
      "step 244 loss -0.3619981110095978 loss_r -0.48449811339378357 loss_kl 0.12250000983476639\n",
      "step 245 loss -0.3016524910926819 loss_r -0.4251469373703003 loss_kl 0.12349443137645721\n",
      "step 246 loss -0.5655933618545532 loss_r -0.7110116481781006 loss_kl 0.14541831612586975\n",
      "step 247 loss -0.41270047426223755 loss_r -0.5478785037994385 loss_kl 0.13517801463603973\n",
      "step 248 loss -0.6054717898368835 loss_r -0.750102162361145 loss_kl 0.14463037252426147\n",
      "step 249 loss -0.6787354350090027 loss_r -0.8293600678443909 loss_kl 0.15062464773654938\n",
      "step 250 loss -0.7820819616317749 loss_r -0.9303129315376282 loss_kl 0.14823096990585327\n",
      "step 251 loss -0.49072667956352234 loss_r -0.637657880783081 loss_kl 0.14693120121955872\n",
      "step 252 loss -0.5282310843467712 loss_r -0.677335798740387 loss_kl 0.14910471439361572\n",
      "step 253 loss -0.5696728825569153 loss_r -0.725135862827301 loss_kl 0.15546298027038574\n",
      "step 254 loss -0.5369574427604675 loss_r -0.6974683403968811 loss_kl 0.16051089763641357\n",
      "step 255 loss -0.5454310178756714 loss_r -0.6928847432136536 loss_kl 0.14745372533798218\n",
      "step 256 loss -0.5086232423782349 loss_r -0.653009831905365 loss_kl 0.14438655972480774\n",
      "step 257 loss -0.5870645046234131 loss_r -0.7393389940261841 loss_kl 0.1522744596004486\n",
      "step 258 loss -0.6816093921661377 loss_r -0.8197342753410339 loss_kl 0.13812486827373505\n",
      "step 259 loss -0.3951267600059509 loss_r -0.5383455157279968 loss_kl 0.1432187408208847\n",
      "step 260 loss -0.5109626650810242 loss_r -0.6414641737937927 loss_kl 0.13050150871276855\n",
      "step 261 loss -0.900053858757019 loss_r -1.0468467473983765 loss_kl 0.1467929184436798\n",
      "step 262 loss -0.5527845025062561 loss_r -0.6911840438842773 loss_kl 0.13839954137802124\n",
      "step 263 loss -0.15568678081035614 loss_r -0.28375181555747986 loss_kl 0.12806503474712372\n",
      "step 264 loss -0.5063570737838745 loss_r -0.6506883502006531 loss_kl 0.14433124661445618\n",
      "step 265 loss -1.0703544616699219 loss_r -1.2287949323654175 loss_kl 0.15844041109085083\n",
      "step 266 loss -0.9220561981201172 loss_r -1.0676069259643555 loss_kl 0.14555074274539948\n",
      "step 267 loss -0.6498979330062866 loss_r -0.7961640954017639 loss_kl 0.14626619219779968\n",
      "step 268 loss -0.5526119470596313 loss_r -0.6883724331855774 loss_kl 0.13576048612594604\n",
      "step 269 loss -0.27564746141433716 loss_r -0.40770721435546875 loss_kl 0.1320597529411316\n",
      "step 270 loss -0.618030309677124 loss_r -0.7528495192527771 loss_kl 0.1348191797733307\n",
      "step 271 loss -0.7063543200492859 loss_r -0.83970707654953 loss_kl 0.13335275650024414\n",
      "step 272 loss -0.3153979480266571 loss_r -0.4437299370765686 loss_kl 0.1283319890499115\n",
      "step 273 loss -0.744002640247345 loss_r -0.8925231099128723 loss_kl 0.14852046966552734\n",
      "step 274 loss -0.5684360265731812 loss_r -0.7126138210296631 loss_kl 0.14417779445648193\n",
      "step 275 loss -0.5195258259773254 loss_r -0.6618126034736633 loss_kl 0.14228679239749908\n",
      "step 276 loss -0.4297250509262085 loss_r -0.578262984752655 loss_kl 0.14853791892528534\n",
      "step 277 loss -1.1624466180801392 loss_r -1.3123538494110107 loss_kl 0.14990723133087158\n",
      "step 278 loss -0.8757421970367432 loss_r -1.0280992984771729 loss_kl 0.15235713124275208\n",
      "step 279 loss -1.0455594062805176 loss_r -1.2027016878128052 loss_kl 0.1571422517299652\n",
      "step 280 loss -0.7999988794326782 loss_r -0.9404661059379578 loss_kl 0.14046719670295715\n",
      "step 281 loss -0.7067718505859375 loss_r -0.8313338756561279 loss_kl 0.12456201016902924\n",
      "step 282 loss -0.6172025203704834 loss_r -0.7418047189712524 loss_kl 0.12460219115018845\n",
      "step 283 loss -0.6502640247344971 loss_r -0.7719764709472656 loss_kl 0.12171241641044617\n",
      "step 284 loss -0.28570833802223206 loss_r -0.412652850151062 loss_kl 0.12694451212882996\n",
      "step 285 loss -0.6062321662902832 loss_r -0.7605082392692566 loss_kl 0.15427608788013458\n",
      "step 286 loss -0.2775837779045105 loss_r -0.408952534198761 loss_kl 0.13136877119541168\n",
      "step 287 loss -0.18625056743621826 loss_r -0.31797516345977783 loss_kl 0.13172459602355957\n",
      "step 288 loss -0.8604309558868408 loss_r -1.0185922384262085 loss_kl 0.15816128253936768\n",
      "step 289 loss -0.6443535685539246 loss_r -0.7900493741035461 loss_kl 0.14569580554962158\n",
      "step 290 loss -0.6765156388282776 loss_r -0.8316738605499268 loss_kl 0.15515822172164917\n",
      "step 291 loss -0.686271071434021 loss_r -0.8310779929161072 loss_kl 0.144806906580925\n",
      "step 292 loss -0.33177027106285095 loss_r -0.4850882589817047 loss_kl 0.15331798791885376\n",
      "step 293 loss -1.0620297193527222 loss_r -1.2282122373580933 loss_kl 0.1661824882030487\n",
      "step 294 loss -1.108231782913208 loss_r -1.2663778066635132 loss_kl 0.1581459641456604\n",
      "step 295 loss -0.8289180994033813 loss_r -0.9780673384666443 loss_kl 0.14914923906326294\n",
      "step 296 loss -0.3593863844871521 loss_r -0.49360302090644836 loss_kl 0.13421663641929626\n",
      "step 297 loss -0.7980204820632935 loss_r -0.9459592700004578 loss_kl 0.1479387879371643\n",
      "step 298 loss -0.5133068561553955 loss_r -0.6643396615982056 loss_kl 0.15103280544281006\n",
      "step 299 loss -0.7265647649765015 loss_r -0.8829534649848938 loss_kl 0.15638868510723114\n",
      "step 300 loss -0.393375039100647 loss_r -0.5547261834144592 loss_kl 0.16135114431381226\n",
      "step 301 loss -0.299010694026947 loss_r -0.44267192482948303 loss_kl 0.14366121590137482\n",
      "step 302 loss -0.6340477466583252 loss_r -0.7869650721549988 loss_kl 0.15291735529899597\n",
      "step 303 loss -0.569284975528717 loss_r -0.7106364369392395 loss_kl 0.14135147631168365\n",
      "step 304 loss -0.4499201774597168 loss_r -0.5792080163955688 loss_kl 0.12928783893585205\n",
      "step 305 loss -0.18942314386367798 loss_r -0.3035995364189148 loss_kl 0.11417639255523682\n",
      "step 306 loss -0.49436813592910767 loss_r -0.632871150970459 loss_kl 0.13850301504135132\n",
      "step 307 loss -0.7839576601982117 loss_r -0.9328331351280212 loss_kl 0.14887547492980957\n",
      "step 308 loss -0.68389892578125 loss_r -0.8237713575363159 loss_kl 0.1398724615573883\n",
      "step 309 loss -0.40368741750717163 loss_r -0.545346736907959 loss_kl 0.14165931940078735\n",
      "step 310 loss -0.41972923278808594 loss_r -0.5647720694541931 loss_kl 0.14504283666610718\n",
      "step 311 loss -0.7962861061096191 loss_r -0.941161036491394 loss_kl 0.1448749303817749\n",
      "step 312 loss -0.617297351360321 loss_r -0.7656673789024353 loss_kl 0.14837002754211426\n",
      "step 313 loss -1.1610037088394165 loss_r -1.3070546388626099 loss_kl 0.14605091512203217\n",
      "step 314 loss -0.6464858651161194 loss_r -0.7833153009414673 loss_kl 0.1368294209241867\n",
      "step 315 loss -0.6289592981338501 loss_r -0.7789822220802307 loss_kl 0.15002292394638062\n",
      "step 316 loss -0.7183404564857483 loss_r -0.8707311153411865 loss_kl 0.15239065885543823\n",
      "step 317 loss -0.7406386137008667 loss_r -0.8984587788581848 loss_kl 0.15782016515731812\n",
      "step 318 loss -0.8191288709640503 loss_r -0.9809520840644836 loss_kl 0.16182321310043335\n",
      "step 319 loss -0.3506687879562378 loss_r -0.48580998182296753 loss_kl 0.13514120876789093\n",
      "step 320 loss -0.4914165735244751 loss_r -0.6475732922554016 loss_kl 0.1561567187309265\n",
      "step 321 loss -0.3525940477848053 loss_r -0.4936272203922272 loss_kl 0.14103317260742188\n",
      "step 322 loss -0.47508537769317627 loss_r -0.6171603202819824 loss_kl 0.14207495748996735\n",
      "step 323 loss -1.503602147102356 loss_r -1.6627241373062134 loss_kl 0.15912199020385742\n",
      "step 324 loss -0.7360751628875732 loss_r -0.8946245908737183 loss_kl 0.15854942798614502\n",
      "step 325 loss -0.981519341468811 loss_r -1.1273242235183716 loss_kl 0.14580491185188293\n",
      "step 326 loss -0.7191522121429443 loss_r -0.8661954402923584 loss_kl 0.14704325795173645\n",
      "step 327 loss -0.4182806611061096 loss_r -0.5544791221618652 loss_kl 0.13619846105575562\n",
      "step 328 loss -0.6967151165008545 loss_r -0.8391095995903015 loss_kl 0.14239448308944702\n",
      "step 329 loss -0.5108202695846558 loss_r -0.6627448201179504 loss_kl 0.15192455053329468\n",
      "step 330 loss -0.42629337310791016 loss_r -0.5588866472244263 loss_kl 0.13259325921535492\n",
      "step 331 loss -1.1920324563980103 loss_r -1.3403737545013428 loss_kl 0.14834129810333252\n",
      "step 332 loss -0.29614147543907166 loss_r -0.41808709502220154 loss_kl 0.12194561958312988\n",
      "step 333 loss -0.7545655369758606 loss_r -0.8876646757125854 loss_kl 0.13309915363788605\n",
      "step 334 loss -0.4503667652606964 loss_r -0.5982494950294495 loss_kl 0.14788272976875305\n",
      "step 335 loss -0.950663149356842 loss_r -1.097283959388733 loss_kl 0.14662082493305206\n",
      "step 336 loss -0.9559448957443237 loss_r -1.1174342632293701 loss_kl 0.16148939728736877\n",
      "step 337 loss -0.32083994150161743 loss_r -0.4848450720310211 loss_kl 0.16400514543056488\n",
      "step 338 loss -0.9626915454864502 loss_r -1.1325615644454956 loss_kl 0.1698700189590454\n",
      "step 339 loss -0.5959300994873047 loss_r -0.7532448768615723 loss_kl 0.15731477737426758\n",
      "step 340 loss -1.1827499866485596 loss_r -1.3480888605117798 loss_kl 0.1653389036655426\n",
      "step 341 loss -0.8327523469924927 loss_r -0.9965783953666687 loss_kl 0.16382604837417603\n",
      "step 342 loss -0.8530490398406982 loss_r -1.014506220817566 loss_kl 0.16145721077919006\n",
      "step 343 loss -0.4796314239501953 loss_r -0.6331425905227661 loss_kl 0.1535111516714096\n",
      "step 344 loss -0.8562482595443726 loss_r -1.0089396238327026 loss_kl 0.15269136428833008\n",
      "step 345 loss -0.7256262898445129 loss_r -0.8542578220367432 loss_kl 0.12863154709339142\n",
      "step 346 loss -0.35898327827453613 loss_r -0.5010361671447754 loss_kl 0.14205290377140045\n",
      "step 347 loss -0.4710322320461273 loss_r -0.6036845445632935 loss_kl 0.13265231251716614\n",
      "step 348 loss -0.34502822160720825 loss_r -0.4730328619480133 loss_kl 0.12800462543964386\n",
      "step 349 loss -0.3601636290550232 loss_r -0.4887014329433441 loss_kl 0.12853781878948212\n",
      "step 350 loss -0.5022920370101929 loss_r -0.6425983905792236 loss_kl 0.14030632376670837\n",
      "step 351 loss -0.6376333236694336 loss_r -0.7684723138809204 loss_kl 0.130839005112648\n",
      "step 352 loss -0.8337749242782593 loss_r -0.9688668847084045 loss_kl 0.13509197533130646\n",
      "step 353 loss -0.5912197828292847 loss_r -0.726855993270874 loss_kl 0.13563618063926697\n",
      "step 354 loss -0.5524387359619141 loss_r -0.6884759664535522 loss_kl 0.1360372006893158\n",
      "step 355 loss -1.0216455459594727 loss_r -1.164721965789795 loss_kl 0.14307639002799988\n",
      "step 356 loss -0.9872108697891235 loss_r -1.143010139465332 loss_kl 0.15579929947853088\n",
      "step 357 loss -0.9803789258003235 loss_r -1.13154137134552 loss_kl 0.15116244554519653\n",
      "step 358 loss -0.7718490958213806 loss_r -0.9265199303627014 loss_kl 0.1546708345413208\n",
      "step 359 loss -0.7125905156135559 loss_r -0.8610813617706299 loss_kl 0.14849084615707397\n",
      "step 360 loss -0.4168764054775238 loss_r -0.558224618434906 loss_kl 0.1413482129573822\n",
      "step 361 loss -0.8387928009033203 loss_r -0.989243745803833 loss_kl 0.1504509299993515\n",
      "step 362 loss -1.083550214767456 loss_r -1.2348735332489014 loss_kl 0.15132328867912292\n",
      "step 363 loss -0.5832056999206543 loss_r -0.7322640419006348 loss_kl 0.14905834197998047\n",
      "step 364 loss -0.6141232252120972 loss_r -0.7565414309501648 loss_kl 0.14241820573806763\n",
      "step 365 loss -0.6088026165962219 loss_r -0.7456543445587158 loss_kl 0.1368517130613327\n",
      "step 366 loss -0.34081608057022095 loss_r -0.4551768898963928 loss_kl 0.11436082422733307\n",
      "step 367 loss -0.39527881145477295 loss_r -0.5352151393890381 loss_kl 0.13993632793426514\n",
      "step 368 loss -1.07900071144104 loss_r -1.2260116338729858 loss_kl 0.1470109522342682\n",
      "step 369 loss -0.6193902492523193 loss_r -0.7381327748298645 loss_kl 0.11874255537986755\n",
      "step 370 loss -1.0482476949691772 loss_r -1.1925382614135742 loss_kl 0.14429059624671936\n",
      "step 371 loss -0.450528085231781 loss_r -0.5777308344841003 loss_kl 0.12720274925231934\n",
      "step 372 loss -0.6868714094161987 loss_r -0.8368810415267944 loss_kl 0.15000960230827332\n",
      "step 373 loss -0.6342087388038635 loss_r -0.7825429439544678 loss_kl 0.14833420515060425\n",
      "step 374 loss -0.41557225584983826 loss_r -0.5464400053024292 loss_kl 0.13086774945259094\n",
      "step 375 loss -0.551575243473053 loss_r -0.7017968893051147 loss_kl 0.15022164583206177\n",
      "step 376 loss -0.7633274793624878 loss_r -0.9115056395530701 loss_kl 0.14817817509174347\n",
      "step 377 loss -0.3480296730995178 loss_r -0.4944402873516083 loss_kl 0.14641059935092926\n",
      "step 378 loss -0.66122967004776 loss_r -0.8166717886924744 loss_kl 0.15544213354587555\n",
      "step 379 loss -0.8387223482131958 loss_r -0.9965593218803406 loss_kl 0.15783697366714478\n",
      "step 380 loss -0.6151228547096252 loss_r -0.7735373377799988 loss_kl 0.15841449797153473\n",
      "step 381 loss -0.6928447484970093 loss_r -0.8453657031059265 loss_kl 0.15252096951007843\n",
      "step 382 loss -0.3331262469291687 loss_r -0.4829813838005066 loss_kl 0.14985515177249908\n",
      "step 383 loss -1.0099329948425293 loss_r -1.1794617176055908 loss_kl 0.16952869296073914\n",
      "step 384 loss -0.4482399821281433 loss_r -0.5959678888320923 loss_kl 0.14772789180278778\n",
      "step 385 loss -0.3985627293586731 loss_r -0.5098875761032104 loss_kl 0.11132483929395676\n",
      "step 386 loss -0.836622953414917 loss_r -0.9785262942314148 loss_kl 0.1419033408164978\n",
      "step 387 loss -0.5531222820281982 loss_r -0.6910220384597778 loss_kl 0.13789978623390198\n",
      "step 388 loss -0.5650056600570679 loss_r -0.7048479318618774 loss_kl 0.13984227180480957\n",
      "step 389 loss -1.0204851627349854 loss_r -1.1658822298049927 loss_kl 0.1453971266746521\n",
      "step 390 loss -0.715202808380127 loss_r -0.8534063696861267 loss_kl 0.13820353150367737\n",
      "step 391 loss -0.4895496666431427 loss_r -0.6315895915031433 loss_kl 0.1420399248600006\n",
      "step 392 loss -0.8581784963607788 loss_r -1.011709213256836 loss_kl 0.15353068709373474\n",
      "step 393 loss -0.5160036087036133 loss_r -0.6502652764320374 loss_kl 0.13426165282726288\n",
      "step 394 loss -0.8933624029159546 loss_r -1.0422981977462769 loss_kl 0.14893577992916107\n",
      "step 395 loss -1.06103515625 loss_r -1.2117875814437866 loss_kl 0.15075238049030304\n",
      "step 396 loss -0.5655745267868042 loss_r -0.718876838684082 loss_kl 0.15330229699611664\n",
      "step 397 loss -0.617139458656311 loss_r -0.7727652192115784 loss_kl 0.15562577545642853\n",
      "step 398 loss -1.300187349319458 loss_r -1.4658175706863403 loss_kl 0.16563017666339874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Initialize the network and the Adam optimizer\n",
    "\"\"\"\n",
    "vae_model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}')\n",
    "    for i_step, data in enumerate(train_dataloader):\n",
    "        xi, _ = data\n",
    "        xi = xi.to(device)\n",
    "        batch_size = len(xi) \n",
    "        # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "        out, mu, logVar = vae_model(xi)\n",
    "\n",
    "        # The loss is the BCE loss combined with the KL divergence to ensure the distribution is learnt\n",
    "        # kl_divergence = 0.5 * torch.sum(-1 - logVar + mu.pow(2) + logVar.exp())\n",
    "        # loss = F.binary_cross_entropy(out, xi, size_average=False) + kl_divergence\n",
    "        loss, loss_r, loss_kl = calculate_vae_loss(out, xi, batch_size)\n",
    "\n",
    "        # Backpropagation based on the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('step', i_step, 'loss', loss.item(), 'loss_r', loss_r.item(), 'loss_kl', loss_kl.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder_conv): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (decoder_conv): Sequential(\n",
       "    (0): ConvTranspose1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Upsample(scale_factor=4.0, mode=linear)\n",
       "    (4): ConvTranspose1d(32, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Upsample(scale_factor=4.0, mode=linear)\n",
       "    (8): ConvTranspose1d(16, 1, kernel_size=(5,), stride=(1,), padding=(2,))\n",
       "    (9): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (encFC1): Linear(in_features=30, out_features=128, bias=True)\n",
       "  (encFC2): Linear(in_features=30, out_features=128, bias=True)\n",
       "  (decFC1): Linear(in_features=128, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict trainset and collect scores\n",
    "train_score_list = []\n",
    "for i_step, data in enumerate(train_dataloader):\n",
    "    xi, target = data\n",
    "    xi = xi.to(device)\n",
    "    target = target.to(device)\n",
    "    # Feeding a batch of images into the network to obtain the output image, mu, and logVar\n",
    "    out, mu, logVar = vae_model(xi)\n",
    "\n",
    "    loss, score, loss_kl = calculate_vae_loss(out, xi, batch_size)\n",
    "    train_score_list.append(score.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.10691833, -1.16776514, -0.92069262, -0.68554103, -0.04624803])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(train_score_list, [0, 25, 50, 75, 100], interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(train_score_list, [99], interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.25783291])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_score_list>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict testset and find anomalies\n",
    "score_list = []\n",
    "for i_step, data in enumerate(test_dataloader):\n",
    "    xi, target = data\n",
    "    xi = xi.to(device)\n",
    "    target = target.to(device)\n",
    "    out, mu, logVar = vae_model(xi)\n",
    "\n",
    "    loss, score, loss_kl = calculate_vae_loss(out, xi, batch_size)\n",
    "    score_list.append(score.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8214"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(score_list>threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_target = test_dataloader.dataset.y_data.squeeze(-1).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12800])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.49,\n",
       " 'auprc': 0.07,\n",
       " 'prec': 0.07,\n",
       " 'recall': 0.64,\n",
       " 'f1': 0.13,\n",
       " 'f2': 0.25,\n",
       " 'specificity': 0.36,\n",
       " 'tn': 4243.0,\n",
       " 'fp': 7610.0,\n",
       " 'fn': 343.0,\n",
       " 'tp': 604.0,\n",
       " 'auprcf1': 0.2}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['prec','recall','f1','f2','specificity',\n",
    "                     'tn','fp','fn','tp',\n",
    "                     'auroc','auprc']\n",
    "calculate_any_metrics(testset_target, metrics, probs=torch.tensor(score_list), threshold=threshold.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnomal_testset = test_dataloader.dataset[score_list>threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnomal_testset = CustomDataset((abnomal_testset[0]).numpy(), (abnomal_testset[1]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=same)\n",
       "  (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "  (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
       "  (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_model.encoder_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventdys_model = AsynchModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['classifier.1.weight', 'classifier.1.bias', 'classifier.1.running_mean', 'classifier.1.running_var', 'classifier.2.weight', 'classifier.2.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'], unexpected_keys=['decoder_conv.0.weight', 'decoder_conv.0.bias', 'decoder_conv.1.weight', 'decoder_conv.1.bias', 'decoder_conv.1.running_mean', 'decoder_conv.1.running_var', 'decoder_conv.1.num_batches_tracked', 'decoder_conv.4.weight', 'decoder_conv.4.bias', 'decoder_conv.5.weight', 'decoder_conv.5.bias', 'decoder_conv.5.running_mean', 'decoder_conv.5.running_var', 'decoder_conv.5.num_batches_tracked', 'decoder_conv.8.weight', 'decoder_conv.8.bias', 'decoder_conv.9.weight', 'decoder_conv.9.bias', 'decoder_conv.9.running_mean', 'decoder_conv.9.running_var', 'decoder_conv.9.num_batches_tracked', 'encFC1.weight', 'encFC1.bias', 'encFC2.weight', 'encFC2.bias', 'decFC1.weight', 'decFC1.bias'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ventdys_model.load_state_dict(vae_model.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 480]              96\n",
      "       BatchNorm1d-2              [-1, 16, 480]              32\n",
      "              ReLU-3              [-1, 16, 480]               0\n",
      "         MaxPool1d-4              [-1, 16, 120]               0\n",
      "            Conv1d-5              [-1, 32, 120]           2,592\n",
      "       BatchNorm1d-6              [-1, 32, 120]              64\n",
      "              ReLU-7              [-1, 32, 120]               0\n",
      "         MaxPool1d-8               [-1, 32, 30]               0\n",
      "            Conv1d-9               [-1, 64, 30]          10,304\n",
      "      BatchNorm1d-10               [-1, 64, 30]             128\n",
      "             ReLU-11               [-1, 64, 30]               0\n",
      "          Flatten-12                 [-1, 1920]               0\n",
      "      BatchNorm1d-13                 [-1, 1920]           3,840\n",
      "           Linear-14                   [-1, 64]         122,944\n",
      "             ReLU-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 32]           2,080\n",
      "             ReLU-17                   [-1, 32]               0\n",
      "           Linear-18                    [-1, 2]              66\n",
      "          Softmax-19                    [-1, 2]               0\n",
      "================================================================\n",
      "Total params: 142,146\n",
      "Trainable params: 142,146\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.36\n",
      "Params size (MB): 0.54\n",
      "Estimated Total Size (MB): 0.90\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ventdys_model, input_size=(1, 480), device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AsynchModel(\n",
       "  (encoder_conv): Sequential(\n",
       "    (0): Conv1d(1, 16, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=same)\n",
       "    (9): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): BatchNorm1d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=1920, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (7): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ventdys_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "epoch 0 loss 0.28029245138168335\n",
      "Epoch 1\n",
      "epoch 1 loss 0.17015030980110168\n",
      "Epoch 2\n",
      "epoch 2 loss 0.397563099861145\n",
      "Epoch 3\n",
      "epoch 3 loss 0.2782415747642517\n",
      "Epoch 4\n",
      "epoch 4 loss 0.34263134002685547\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(ventdys_model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Training the network for a given number of epochs\n",
    "The loss after every epoch is printed\n",
    "\"\"\"\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}')\n",
    "    for i_step, data in enumerate(train_dataloader):\n",
    "        xi, target = data\n",
    "        xi = xi.to(device)\n",
    "        target = target.to(device).squeeze(-1).squeeze(-1).float()\n",
    "        out = ventdys_model(xi)\n",
    "\n",
    "        loss = calculate_bce_loss(out, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print('step', i_step, 'loss', loss.item())\n",
    "    print('epoch', epoch, 'loss', loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = set_dataloader(abnomal_testset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict testset and find anomalies\n",
    "pred_list = []\n",
    "for i_step, data in enumerate(test_dataloader):\n",
    "    xi, target = data\n",
    "    xi = xi.to(device).permute(0,2,1) # torch.Size([bs, 1, 480])\n",
    "    target = target.to(device).squeeze(-1).squeeze(-1).float() # torch.Size([bs, 1])\n",
    "    out = ventdys_model(xi)\n",
    "\n",
    "    loss = calculate_bce_loss(out, target)\n",
    "    pred_list.append(out.detach().cpu().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8214,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.concatenate(pred_list)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8214, 1, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnomal_testset.y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_target = abnomal_testset.y_data.squeeze(-1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auroc': 0.51,\n",
       " 'auprc': 0.07,\n",
       " 'prec': 0.07,\n",
       " 'recall': 0.21,\n",
       " 'f1': 0.11,\n",
       " 'f2': 0.15,\n",
       " 'specificity': 0.8,\n",
       " 'tn': 6050.0,\n",
       " 'fp': 1560.0,\n",
       " 'fn': 478.0,\n",
       " 'tp': 126.0,\n",
       " 'auprcf1': 0.18}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['prec','recall','f1','f2','specificity',\n",
    "                     'tn','fp','fn','tp',\n",
    "                     'auroc','auprc']\n",
    "calculate_any_metrics(testset_target, metrics, probs=pred, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
